{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c9fd310d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import rioxarray as rxr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "774fdf4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "caliop_file = r\"/Users/lukebrown/Downloads/CAL_LID_L3_Stratospheric_APro-Standard-V1-01.2021-12N.hdf\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "deb28d68",
   "metadata": {},
   "outputs": [],
   "source": [
    "import rasterio as rio\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "44474fa1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'driver': 'HDF4', 'dtype': 'float_', 'nodata': None, 'width': 512, 'height': 512, 'count': 0, 'crs': None, 'transform': Affine(1.0, 0.0, 0.0,\n",
      "       0.0, 1.0, 0.0), 'tiled': False}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/Caskroom/miniforge/base/envs/pangeo/lib/python3.9/site-packages/rasterio/__init__.py:277: NotGeoreferencedWarning: Dataset has no geotransform, gcps, or rpcs. The identity matrix will be returned.\n",
      "  dataset = DatasetReader(path, driver=driver, sharing=sharing, **kwargs)\n"
     ]
    }
   ],
   "source": [
    "with rio.open(caliop_file, 'r') as src:\n",
    "    print(src.profile)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "a73652ed",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/Caskroom/miniforge/base/envs/pangeo/lib/python3.9/site-packages/rasterio/__init__.py:277: NotGeoreferencedWarning: Dataset has no geotransform, gcps, or rpcs. The identity matrix will be returned.\n",
      "  dataset = DatasetReader(path, driver=driver, sharing=sharing, **kwargs)\n",
      "/opt/homebrew/Caskroom/miniforge/base/envs/pangeo/lib/python3.9/site-packages/rioxarray/_io.py:851: NotGeoreferencedWarning: Dataset has no geotransform, gcps, or rpcs. The identity matrix will be returned.\n",
      "  warnings.warn(str(rio_warning.message), type(rio_warning.message))  # type: ignore\n",
      "/opt/homebrew/Caskroom/miniforge/base/envs/pangeo/lib/python3.9/site-packages/rasterio/__init__.py:277: NotGeoreferencedWarning: Dataset has no geotransform, gcps, or rpcs. The identity matrix will be returned.\n",
      "  dataset = DatasetReader(path, driver=driver, sharing=sharing, **kwargs)\n",
      "/opt/homebrew/Caskroom/miniforge/base/envs/pangeo/lib/python3.9/site-packages/rioxarray/_io.py:851: NotGeoreferencedWarning: Dataset has no geotransform, gcps, or rpcs. The identity matrix will be returned.\n",
      "  warnings.warn(str(rio_warning.message), type(rio_warning.message))  # type: ignore\n",
      "/opt/homebrew/Caskroom/miniforge/base/envs/pangeo/lib/python3.9/site-packages/rasterio/__init__.py:277: NotGeoreferencedWarning: Dataset has no geotransform, gcps, or rpcs. The identity matrix will be returned.\n",
      "  dataset = DatasetReader(path, driver=driver, sharing=sharing, **kwargs)\n",
      "/opt/homebrew/Caskroom/miniforge/base/envs/pangeo/lib/python3.9/site-packages/rioxarray/_io.py:851: NotGeoreferencedWarning: Dataset has no geotransform, gcps, or rpcs. The identity matrix will be returned.\n",
      "  warnings.warn(str(rio_warning.message), type(rio_warning.message))  # type: ignore\n",
      "/opt/homebrew/Caskroom/miniforge/base/envs/pangeo/lib/python3.9/site-packages/rasterio/__init__.py:277: NotGeoreferencedWarning: Dataset has no geotransform, gcps, or rpcs. The identity matrix will be returned.\n",
      "  dataset = DatasetReader(path, driver=driver, sharing=sharing, **kwargs)\n",
      "/opt/homebrew/Caskroom/miniforge/base/envs/pangeo/lib/python3.9/site-packages/rioxarray/_io.py:851: NotGeoreferencedWarning: Dataset has no geotransform, gcps, or rpcs. The identity matrix will be returned.\n",
      "  warnings.warn(str(rio_warning.message), type(rio_warning.message))  # type: ignore\n",
      "/opt/homebrew/Caskroom/miniforge/base/envs/pangeo/lib/python3.9/site-packages/rasterio/__init__.py:277: NotGeoreferencedWarning: Dataset has no geotransform, gcps, or rpcs. The identity matrix will be returned.\n",
      "  dataset = DatasetReader(path, driver=driver, sharing=sharing, **kwargs)\n",
      "/opt/homebrew/Caskroom/miniforge/base/envs/pangeo/lib/python3.9/site-packages/rioxarray/_io.py:851: NotGeoreferencedWarning: Dataset has no geotransform, gcps, or rpcs. The identity matrix will be returned.\n",
      "  warnings.warn(str(rio_warning.message), type(rio_warning.message))  # type: ignore\n",
      "/opt/homebrew/Caskroom/miniforge/base/envs/pangeo/lib/python3.9/site-packages/rasterio/__init__.py:277: NotGeoreferencedWarning: Dataset has no geotransform, gcps, or rpcs. The identity matrix will be returned.\n",
      "  dataset = DatasetReader(path, driver=driver, sharing=sharing, **kwargs)\n",
      "/opt/homebrew/Caskroom/miniforge/base/envs/pangeo/lib/python3.9/site-packages/rioxarray/_io.py:851: NotGeoreferencedWarning: Dataset has no geotransform, gcps, or rpcs. The identity matrix will be returned.\n",
      "  warnings.warn(str(rio_warning.message), type(rio_warning.message))  # type: ignore\n",
      "/opt/homebrew/Caskroom/miniforge/base/envs/pangeo/lib/python3.9/site-packages/rasterio/__init__.py:277: NotGeoreferencedWarning: Dataset has no geotransform, gcps, or rpcs. The identity matrix will be returned.\n",
      "  dataset = DatasetReader(path, driver=driver, sharing=sharing, **kwargs)\n",
      "/opt/homebrew/Caskroom/miniforge/base/envs/pangeo/lib/python3.9/site-packages/rioxarray/_io.py:851: NotGeoreferencedWarning: Dataset has no geotransform, gcps, or rpcs. The identity matrix will be returned.\n",
      "  warnings.warn(str(rio_warning.message), type(rio_warning.message))  # type: ignore\n",
      "/opt/homebrew/Caskroom/miniforge/base/envs/pangeo/lib/python3.9/site-packages/rasterio/__init__.py:277: NotGeoreferencedWarning: Dataset has no geotransform, gcps, or rpcs. The identity matrix will be returned.\n",
      "  dataset = DatasetReader(path, driver=driver, sharing=sharing, **kwargs)\n",
      "/opt/homebrew/Caskroom/miniforge/base/envs/pangeo/lib/python3.9/site-packages/rioxarray/_io.py:851: NotGeoreferencedWarning: Dataset has no geotransform, gcps, or rpcs. The identity matrix will be returned.\n",
      "  warnings.warn(str(rio_warning.message), type(rio_warning.message))  # type: ignore\n",
      "/opt/homebrew/Caskroom/miniforge/base/envs/pangeo/lib/python3.9/site-packages/rasterio/__init__.py:277: NotGeoreferencedWarning: Dataset has no geotransform, gcps, or rpcs. The identity matrix will be returned.\n",
      "  dataset = DatasetReader(path, driver=driver, sharing=sharing, **kwargs)\n",
      "/opt/homebrew/Caskroom/miniforge/base/envs/pangeo/lib/python3.9/site-packages/rioxarray/_io.py:851: NotGeoreferencedWarning: Dataset has no geotransform, gcps, or rpcs. The identity matrix will be returned.\n",
      "  warnings.warn(str(rio_warning.message), type(rio_warning.message))  # type: ignore\n",
      "/opt/homebrew/Caskroom/miniforge/base/envs/pangeo/lib/python3.9/site-packages/rasterio/__init__.py:277: NotGeoreferencedWarning: Dataset has no geotransform, gcps, or rpcs. The identity matrix will be returned.\n",
      "  dataset = DatasetReader(path, driver=driver, sharing=sharing, **kwargs)\n",
      "/opt/homebrew/Caskroom/miniforge/base/envs/pangeo/lib/python3.9/site-packages/rioxarray/_io.py:851: NotGeoreferencedWarning: Dataset has no geotransform, gcps, or rpcs. The identity matrix will be returned.\n",
      "  warnings.warn(str(rio_warning.message), type(rio_warning.message))  # type: ignore\n",
      "/opt/homebrew/Caskroom/miniforge/base/envs/pangeo/lib/python3.9/site-packages/rasterio/__init__.py:277: NotGeoreferencedWarning: Dataset has no geotransform, gcps, or rpcs. The identity matrix will be returned.\n",
      "  dataset = DatasetReader(path, driver=driver, sharing=sharing, **kwargs)\n",
      "/opt/homebrew/Caskroom/miniforge/base/envs/pangeo/lib/python3.9/site-packages/rioxarray/_io.py:851: NotGeoreferencedWarning: Dataset has no geotransform, gcps, or rpcs. The identity matrix will be returned.\n",
      "  warnings.warn(str(rio_warning.message), type(rio_warning.message))  # type: ignore\n",
      "/opt/homebrew/Caskroom/miniforge/base/envs/pangeo/lib/python3.9/site-packages/rasterio/__init__.py:277: NotGeoreferencedWarning: Dataset has no geotransform, gcps, or rpcs. The identity matrix will be returned.\n",
      "  dataset = DatasetReader(path, driver=driver, sharing=sharing, **kwargs)\n",
      "/opt/homebrew/Caskroom/miniforge/base/envs/pangeo/lib/python3.9/site-packages/rioxarray/_io.py:851: NotGeoreferencedWarning: Dataset has no geotransform, gcps, or rpcs. The identity matrix will be returned.\n",
      "  warnings.warn(str(rio_warning.message), type(rio_warning.message))  # type: ignore\n",
      "/opt/homebrew/Caskroom/miniforge/base/envs/pangeo/lib/python3.9/site-packages/rasterio/__init__.py:277: NotGeoreferencedWarning: Dataset has no geotransform, gcps, or rpcs. The identity matrix will be returned.\n",
      "  dataset = DatasetReader(path, driver=driver, sharing=sharing, **kwargs)\n",
      "/opt/homebrew/Caskroom/miniforge/base/envs/pangeo/lib/python3.9/site-packages/rioxarray/_io.py:851: NotGeoreferencedWarning: Dataset has no geotransform, gcps, or rpcs. The identity matrix will be returned.\n",
      "  warnings.warn(str(rio_warning.message), type(rio_warning.message))  # type: ignore\n",
      "/opt/homebrew/Caskroom/miniforge/base/envs/pangeo/lib/python3.9/site-packages/rasterio/__init__.py:277: NotGeoreferencedWarning: Dataset has no geotransform, gcps, or rpcs. The identity matrix will be returned.\n",
      "  dataset = DatasetReader(path, driver=driver, sharing=sharing, **kwargs)\n",
      "/opt/homebrew/Caskroom/miniforge/base/envs/pangeo/lib/python3.9/site-packages/rioxarray/_io.py:851: NotGeoreferencedWarning: Dataset has no geotransform, gcps, or rpcs. The identity matrix will be returned.\n",
      "  warnings.warn(str(rio_warning.message), type(rio_warning.message))  # type: ignore\n",
      "/opt/homebrew/Caskroom/miniforge/base/envs/pangeo/lib/python3.9/site-packages/rasterio/__init__.py:277: NotGeoreferencedWarning: Dataset has no geotransform, gcps, or rpcs. The identity matrix will be returned.\n",
      "  dataset = DatasetReader(path, driver=driver, sharing=sharing, **kwargs)\n",
      "/opt/homebrew/Caskroom/miniforge/base/envs/pangeo/lib/python3.9/site-packages/rioxarray/_io.py:851: NotGeoreferencedWarning: Dataset has no geotransform, gcps, or rpcs. The identity matrix will be returned.\n",
      "  warnings.warn(str(rio_warning.message), type(rio_warning.message))  # type: ignore\n",
      "/opt/homebrew/Caskroom/miniforge/base/envs/pangeo/lib/python3.9/site-packages/rasterio/__init__.py:277: NotGeoreferencedWarning: Dataset has no geotransform, gcps, or rpcs. The identity matrix will be returned.\n",
      "  dataset = DatasetReader(path, driver=driver, sharing=sharing, **kwargs)\n",
      "/opt/homebrew/Caskroom/miniforge/base/envs/pangeo/lib/python3.9/site-packages/rioxarray/_io.py:851: NotGeoreferencedWarning: Dataset has no geotransform, gcps, or rpcs. The identity matrix will be returned.\n",
      "  warnings.warn(str(rio_warning.message), type(rio_warning.message))  # type: ignore\n",
      "/opt/homebrew/Caskroom/miniforge/base/envs/pangeo/lib/python3.9/site-packages/rasterio/__init__.py:277: NotGeoreferencedWarning: Dataset has no geotransform, gcps, or rpcs. The identity matrix will be returned.\n",
      "  dataset = DatasetReader(path, driver=driver, sharing=sharing, **kwargs)\n",
      "/opt/homebrew/Caskroom/miniforge/base/envs/pangeo/lib/python3.9/site-packages/rioxarray/_io.py:851: NotGeoreferencedWarning: Dataset has no geotransform, gcps, or rpcs. The identity matrix will be returned.\n",
      "  warnings.warn(str(rio_warning.message), type(rio_warning.message))  # type: ignore\n",
      "/opt/homebrew/Caskroom/miniforge/base/envs/pangeo/lib/python3.9/site-packages/rasterio/__init__.py:277: NotGeoreferencedWarning: Dataset has no geotransform, gcps, or rpcs. The identity matrix will be returned.\n",
      "  dataset = DatasetReader(path, driver=driver, sharing=sharing, **kwargs)\n",
      "/opt/homebrew/Caskroom/miniforge/base/envs/pangeo/lib/python3.9/site-packages/rioxarray/_io.py:851: NotGeoreferencedWarning: Dataset has no geotransform, gcps, or rpcs. The identity matrix will be returned.\n",
      "  warnings.warn(str(rio_warning.message), type(rio_warning.message))  # type: ignore\n",
      "/opt/homebrew/Caskroom/miniforge/base/envs/pangeo/lib/python3.9/site-packages/rasterio/__init__.py:277: NotGeoreferencedWarning: Dataset has no geotransform, gcps, or rpcs. The identity matrix will be returned.\n",
      "  dataset = DatasetReader(path, driver=driver, sharing=sharing, **kwargs)\n",
      "/opt/homebrew/Caskroom/miniforge/base/envs/pangeo/lib/python3.9/site-packages/rioxarray/_io.py:851: NotGeoreferencedWarning: Dataset has no geotransform, gcps, or rpcs. The identity matrix will be returned.\n",
      "  warnings.warn(str(rio_warning.message), type(rio_warning.message))  # type: ignore\n",
      "/opt/homebrew/Caskroom/miniforge/base/envs/pangeo/lib/python3.9/site-packages/rasterio/__init__.py:277: NotGeoreferencedWarning: Dataset has no geotransform, gcps, or rpcs. The identity matrix will be returned.\n",
      "  dataset = DatasetReader(path, driver=driver, sharing=sharing, **kwargs)\n",
      "/opt/homebrew/Caskroom/miniforge/base/envs/pangeo/lib/python3.9/site-packages/rioxarray/_io.py:851: NotGeoreferencedWarning: Dataset has no geotransform, gcps, or rpcs. The identity matrix will be returned.\n",
      "  warnings.warn(str(rio_warning.message), type(rio_warning.message))  # type: ignore\n",
      "/opt/homebrew/Caskroom/miniforge/base/envs/pangeo/lib/python3.9/site-packages/rasterio/__init__.py:277: NotGeoreferencedWarning: Dataset has no geotransform, gcps, or rpcs. The identity matrix will be returned.\n",
      "  dataset = DatasetReader(path, driver=driver, sharing=sharing, **kwargs)\n",
      "/opt/homebrew/Caskroom/miniforge/base/envs/pangeo/lib/python3.9/site-packages/rioxarray/_io.py:851: NotGeoreferencedWarning: Dataset has no geotransform, gcps, or rpcs. The identity matrix will be returned.\n",
      "  warnings.warn(str(rio_warning.message), type(rio_warning.message))  # type: ignore\n",
      "/opt/homebrew/Caskroom/miniforge/base/envs/pangeo/lib/python3.9/site-packages/rasterio/__init__.py:277: NotGeoreferencedWarning: Dataset has no geotransform, gcps, or rpcs. The identity matrix will be returned.\n",
      "  dataset = DatasetReader(path, driver=driver, sharing=sharing, **kwargs)\n",
      "/opt/homebrew/Caskroom/miniforge/base/envs/pangeo/lib/python3.9/site-packages/rioxarray/_io.py:851: NotGeoreferencedWarning: Dataset has no geotransform, gcps, or rpcs. The identity matrix will be returned.\n",
      "  warnings.warn(str(rio_warning.message), type(rio_warning.message))  # type: ignore\n",
      "/opt/homebrew/Caskroom/miniforge/base/envs/pangeo/lib/python3.9/site-packages/rasterio/__init__.py:277: NotGeoreferencedWarning: Dataset has no geotransform, gcps, or rpcs. The identity matrix will be returned.\n",
      "  dataset = DatasetReader(path, driver=driver, sharing=sharing, **kwargs)\n",
      "/opt/homebrew/Caskroom/miniforge/base/envs/pangeo/lib/python3.9/site-packages/rioxarray/_io.py:851: NotGeoreferencedWarning: Dataset has no geotransform, gcps, or rpcs. The identity matrix will be returned.\n",
      "  warnings.warn(str(rio_warning.message), type(rio_warning.message))  # type: ignore\n",
      "/opt/homebrew/Caskroom/miniforge/base/envs/pangeo/lib/python3.9/site-packages/rasterio/__init__.py:277: NotGeoreferencedWarning: Dataset has no geotransform, gcps, or rpcs. The identity matrix will be returned.\n",
      "  dataset = DatasetReader(path, driver=driver, sharing=sharing, **kwargs)\n",
      "/opt/homebrew/Caskroom/miniforge/base/envs/pangeo/lib/python3.9/site-packages/rioxarray/_io.py:851: NotGeoreferencedWarning: Dataset has no geotransform, gcps, or rpcs. The identity matrix will be returned.\n",
      "  warnings.warn(str(rio_warning.message), type(rio_warning.message))  # type: ignore\n",
      "/opt/homebrew/Caskroom/miniforge/base/envs/pangeo/lib/python3.9/site-packages/rasterio/__init__.py:277: NotGeoreferencedWarning: Dataset has no geotransform, gcps, or rpcs. The identity matrix will be returned.\n",
      "  dataset = DatasetReader(path, driver=driver, sharing=sharing, **kwargs)\n",
      "/opt/homebrew/Caskroom/miniforge/base/envs/pangeo/lib/python3.9/site-packages/rioxarray/_io.py:851: NotGeoreferencedWarning: Dataset has no geotransform, gcps, or rpcs. The identity matrix will be returned.\n",
      "  warnings.warn(str(rio_warning.message), type(rio_warning.message))  # type: ignore\n",
      "/opt/homebrew/Caskroom/miniforge/base/envs/pangeo/lib/python3.9/site-packages/rasterio/__init__.py:277: NotGeoreferencedWarning: Dataset has no geotransform, gcps, or rpcs. The identity matrix will be returned.\n",
      "  dataset = DatasetReader(path, driver=driver, sharing=sharing, **kwargs)\n",
      "/opt/homebrew/Caskroom/miniforge/base/envs/pangeo/lib/python3.9/site-packages/rioxarray/_io.py:851: NotGeoreferencedWarning: Dataset has no geotransform, gcps, or rpcs. The identity matrix will be returned.\n",
      "  warnings.warn(str(rio_warning.message), type(rio_warning.message))  # type: ignore\n",
      "/opt/homebrew/Caskroom/miniforge/base/envs/pangeo/lib/python3.9/site-packages/rasterio/__init__.py:277: NotGeoreferencedWarning: Dataset has no geotransform, gcps, or rpcs. The identity matrix will be returned.\n",
      "  dataset = DatasetReader(path, driver=driver, sharing=sharing, **kwargs)\n",
      "/opt/homebrew/Caskroom/miniforge/base/envs/pangeo/lib/python3.9/site-packages/rioxarray/_io.py:851: NotGeoreferencedWarning: Dataset has no geotransform, gcps, or rpcs. The identity matrix will be returned.\n",
      "  warnings.warn(str(rio_warning.message), type(rio_warning.message))  # type: ignore\n",
      "/opt/homebrew/Caskroom/miniforge/base/envs/pangeo/lib/python3.9/site-packages/rasterio/__init__.py:277: NotGeoreferencedWarning: Dataset has no geotransform, gcps, or rpcs. The identity matrix will be returned.\n",
      "  dataset = DatasetReader(path, driver=driver, sharing=sharing, **kwargs)\n",
      "/opt/homebrew/Caskroom/miniforge/base/envs/pangeo/lib/python3.9/site-packages/rioxarray/_io.py:851: NotGeoreferencedWarning: Dataset has no geotransform, gcps, or rpcs. The identity matrix will be returned.\n",
      "  warnings.warn(str(rio_warning.message), type(rio_warning.message))  # type: ignore\n",
      "/opt/homebrew/Caskroom/miniforge/base/envs/pangeo/lib/python3.9/site-packages/rasterio/__init__.py:277: NotGeoreferencedWarning: Dataset has no geotransform, gcps, or rpcs. The identity matrix will be returned.\n",
      "  dataset = DatasetReader(path, driver=driver, sharing=sharing, **kwargs)\n",
      "/opt/homebrew/Caskroom/miniforge/base/envs/pangeo/lib/python3.9/site-packages/rioxarray/_io.py:851: NotGeoreferencedWarning: Dataset has no geotransform, gcps, or rpcs. The identity matrix will be returned.\n",
      "  warnings.warn(str(rio_warning.message), type(rio_warning.message))  # type: ignore\n",
      "/opt/homebrew/Caskroom/miniforge/base/envs/pangeo/lib/python3.9/site-packages/rasterio/__init__.py:277: NotGeoreferencedWarning: Dataset has no geotransform, gcps, or rpcs. The identity matrix will be returned.\n",
      "  dataset = DatasetReader(path, driver=driver, sharing=sharing, **kwargs)\n",
      "/opt/homebrew/Caskroom/miniforge/base/envs/pangeo/lib/python3.9/site-packages/rioxarray/_io.py:851: NotGeoreferencedWarning: Dataset has no geotransform, gcps, or rpcs. The identity matrix will be returned.\n",
      "  warnings.warn(str(rio_warning.message), type(rio_warning.message))  # type: ignore\n",
      "/opt/homebrew/Caskroom/miniforge/base/envs/pangeo/lib/python3.9/site-packages/rasterio/__init__.py:277: NotGeoreferencedWarning: Dataset has no geotransform, gcps, or rpcs. The identity matrix will be returned.\n",
      "  dataset = DatasetReader(path, driver=driver, sharing=sharing, **kwargs)\n",
      "/opt/homebrew/Caskroom/miniforge/base/envs/pangeo/lib/python3.9/site-packages/rioxarray/_io.py:851: NotGeoreferencedWarning: Dataset has no geotransform, gcps, or rpcs. The identity matrix will be returned.\n",
      "  warnings.warn(str(rio_warning.message), type(rio_warning.message))  # type: ignore\n",
      "/opt/homebrew/Caskroom/miniforge/base/envs/pangeo/lib/python3.9/site-packages/rasterio/__init__.py:277: NotGeoreferencedWarning: Dataset has no geotransform, gcps, or rpcs. The identity matrix will be returned.\n",
      "  dataset = DatasetReader(path, driver=driver, sharing=sharing, **kwargs)\n",
      "/opt/homebrew/Caskroom/miniforge/base/envs/pangeo/lib/python3.9/site-packages/rioxarray/_io.py:851: NotGeoreferencedWarning: Dataset has no geotransform, gcps, or rpcs. The identity matrix will be returned.\n",
      "  warnings.warn(str(rio_warning.message), type(rio_warning.message))  # type: ignore\n",
      "/opt/homebrew/Caskroom/miniforge/base/envs/pangeo/lib/python3.9/site-packages/rasterio/__init__.py:277: NotGeoreferencedWarning: Dataset has no geotransform, gcps, or rpcs. The identity matrix will be returned.\n",
      "  dataset = DatasetReader(path, driver=driver, sharing=sharing, **kwargs)\n",
      "/opt/homebrew/Caskroom/miniforge/base/envs/pangeo/lib/python3.9/site-packages/rioxarray/_io.py:851: NotGeoreferencedWarning: Dataset has no geotransform, gcps, or rpcs. The identity matrix will be returned.\n",
      "  warnings.warn(str(rio_warning.message), type(rio_warning.message))  # type: ignore\n",
      "/opt/homebrew/Caskroom/miniforge/base/envs/pangeo/lib/python3.9/site-packages/rasterio/__init__.py:277: NotGeoreferencedWarning: Dataset has no geotransform, gcps, or rpcs. The identity matrix will be returned.\n",
      "  dataset = DatasetReader(path, driver=driver, sharing=sharing, **kwargs)\n",
      "/opt/homebrew/Caskroom/miniforge/base/envs/pangeo/lib/python3.9/site-packages/rioxarray/_io.py:851: NotGeoreferencedWarning: Dataset has no geotransform, gcps, or rpcs. The identity matrix will be returned.\n",
      "  warnings.warn(str(rio_warning.message), type(rio_warning.message))  # type: ignore\n",
      "/opt/homebrew/Caskroom/miniforge/base/envs/pangeo/lib/python3.9/site-packages/rasterio/__init__.py:277: NotGeoreferencedWarning: Dataset has no geotransform, gcps, or rpcs. The identity matrix will be returned.\n",
      "  dataset = DatasetReader(path, driver=driver, sharing=sharing, **kwargs)\n",
      "/opt/homebrew/Caskroom/miniforge/base/envs/pangeo/lib/python3.9/site-packages/rioxarray/_io.py:851: NotGeoreferencedWarning: Dataset has no geotransform, gcps, or rpcs. The identity matrix will be returned.\n",
      "  warnings.warn(str(rio_warning.message), type(rio_warning.message))  # type: ignore\n",
      "/opt/homebrew/Caskroom/miniforge/base/envs/pangeo/lib/python3.9/site-packages/rasterio/__init__.py:277: NotGeoreferencedWarning: Dataset has no geotransform, gcps, or rpcs. The identity matrix will be returned.\n",
      "  dataset = DatasetReader(path, driver=driver, sharing=sharing, **kwargs)\n",
      "/opt/homebrew/Caskroom/miniforge/base/envs/pangeo/lib/python3.9/site-packages/rioxarray/_io.py:851: NotGeoreferencedWarning: Dataset has no geotransform, gcps, or rpcs. The identity matrix will be returned.\n",
      "  warnings.warn(str(rio_warning.message), type(rio_warning.message))  # type: ignore\n",
      "/opt/homebrew/Caskroom/miniforge/base/envs/pangeo/lib/python3.9/site-packages/rasterio/__init__.py:277: NotGeoreferencedWarning: Dataset has no geotransform, gcps, or rpcs. The identity matrix will be returned.\n",
      "  dataset = DatasetReader(path, driver=driver, sharing=sharing, **kwargs)\n",
      "/opt/homebrew/Caskroom/miniforge/base/envs/pangeo/lib/python3.9/site-packages/rioxarray/_io.py:851: NotGeoreferencedWarning: Dataset has no geotransform, gcps, or rpcs. The identity matrix will be returned.\n",
      "  warnings.warn(str(rio_warning.message), type(rio_warning.message))  # type: ignore\n",
      "/opt/homebrew/Caskroom/miniforge/base/envs/pangeo/lib/python3.9/site-packages/rasterio/__init__.py:277: NotGeoreferencedWarning: Dataset has no geotransform, gcps, or rpcs. The identity matrix will be returned.\n",
      "  dataset = DatasetReader(path, driver=driver, sharing=sharing, **kwargs)\n",
      "/opt/homebrew/Caskroom/miniforge/base/envs/pangeo/lib/python3.9/site-packages/rioxarray/_io.py:851: NotGeoreferencedWarning: Dataset has no geotransform, gcps, or rpcs. The identity matrix will be returned.\n",
      "  warnings.warn(str(rio_warning.message), type(rio_warning.message))  # type: ignore\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/Caskroom/miniforge/base/envs/pangeo/lib/python3.9/site-packages/rasterio/__init__.py:277: NotGeoreferencedWarning: Dataset has no geotransform, gcps, or rpcs. The identity matrix will be returned.\n",
      "  dataset = DatasetReader(path, driver=driver, sharing=sharing, **kwargs)\n",
      "/opt/homebrew/Caskroom/miniforge/base/envs/pangeo/lib/python3.9/site-packages/rioxarray/_io.py:851: NotGeoreferencedWarning: Dataset has no geotransform, gcps, or rpcs. The identity matrix will be returned.\n",
      "  warnings.warn(str(rio_warning.message), type(rio_warning.message))  # type: ignore\n",
      "/opt/homebrew/Caskroom/miniforge/base/envs/pangeo/lib/python3.9/site-packages/rasterio/__init__.py:277: NotGeoreferencedWarning: Dataset has no geotransform, gcps, or rpcs. The identity matrix will be returned.\n",
      "  dataset = DatasetReader(path, driver=driver, sharing=sharing, **kwargs)\n",
      "/opt/homebrew/Caskroom/miniforge/base/envs/pangeo/lib/python3.9/site-packages/rioxarray/_io.py:851: NotGeoreferencedWarning: Dataset has no geotransform, gcps, or rpcs. The identity matrix will be returned.\n",
      "  warnings.warn(str(rio_warning.message), type(rio_warning.message))  # type: ignore\n",
      "/opt/homebrew/Caskroom/miniforge/base/envs/pangeo/lib/python3.9/site-packages/rasterio/__init__.py:277: NotGeoreferencedWarning: Dataset has no geotransform, gcps, or rpcs. The identity matrix will be returned.\n",
      "  dataset = DatasetReader(path, driver=driver, sharing=sharing, **kwargs)\n",
      "/opt/homebrew/Caskroom/miniforge/base/envs/pangeo/lib/python3.9/site-packages/rioxarray/_io.py:851: NotGeoreferencedWarning: Dataset has no geotransform, gcps, or rpcs. The identity matrix will be returned.\n",
      "  warnings.warn(str(rio_warning.message), type(rio_warning.message))  # type: ignore\n"
     ]
    }
   ],
   "source": [
    "caliop_raster = rxr.open_rasterio(caliop_file)\n",
    "caliop_raster = caliop_raster[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "ae6636e8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on Dataset in module xarray.core.dataset object:\n",
      "\n",
      "class Dataset(xarray.core.common.DataWithCoords, xarray.core._reductions.DatasetReductions, xarray.core.arithmetic.DatasetArithmetic, collections.abc.Mapping, typing.Generic)\n",
      " |  Dataset(data_vars: 'Mapping[Any, Any] | None' = None, coords: 'Mapping[Any, Any] | None' = None, attrs: 'Mapping[Any, Any] | None' = None) -> 'None'\n",
      " |  \n",
      " |  A multi-dimensional, in memory, array database.\n",
      " |  \n",
      " |  A dataset resembles an in-memory representation of a NetCDF file,\n",
      " |  and consists of variables, coordinates and attributes which\n",
      " |  together form a self describing dataset.\n",
      " |  \n",
      " |  Dataset implements the mapping interface with keys given by variable\n",
      " |  names and values given by DataArray objects for each variable name.\n",
      " |  \n",
      " |  One dimensional variables with name equal to their dimension are\n",
      " |  index coordinates used for label based indexing.\n",
      " |  \n",
      " |  To load data from a file or file-like object, use the `open_dataset`\n",
      " |  function.\n",
      " |  \n",
      " |  Parameters\n",
      " |  ----------\n",
      " |  data_vars : dict-like, optional\n",
      " |      A mapping from variable names to :py:class:`~xarray.DataArray`\n",
      " |      objects, :py:class:`~xarray.Variable` objects or to tuples of\n",
      " |      the form ``(dims, data[, attrs])`` which can be used as\n",
      " |      arguments to create a new ``Variable``. Each dimension must\n",
      " |      have the same length in all variables in which it appears.\n",
      " |  \n",
      " |      The following notations are accepted:\n",
      " |  \n",
      " |      - mapping {var name: DataArray}\n",
      " |      - mapping {var name: Variable}\n",
      " |      - mapping {var name: (dimension name, array-like)}\n",
      " |      - mapping {var name: (tuple of dimension names, array-like)}\n",
      " |      - mapping {dimension name: array-like}\n",
      " |        (it will be automatically moved to coords, see below)\n",
      " |  \n",
      " |      Each dimension must have the same length in all variables in\n",
      " |      which it appears.\n",
      " |  coords : dict-like, optional\n",
      " |      Another mapping in similar form as the `data_vars` argument,\n",
      " |      except the each item is saved on the dataset as a \"coordinate\".\n",
      " |      These variables have an associated meaning: they describe\n",
      " |      constant/fixed/independent quantities, unlike the\n",
      " |      varying/measured/dependent quantities that belong in\n",
      " |      `variables`. Coordinates values may be given by 1-dimensional\n",
      " |      arrays or scalars, in which case `dims` do not need to be\n",
      " |      supplied: 1D arrays will be assumed to give index values along\n",
      " |      the dimension with the same name.\n",
      " |  \n",
      " |      The following notations are accepted:\n",
      " |  \n",
      " |      - mapping {coord name: DataArray}\n",
      " |      - mapping {coord name: Variable}\n",
      " |      - mapping {coord name: (dimension name, array-like)}\n",
      " |      - mapping {coord name: (tuple of dimension names, array-like)}\n",
      " |      - mapping {dimension name: array-like}\n",
      " |        (the dimension name is implicitly set to be the same as the\n",
      " |        coord name)\n",
      " |  \n",
      " |      The last notation implies that the coord name is the same as\n",
      " |      the dimension name.\n",
      " |  \n",
      " |  attrs : dict-like, optional\n",
      " |      Global attributes to save on this dataset.\n",
      " |  \n",
      " |  Examples\n",
      " |  --------\n",
      " |  Create data:\n",
      " |  \n",
      " |  >>> np.random.seed(0)\n",
      " |  >>> temperature = 15 + 8 * np.random.randn(2, 2, 3)\n",
      " |  >>> precipitation = 10 * np.random.rand(2, 2, 3)\n",
      " |  >>> lon = [[-99.83, -99.32], [-99.79, -99.23]]\n",
      " |  >>> lat = [[42.25, 42.21], [42.63, 42.59]]\n",
      " |  >>> time = pd.date_range(\"2014-09-06\", periods=3)\n",
      " |  >>> reference_time = pd.Timestamp(\"2014-09-05\")\n",
      " |  \n",
      " |  Initialize a dataset with multiple dimensions:\n",
      " |  \n",
      " |  >>> ds = xr.Dataset(\n",
      " |  ...     data_vars=dict(\n",
      " |  ...         temperature=([\"x\", \"y\", \"time\"], temperature),\n",
      " |  ...         precipitation=([\"x\", \"y\", \"time\"], precipitation),\n",
      " |  ...     ),\n",
      " |  ...     coords=dict(\n",
      " |  ...         lon=([\"x\", \"y\"], lon),\n",
      " |  ...         lat=([\"x\", \"y\"], lat),\n",
      " |  ...         time=time,\n",
      " |  ...         reference_time=reference_time,\n",
      " |  ...     ),\n",
      " |  ...     attrs=dict(description=\"Weather related data.\"),\n",
      " |  ... )\n",
      " |  >>> ds\n",
      " |  <xarray.Dataset>\n",
      " |  Dimensions:         (x: 2, y: 2, time: 3)\n",
      " |  Coordinates:\n",
      " |      lon             (x, y) float64 -99.83 -99.32 -99.79 -99.23\n",
      " |      lat             (x, y) float64 42.25 42.21 42.63 42.59\n",
      " |    * time            (time) datetime64[ns] 2014-09-06 2014-09-07 2014-09-08\n",
      " |      reference_time  datetime64[ns] 2014-09-05\n",
      " |  Dimensions without coordinates: x, y\n",
      " |  Data variables:\n",
      " |      temperature     (x, y, time) float64 29.11 18.2 22.83 ... 18.28 16.15 26.63\n",
      " |      precipitation   (x, y, time) float64 5.68 9.256 0.7104 ... 7.992 4.615 7.805\n",
      " |  Attributes:\n",
      " |      description:  Weather related data.\n",
      " |  \n",
      " |  Find out where the coldest temperature was and what values the\n",
      " |  other variables had:\n",
      " |  \n",
      " |  >>> ds.isel(ds.temperature.argmin(...))\n",
      " |  <xarray.Dataset>\n",
      " |  Dimensions:         ()\n",
      " |  Coordinates:\n",
      " |      lon             float64 -99.32\n",
      " |      lat             float64 42.21\n",
      " |      time            datetime64[ns] 2014-09-08\n",
      " |      reference_time  datetime64[ns] 2014-09-05\n",
      " |  Data variables:\n",
      " |      temperature     float64 7.182\n",
      " |      precipitation   float64 8.326\n",
      " |  Attributes:\n",
      " |      description:  Weather related data.\n",
      " |  \n",
      " |  Method resolution order:\n",
      " |      Dataset\n",
      " |      xarray.core.common.DataWithCoords\n",
      " |      xarray.core.common.AttrAccessMixin\n",
      " |      xarray.core._reductions.DatasetReductions\n",
      " |      xarray.core.arithmetic.DatasetArithmetic\n",
      " |      xarray.core.common.ImplementsDatasetReduce\n",
      " |      xarray.core.ops.IncludeCumMethods\n",
      " |      xarray.core.arithmetic.SupportsArithmetic\n",
      " |      xarray.core._typed_ops.DatasetOpsMixin\n",
      " |      collections.abc.Mapping\n",
      " |      collections.abc.Collection\n",
      " |      collections.abc.Sized\n",
      " |      collections.abc.Iterable\n",
      " |      collections.abc.Container\n",
      " |      typing.Generic\n",
      " |      builtins.object\n",
      " |  \n",
      " |  Methods defined here:\n",
      " |  \n",
      " |  __array__(self, dtype=None)\n",
      " |  \n",
      " |  __bool__(self) -> 'bool'\n",
      " |  \n",
      " |  __contains__(self, key: 'object') -> 'bool'\n",
      " |      The 'in' operator will return true or false depending on whether\n",
      " |      'key' is an array in the dataset or not.\n",
      " |  \n",
      " |  __copy__(self: 'T_Dataset') -> 'T_Dataset'\n",
      " |  \n",
      " |  __dask_graph__(self)\n",
      " |  \n",
      " |  __dask_keys__(self)\n",
      " |  \n",
      " |  __dask_layers__(self)\n",
      " |  \n",
      " |  __dask_postcompute__(self)\n",
      " |  \n",
      " |  __dask_postpersist__(self)\n",
      " |  \n",
      " |  __dask_tokenize__(self)\n",
      " |  \n",
      " |  __deepcopy__(self: 'T_Dataset', memo=None) -> 'T_Dataset'\n",
      " |  \n",
      " |  __delitem__(self, key: 'Hashable') -> 'None'\n",
      " |      Remove a variable from this dataset.\n",
      " |  \n",
      " |  __getitem__(self: 'T_Dataset', key: 'Mapping[Any, Any] | Hashable | Iterable[Hashable]') -> 'T_Dataset | DataArray'\n",
      " |      Access variables or coordinates of this dataset as a\n",
      " |      :py:class:`~xarray.DataArray` or a subset of variables or a indexed dataset.\n",
      " |      \n",
      " |      Indexing with a list of names will return a new ``Dataset`` object.\n",
      " |  \n",
      " |  __init__(self, data_vars: 'Mapping[Any, Any] | None' = None, coords: 'Mapping[Any, Any] | None' = None, attrs: 'Mapping[Any, Any] | None' = None) -> 'None'\n",
      " |      Initialize self.  See help(type(self)) for accurate signature.\n",
      " |  \n",
      " |  __iter__(self) -> 'Iterator[Hashable]'\n",
      " |  \n",
      " |  __len__(self) -> 'int'\n",
      " |  \n",
      " |  __repr__(self) -> 'str'\n",
      " |      Return repr(self).\n",
      " |  \n",
      " |  __setitem__(self, key: 'Hashable | Iterable[Hashable] | Mapping', value: 'Any') -> 'None'\n",
      " |      Add an array to this dataset.\n",
      " |      Multiple arrays can be added at the same time, in which case each of\n",
      " |      the following operations is applied to the respective value.\n",
      " |      \n",
      " |      If key is dict-like, update all variables in the dataset\n",
      " |      one by one with the given value at the given location.\n",
      " |      If the given value is also a dataset, select corresponding variables\n",
      " |      in the given value and in the dataset to be changed.\n",
      " |      \n",
      " |      If value is a `\n",
      " |      from .dataarray import DataArray`, call its `select_vars()` method, rename it\n",
      " |      to `key` and merge the contents of the resulting dataset into this\n",
      " |      dataset.\n",
      " |      \n",
      " |      If value is a `Variable` object (or tuple of form\n",
      " |      ``(dims, data[, attrs])``), add it to this dataset as a new\n",
      " |      variable.\n",
      " |  \n",
      " |  apply(self: 'T_Dataset', func: 'Callable', keep_attrs: 'bool | None' = None, args: 'Iterable[Any]' = (), **kwargs: 'Any') -> 'T_Dataset'\n",
      " |      Backward compatible implementation of ``map``\n",
      " |      \n",
      " |      See Also\n",
      " |      --------\n",
      " |      Dataset.map\n",
      " |  \n",
      " |  argmax(self: 'T_Dataset', dim: 'Hashable | None' = None, **kwargs) -> 'T_Dataset'\n",
      " |      Indices of the maxima of the member variables.\n",
      " |      \n",
      " |      If there are multiple maxima, the indices of the first one found will be\n",
      " |      returned.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      dim : str, optional\n",
      " |          The dimension over which to find the maximum. By default, finds maximum over\n",
      " |          all dimensions - for now returning an int for backward compatibility, but\n",
      " |          this is deprecated, in future will be an error, since DataArray.argmax will\n",
      " |          return a dict with indices for all dimensions, which does not make sense for\n",
      " |          a Dataset.\n",
      " |      keep_attrs : bool, optional\n",
      " |          If True, the attributes (`attrs`) will be copied from the original\n",
      " |          object to the new one.  If False (default), the new object will be\n",
      " |          returned without attributes.\n",
      " |      skipna : bool, optional\n",
      " |          If True, skip missing values (as marked by NaN). By default, only\n",
      " |          skips missing values for float dtypes; other dtypes either do not\n",
      " |          have a sentinel missing value (int) or skipna=True has not been\n",
      " |          implemented (object, datetime64 or timedelta64).\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      result : Dataset\n",
      " |      \n",
      " |      See Also\n",
      " |      --------\n",
      " |      DataArray.argmax\n",
      " |  \n",
      " |  argmin(self: 'T_Dataset', dim: 'Hashable | None' = None, **kwargs) -> 'T_Dataset'\n",
      " |      Indices of the minima of the member variables.\n",
      " |      \n",
      " |      If there are multiple minima, the indices of the first one found will be\n",
      " |      returned.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      dim : Hashable, optional\n",
      " |          The dimension over which to find the minimum. By default, finds minimum over\n",
      " |          all dimensions - for now returning an int for backward compatibility, but\n",
      " |          this is deprecated, in future will be an error, since DataArray.argmin will\n",
      " |          return a dict with indices for all dimensions, which does not make sense for\n",
      " |          a Dataset.\n",
      " |      keep_attrs : bool, optional\n",
      " |          If True, the attributes (`attrs`) will be copied from the original\n",
      " |          object to the new one.  If False (default), the new object will be\n",
      " |          returned without attributes.\n",
      " |      skipna : bool, optional\n",
      " |          If True, skip missing values (as marked by NaN). By default, only\n",
      " |          skips missing values for float dtypes; other dtypes either do not\n",
      " |          have a sentinel missing value (int) or skipna=True has not been\n",
      " |          implemented (object, datetime64 or timedelta64).\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      result : Dataset\n",
      " |      \n",
      " |      See Also\n",
      " |      --------\n",
      " |      DataArray.argmin\n",
      " |  \n",
      " |  as_numpy(self: 'T_Dataset') -> 'T_Dataset'\n",
      " |      Coerces wrapped data and coordinates into numpy arrays, returning a Dataset.\n",
      " |      \n",
      " |      See also\n",
      " |      --------\n",
      " |      DataArray.as_numpy\n",
      " |      DataArray.to_numpy : Returns only the data as a numpy.ndarray object.\n",
      " |  \n",
      " |  assign(self: 'T_Dataset', variables: 'Mapping[Any, Any] | None' = None, **variables_kwargs: 'Any') -> 'T_Dataset'\n",
      " |      Assign new data variables to a Dataset, returning a new object\n",
      " |      with all the original variables in addition to the new ones.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      variables : mapping of hashable to Any\n",
      " |          Mapping from variables names to the new values. If the new values\n",
      " |          are callable, they are computed on the Dataset and assigned to new\n",
      " |          data variables. If the values are not callable, (e.g. a DataArray,\n",
      " |          scalar, or array), they are simply assigned.\n",
      " |      **variables_kwargs\n",
      " |          The keyword arguments form of ``variables``.\n",
      " |          One of variables or variables_kwargs must be provided.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      ds : Dataset\n",
      " |          A new Dataset with the new variables in addition to all the\n",
      " |          existing variables.\n",
      " |      \n",
      " |      Notes\n",
      " |      -----\n",
      " |      Since ``kwargs`` is a dictionary, the order of your arguments may not\n",
      " |      be preserved, and so the order of the new variables is not well\n",
      " |      defined. Assigning multiple variables within the same ``assign`` is\n",
      " |      possible, but you cannot reference other variables created within the\n",
      " |      same ``assign`` call.\n",
      " |      \n",
      " |      See Also\n",
      " |      --------\n",
      " |      pandas.DataFrame.assign\n",
      " |      \n",
      " |      Examples\n",
      " |      --------\n",
      " |      >>> x = xr.Dataset(\n",
      " |      ...     {\n",
      " |      ...         \"temperature_c\": (\n",
      " |      ...             (\"lat\", \"lon\"),\n",
      " |      ...             20 * np.random.rand(4).reshape(2, 2),\n",
      " |      ...         ),\n",
      " |      ...         \"precipitation\": ((\"lat\", \"lon\"), np.random.rand(4).reshape(2, 2)),\n",
      " |      ...     },\n",
      " |      ...     coords={\"lat\": [10, 20], \"lon\": [150, 160]},\n",
      " |      ... )\n",
      " |      >>> x\n",
      " |      <xarray.Dataset>\n",
      " |      Dimensions:        (lat: 2, lon: 2)\n",
      " |      Coordinates:\n",
      " |        * lat            (lat) int64 10 20\n",
      " |        * lon            (lon) int64 150 160\n",
      " |      Data variables:\n",
      " |          temperature_c  (lat, lon) float64 10.98 14.3 12.06 10.9\n",
      " |          precipitation  (lat, lon) float64 0.4237 0.6459 0.4376 0.8918\n",
      " |      \n",
      " |      Where the value is a callable, evaluated on dataset:\n",
      " |      \n",
      " |      >>> x.assign(temperature_f=lambda x: x.temperature_c * 9 / 5 + 32)\n",
      " |      <xarray.Dataset>\n",
      " |      Dimensions:        (lat: 2, lon: 2)\n",
      " |      Coordinates:\n",
      " |        * lat            (lat) int64 10 20\n",
      " |        * lon            (lon) int64 150 160\n",
      " |      Data variables:\n",
      " |          temperature_c  (lat, lon) float64 10.98 14.3 12.06 10.9\n",
      " |          precipitation  (lat, lon) float64 0.4237 0.6459 0.4376 0.8918\n",
      " |          temperature_f  (lat, lon) float64 51.76 57.75 53.7 51.62\n",
      " |      \n",
      " |      Alternatively, the same behavior can be achieved by directly referencing an existing dataarray:\n",
      " |      \n",
      " |      >>> x.assign(temperature_f=x[\"temperature_c\"] * 9 / 5 + 32)\n",
      " |      <xarray.Dataset>\n",
      " |      Dimensions:        (lat: 2, lon: 2)\n",
      " |      Coordinates:\n",
      " |        * lat            (lat) int64 10 20\n",
      " |        * lon            (lon) int64 150 160\n",
      " |      Data variables:\n",
      " |          temperature_c  (lat, lon) float64 10.98 14.3 12.06 10.9\n",
      " |          precipitation  (lat, lon) float64 0.4237 0.6459 0.4376 0.8918\n",
      " |          temperature_f  (lat, lon) float64 51.76 57.75 53.7 51.62\n",
      " |  \n",
      " |  bfill(self: 'T_Dataset', dim: 'Hashable', limit: 'int | None' = None) -> 'T_Dataset'\n",
      " |      Fill NaN values by propagating values backward\n",
      " |      \n",
      " |      *Requires bottleneck.*\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      dim : Hashable\n",
      " |          Specifies the dimension along which to propagate values when\n",
      " |          filling.\n",
      " |      limit : int or None, optional\n",
      " |          The maximum number of consecutive NaN values to backward fill. In\n",
      " |          other words, if there is a gap with more than this number of\n",
      " |          consecutive NaNs, it will only be partially filled. Must be greater\n",
      " |          than 0 or None for no limit. Must be None or greater than or equal\n",
      " |          to axis length if filling along chunked axes (dimensions).\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      Dataset\n",
      " |  \n",
      " |  broadcast_equals(self, other: 'Dataset') -> 'bool'\n",
      " |      Two Datasets are broadcast equal if they are equal after\n",
      " |      broadcasting all variables against each other.\n",
      " |      \n",
      " |      For example, variables that are scalar in one dataset but non-scalar in\n",
      " |      the other dataset can still be broadcast equal if the the non-scalar\n",
      " |      variable is a constant.\n",
      " |      \n",
      " |      See Also\n",
      " |      --------\n",
      " |      Dataset.equals\n",
      " |      Dataset.identical\n",
      " |  \n",
      " |  broadcast_like(self: 'T_Dataset', other: 'Dataset | DataArray', exclude: 'Iterable[Hashable]' = None) -> 'T_Dataset'\n",
      " |      Broadcast this DataArray against another Dataset or DataArray.\n",
      " |      This is equivalent to xr.broadcast(other, self)[1]\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      other : Dataset or DataArray\n",
      " |          Object against which to broadcast this array.\n",
      " |      exclude : iterable of hashable, optional\n",
      " |          Dimensions that must not be broadcasted\n",
      " |  \n",
      " |  chunk(self: 'T_Dataset', chunks: \"int | Literal['auto'] | Mapping[Any, None | int | str | tuple[int, ...]]\" = {}, name_prefix: 'str' = 'xarray-', token: 'str | None' = None, lock: 'bool' = False, inline_array: 'bool' = False, **chunks_kwargs: 'Any') -> 'T_Dataset'\n",
      " |      Coerce all arrays in this dataset into dask arrays with the given\n",
      " |      chunks.\n",
      " |      \n",
      " |      Non-dask arrays in this dataset will be converted to dask arrays. Dask\n",
      " |      arrays will be rechunked to the given chunk sizes.\n",
      " |      \n",
      " |      If neither chunks is not provided for one or more dimensions, chunk\n",
      " |      sizes along that dimension will not be updated; non-dask arrays will be\n",
      " |      converted into dask arrays with a single block.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      chunks : int, tuple of int, \"auto\" or mapping of hashable to int, optional\n",
      " |          Chunk sizes along each dimension, e.g., ``5``, ``\"auto\"``, or\n",
      " |          ``{\"x\": 5, \"y\": 5}``.\n",
      " |      name_prefix : str, default: \"xarray-\"\n",
      " |          Prefix for the name of any new dask arrays.\n",
      " |      token : str, optional\n",
      " |          Token uniquely identifying this dataset.\n",
      " |      lock : bool, default: False\n",
      " |          Passed on to :py:func:`dask.array.from_array`, if the array is not\n",
      " |          already as dask array.\n",
      " |      inline_array: bool, default: False\n",
      " |          Passed on to :py:func:`dask.array.from_array`, if the array is not\n",
      " |          already as dask array.\n",
      " |      **chunks_kwargs : {dim: chunks, ...}, optional\n",
      " |          The keyword arguments form of ``chunks``.\n",
      " |          One of chunks or chunks_kwargs must be provided\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      chunked : xarray.Dataset\n",
      " |      \n",
      " |      See Also\n",
      " |      --------\n",
      " |      Dataset.chunks\n",
      " |      Dataset.chunksizes\n",
      " |      xarray.unify_chunks\n",
      " |      dask.array.from_array\n",
      " |  \n",
      " |  coarsen(self, dim: 'Mapping[Any, int] | None' = None, boundary: 'CoarsenBoundaryOptions' = 'exact', side: 'SideOptions | Mapping[Any, SideOptions]' = 'left', coord_func: 'str | Callable | Mapping[Any, str | Callable]' = 'mean', **window_kwargs: 'int') -> 'DatasetCoarsen'\n",
      " |      Coarsen object for Datasets.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      dim : mapping of hashable to int, optional\n",
      " |          Mapping from the dimension name to the window size.\n",
      " |      boundary : {\"exact\", \"trim\", \"pad\"}, default: \"exact\"\n",
      " |          If 'exact', a ValueError will be raised if dimension size is not a\n",
      " |          multiple of the window size. If 'trim', the excess entries are\n",
      " |          dropped. If 'pad', NA will be padded.\n",
      " |      side : {\"left\", \"right\"} or mapping of str to {\"left\", \"right\"}, default: \"left\"\n",
      " |      coord_func : str or mapping of hashable to str, default: \"mean\"\n",
      " |          function (name) that is applied to the coordinates,\n",
      " |          or a mapping from coordinate name to function (name).\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      core.rolling.DatasetCoarsen\n",
      " |      \n",
      " |      See Also\n",
      " |      --------\n",
      " |      core.rolling.DatasetCoarsen\n",
      " |      DataArray.coarsen\n",
      " |  \n",
      " |  combine_first(self: 'T_Dataset', other: 'T_Dataset') -> 'T_Dataset'\n",
      " |      Combine two Datasets, default to data_vars of self.\n",
      " |      \n",
      " |      The new coordinates follow the normal broadcasting and alignment rules\n",
      " |      of ``join='outer'``.  Vacant cells in the expanded coordinates are\n",
      " |      filled with np.nan.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      other : Dataset\n",
      " |          Used to fill all matching missing values in this array.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      Dataset\n",
      " |  \n",
      " |  compute(self: 'T_Dataset', **kwargs) -> 'T_Dataset'\n",
      " |      Manually trigger loading and/or computation of this dataset's data\n",
      " |      from disk or a remote source into memory and return a new dataset.\n",
      " |      Unlike load, the original dataset is left unaltered.\n",
      " |      \n",
      " |      Normally, it should not be necessary to call this method in user code,\n",
      " |      because all xarray functions should either work on deferred data or\n",
      " |      load data automatically. However, this method can be necessary when\n",
      " |      working with many file objects on disk.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      **kwargs : dict\n",
      " |          Additional keyword arguments passed on to ``dask.compute``.\n",
      " |      \n",
      " |      See Also\n",
      " |      --------\n",
      " |      dask.compute\n",
      " |  \n",
      " |  convert_calendar(self: 'T_Dataset', calendar: 'CFCalendar', dim: 'Hashable' = 'time', align_on: \"Literal['date', 'year', None]\" = None, missing: 'Any | None' = None, use_cftime: 'bool | None' = None) -> 'T_Dataset'\n",
      " |      Convert the Dataset to another calendar.\n",
      " |      \n",
      " |      Only converts the individual timestamps, does not modify any data except\n",
      " |      in dropping invalid/surplus dates or inserting missing dates.\n",
      " |      \n",
      " |      If the source and target calendars are either no_leap, all_leap or a\n",
      " |      standard type, only the type of the time array is modified.\n",
      " |      When converting to a leap year from a non-leap year, the 29th of February\n",
      " |      is removed from the array. In the other direction the 29th of February\n",
      " |      will be missing in the output, unless `missing` is specified,\n",
      " |      in which case that value is inserted.\n",
      " |      \n",
      " |      For conversions involving `360_day` calendars, see Notes.\n",
      " |      \n",
      " |      This method is safe to use with sub-daily data as it doesn't touch the\n",
      " |      time part of the timestamps.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ---------\n",
      " |      calendar : str\n",
      " |          The target calendar name.\n",
      " |      dim : Hashable, default: \"time\"\n",
      " |          Name of the time coordinate.\n",
      " |      align_on : {None, 'date', 'year'}, optional\n",
      " |          Must be specified when either source or target is a `360_day` calendar,\n",
      " |          ignored otherwise. See Notes.\n",
      " |      missing : Any or None, optional\n",
      " |          By default, i.e. if the value is None, this method will simply attempt\n",
      " |          to convert the dates in the source calendar to the same dates in the\n",
      " |          target calendar, and drop any of those that are not possible to\n",
      " |          represent.  If a value is provided, a new time coordinate will be\n",
      " |          created in the target calendar with the same frequency as the original\n",
      " |          time coordinate; for any dates that are not present in the source, the\n",
      " |          data will be filled with this value.  Note that using this mode requires\n",
      " |          that the source data have an inferable frequency; for more information\n",
      " |          see :py:func:`xarray.infer_freq`.  For certain frequency, source, and\n",
      " |          target calendar combinations, this could result in many missing values, see notes.\n",
      " |      use_cftime : bool or None, optional\n",
      " |          Whether to use cftime objects in the output, only used if `calendar`\n",
      " |          is one of {\"proleptic_gregorian\", \"gregorian\" or \"standard\"}.\n",
      " |          If True, the new time axis uses cftime objects.\n",
      " |          If None (default), it uses :py:class:`numpy.datetime64` values if the\n",
      " |          date range permits it, and :py:class:`cftime.datetime` objects if not.\n",
      " |          If False, it uses :py:class:`numpy.datetime64`  or fails.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      Dataset\n",
      " |          Copy of the dataarray with the time coordinate converted to the\n",
      " |          target calendar. If 'missing' was None (default), invalid dates in\n",
      " |          the new calendar are dropped, but missing dates are not inserted.\n",
      " |          If `missing` was given, the new data is reindexed to have a time axis\n",
      " |          with the same frequency as the source, but in the new calendar; any\n",
      " |          missing datapoints are filled with `missing`.\n",
      " |      \n",
      " |      Notes\n",
      " |      -----\n",
      " |      Passing a value to `missing` is only usable if the source's time coordinate as an\n",
      " |      inferable frequencies (see :py:func:`~xarray.infer_freq`) and is only appropriate\n",
      " |      if the target coordinate, generated from this frequency, has dates equivalent to the\n",
      " |      source. It is usually **not** appropriate to use this mode with:\n",
      " |      \n",
      " |      - Period-end frequencies : 'A', 'Y', 'Q' or 'M', in opposition to 'AS' 'YS', 'QS' and 'MS'\n",
      " |      - Sub-monthly frequencies that do not divide a day evenly : 'W', 'nD' where `N != 1`\n",
      " |          or 'mH' where 24 % m != 0).\n",
      " |      \n",
      " |      If one of the source or target calendars is `\"360_day\"`, `align_on` must\n",
      " |      be specified and two options are offered.\n",
      " |      \n",
      " |      - \"year\"\n",
      " |          The dates are translated according to their relative position in the year,\n",
      " |          ignoring their original month and day information, meaning that the\n",
      " |          missing/surplus days are added/removed at regular intervals.\n",
      " |      \n",
      " |          From a `360_day` to a standard calendar, the output will be missing the\n",
      " |          following dates (day of year in parentheses):\n",
      " |      \n",
      " |          To a leap year:\n",
      " |              January 31st (31), March 31st (91), June 1st (153), July 31st (213),\n",
      " |              September 31st (275) and November 30th (335).\n",
      " |          To a non-leap year:\n",
      " |              February 6th (36), April 19th (109), July 2nd (183),\n",
      " |              September 12th (255), November 25th (329).\n",
      " |      \n",
      " |          From a standard calendar to a `\"360_day\"`, the following dates in the\n",
      " |          source array will be dropped:\n",
      " |      \n",
      " |          From a leap year:\n",
      " |              January 31st (31), April 1st (92), June 1st (153), August 1st (214),\n",
      " |              September 31st (275), December 1st (336)\n",
      " |          From a non-leap year:\n",
      " |              February 6th (37), April 20th (110), July 2nd (183),\n",
      " |              September 13th (256), November 25th (329)\n",
      " |      \n",
      " |          This option is best used on daily and subdaily data.\n",
      " |      \n",
      " |      - \"date\"\n",
      " |          The month/day information is conserved and invalid dates are dropped\n",
      " |          from the output. This means that when converting from a `\"360_day\"` to a\n",
      " |          standard calendar, all 31st (Jan, March, May, July, August, October and\n",
      " |          December) will be missing as there is no equivalent dates in the\n",
      " |          `\"360_day\"` calendar and the 29th (on non-leap years) and 30th of February\n",
      " |          will be dropped as there are no equivalent dates in a standard calendar.\n",
      " |      \n",
      " |          This option is best used with data on a frequency coarser than daily.\n",
      " |  \n",
      " |  copy(self: 'T_Dataset', deep: 'bool' = False, data: 'Mapping | None' = None) -> 'T_Dataset'\n",
      " |      Returns a copy of this dataset.\n",
      " |      \n",
      " |      If `deep=True`, a deep copy is made of each of the component variables.\n",
      " |      Otherwise, a shallow copy of each of the component variable is made, so\n",
      " |      that the underlying memory region of the new dataset is the same as in\n",
      " |      the original dataset.\n",
      " |      \n",
      " |      Use `data` to create a new object with the same structure as\n",
      " |      original but entirely new data.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      deep : bool, default: False\n",
      " |          Whether each component variable is loaded into memory and copied onto\n",
      " |          the new object. Default is False.\n",
      " |      data : dict-like or None, optional\n",
      " |          Data to use in the new object. Each item in `data` must have same\n",
      " |          shape as corresponding data variable in original. When `data` is\n",
      " |          used, `deep` is ignored for the data variables and only used for\n",
      " |          coords.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      object : Dataset\n",
      " |          New object with dimensions, attributes, coordinates, name, encoding,\n",
      " |          and optionally data copied from original.\n",
      " |      \n",
      " |      Examples\n",
      " |      --------\n",
      " |      Shallow copy versus deep copy\n",
      " |      \n",
      " |      >>> da = xr.DataArray(np.random.randn(2, 3))\n",
      " |      >>> ds = xr.Dataset(\n",
      " |      ...     {\"foo\": da, \"bar\": (\"x\", [-1, 2])},\n",
      " |      ...     coords={\"x\": [\"one\", \"two\"]},\n",
      " |      ... )\n",
      " |      >>> ds.copy()\n",
      " |      <xarray.Dataset>\n",
      " |      Dimensions:  (dim_0: 2, dim_1: 3, x: 2)\n",
      " |      Coordinates:\n",
      " |        * x        (x) <U3 'one' 'two'\n",
      " |      Dimensions without coordinates: dim_0, dim_1\n",
      " |      Data variables:\n",
      " |          foo      (dim_0, dim_1) float64 1.764 0.4002 0.9787 2.241 1.868 -0.9773\n",
      " |          bar      (x) int64 -1 2\n",
      " |      \n",
      " |      >>> ds_0 = ds.copy(deep=False)\n",
      " |      >>> ds_0[\"foo\"][0, 0] = 7\n",
      " |      >>> ds_0\n",
      " |      <xarray.Dataset>\n",
      " |      Dimensions:  (dim_0: 2, dim_1: 3, x: 2)\n",
      " |      Coordinates:\n",
      " |        * x        (x) <U3 'one' 'two'\n",
      " |      Dimensions without coordinates: dim_0, dim_1\n",
      " |      Data variables:\n",
      " |          foo      (dim_0, dim_1) float64 7.0 0.4002 0.9787 2.241 1.868 -0.9773\n",
      " |          bar      (x) int64 -1 2\n",
      " |      \n",
      " |      >>> ds\n",
      " |      <xarray.Dataset>\n",
      " |      Dimensions:  (dim_0: 2, dim_1: 3, x: 2)\n",
      " |      Coordinates:\n",
      " |        * x        (x) <U3 'one' 'two'\n",
      " |      Dimensions without coordinates: dim_0, dim_1\n",
      " |      Data variables:\n",
      " |          foo      (dim_0, dim_1) float64 7.0 0.4002 0.9787 2.241 1.868 -0.9773\n",
      " |          bar      (x) int64 -1 2\n",
      " |      \n",
      " |      Changing the data using the ``data`` argument maintains the\n",
      " |      structure of the original object, but with the new data. Original\n",
      " |      object is unaffected.\n",
      " |      \n",
      " |      >>> ds.copy(data={\"foo\": np.arange(6).reshape(2, 3), \"bar\": [\"a\", \"b\"]})\n",
      " |      <xarray.Dataset>\n",
      " |      Dimensions:  (dim_0: 2, dim_1: 3, x: 2)\n",
      " |      Coordinates:\n",
      " |        * x        (x) <U3 'one' 'two'\n",
      " |      Dimensions without coordinates: dim_0, dim_1\n",
      " |      Data variables:\n",
      " |          foo      (dim_0, dim_1) int64 0 1 2 3 4 5\n",
      " |          bar      (x) <U1 'a' 'b'\n",
      " |      \n",
      " |      >>> ds\n",
      " |      <xarray.Dataset>\n",
      " |      Dimensions:  (dim_0: 2, dim_1: 3, x: 2)\n",
      " |      Coordinates:\n",
      " |        * x        (x) <U3 'one' 'two'\n",
      " |      Dimensions without coordinates: dim_0, dim_1\n",
      " |      Data variables:\n",
      " |          foo      (dim_0, dim_1) float64 7.0 0.4002 0.9787 2.241 1.868 -0.9773\n",
      " |          bar      (x) int64 -1 2\n",
      " |      \n",
      " |      See Also\n",
      " |      --------\n",
      " |      pandas.DataFrame.copy\n",
      " |  \n",
      " |  cumprod(self, dim=None, skipna=None, **kwargs)\n",
      " |      Apply `cumprod` along some dimension of Dataset.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      dim : str or sequence of str, optional\n",
      " |          Dimension over which to apply `cumprod`.\n",
      " |      axis : int or sequence of int, optional\n",
      " |          Axis over which to apply `cumprod`. Only one of the 'dim'\n",
      " |          and 'axis' arguments can be supplied.\n",
      " |      skipna : bool, optional\n",
      " |          If True, skip missing values (as marked by NaN). By default, only\n",
      " |          skips missing values for float dtypes; other dtypes either do not\n",
      " |          have a sentinel missing value (int) or skipna=True has not been\n",
      " |          implemented (object, datetime64 or timedelta64).\n",
      " |      keep_attrs : bool, optional\n",
      " |          If True, the attributes (`attrs`) will be copied from the original\n",
      " |          object to the new one.  If False (default), the new object will be\n",
      " |          returned without attributes.\n",
      " |      **kwargs : dict\n",
      " |          Additional keyword arguments passed on to `cumprod`.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      cumvalue : Dataset\n",
      " |          New Dataset object with `cumprod` applied to its data along the\n",
      " |          indicated dimension.\n",
      " |  \n",
      " |  cumsum(self, dim=None, skipna=None, **kwargs)\n",
      " |      Apply `cumsum` along some dimension of Dataset.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      dim : str or sequence of str, optional\n",
      " |          Dimension over which to apply `cumsum`.\n",
      " |      axis : int or sequence of int, optional\n",
      " |          Axis over which to apply `cumsum`. Only one of the 'dim'\n",
      " |          and 'axis' arguments can be supplied.\n",
      " |      skipna : bool, optional\n",
      " |          If True, skip missing values (as marked by NaN). By default, only\n",
      " |          skips missing values for float dtypes; other dtypes either do not\n",
      " |          have a sentinel missing value (int) or skipna=True has not been\n",
      " |          implemented (object, datetime64 or timedelta64).\n",
      " |      keep_attrs : bool, optional\n",
      " |          If True, the attributes (`attrs`) will be copied from the original\n",
      " |          object to the new one.  If False (default), the new object will be\n",
      " |          returned without attributes.\n",
      " |      **kwargs : dict\n",
      " |          Additional keyword arguments passed on to `cumsum`.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      cumvalue : Dataset\n",
      " |          New Dataset object with `cumsum` applied to its data along the\n",
      " |          indicated dimension.\n",
      " |  \n",
      " |  cumulative_integrate(self: 'T_Dataset', coord: 'Hashable | Sequence[Hashable]', datetime_unit: 'DatetimeUnitOptions' = None) -> 'T_Dataset'\n",
      " |      Integrate along the given coordinate using the trapezoidal rule.\n",
      " |      \n",
      " |      .. note::\n",
      " |          This feature is limited to simple cartesian geometry, i.e. coord\n",
      " |          must be one dimensional.\n",
      " |      \n",
      " |          The first entry of the cumulative integral of each variable is always 0, in\n",
      " |          order to keep the length of the dimension unchanged between input and\n",
      " |          output.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      coord : hashable, or sequence of hashable\n",
      " |          Coordinate(s) used for the integration.\n",
      " |      datetime_unit : {'Y', 'M', 'W', 'D', 'h', 'm', 's', 'ms', 'us', 'ns',                         'ps', 'fs', 'as', None}, optional\n",
      " |          Specify the unit if datetime coordinate is used.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      integrated : Dataset\n",
      " |      \n",
      " |      See also\n",
      " |      --------\n",
      " |      DataArray.cumulative_integrate\n",
      " |      scipy.integrate.cumulative_trapezoid : corresponding scipy function\n",
      " |      \n",
      " |      Examples\n",
      " |      --------\n",
      " |      >>> ds = xr.Dataset(\n",
      " |      ...     data_vars={\"a\": (\"x\", [5, 5, 6, 6]), \"b\": (\"x\", [1, 2, 1, 0])},\n",
      " |      ...     coords={\"x\": [0, 1, 2, 3], \"y\": (\"x\", [1, 7, 3, 5])},\n",
      " |      ... )\n",
      " |      >>> ds\n",
      " |      <xarray.Dataset>\n",
      " |      Dimensions:  (x: 4)\n",
      " |      Coordinates:\n",
      " |        * x        (x) int64 0 1 2 3\n",
      " |          y        (x) int64 1 7 3 5\n",
      " |      Data variables:\n",
      " |          a        (x) int64 5 5 6 6\n",
      " |          b        (x) int64 1 2 1 0\n",
      " |      >>> ds.cumulative_integrate(\"x\")\n",
      " |      <xarray.Dataset>\n",
      " |      Dimensions:  (x: 4)\n",
      " |      Coordinates:\n",
      " |        * x        (x) int64 0 1 2 3\n",
      " |          y        (x) int64 1 7 3 5\n",
      " |      Data variables:\n",
      " |          a        (x) float64 0.0 5.0 10.5 16.5\n",
      " |          b        (x) float64 0.0 1.5 3.0 3.5\n",
      " |      >>> ds.cumulative_integrate(\"y\")\n",
      " |      <xarray.Dataset>\n",
      " |      Dimensions:  (x: 4)\n",
      " |      Coordinates:\n",
      " |        * x        (x) int64 0 1 2 3\n",
      " |          y        (x) int64 1 7 3 5\n",
      " |      Data variables:\n",
      " |          a        (x) float64 0.0 30.0 8.0 20.0\n",
      " |          b        (x) float64 0.0 9.0 3.0 4.0\n",
      " |  \n",
      " |  curvefit(self: 'T_Dataset', coords: 'str | DataArray | Iterable[str | DataArray]', func: 'Callable[..., Any]', reduce_dims: 'Hashable | Iterable[Hashable] | None' = None, skipna: 'bool' = True, p0: 'dict[str, Any] | None' = None, bounds: 'dict[str, Any] | None' = None, param_names: 'Sequence[str] | None' = None, kwargs: 'dict[str, Any] | None' = None) -> 'T_Dataset'\n",
      " |      Curve fitting optimization for arbitrary functions.\n",
      " |      \n",
      " |      Wraps `scipy.optimize.curve_fit` with `apply_ufunc`.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      coords : hashable, DataArray, or sequence of hashable or DataArray\n",
      " |          Independent coordinate(s) over which to perform the curve fitting. Must share\n",
      " |          at least one dimension with the calling object. When fitting multi-dimensional\n",
      " |          functions, supply `coords` as a sequence in the same order as arguments in\n",
      " |          `func`. To fit along existing dimensions of the calling object, `coords` can\n",
      " |          also be specified as a str or sequence of strs.\n",
      " |      func : callable\n",
      " |          User specified function in the form `f(x, *params)` which returns a numpy\n",
      " |          array of length `len(x)`. `params` are the fittable parameters which are optimized\n",
      " |          by scipy curve_fit. `x` can also be specified as a sequence containing multiple\n",
      " |          coordinates, e.g. `f((x0, x1), *params)`.\n",
      " |      reduce_dims : hashable or sequence of hashable\n",
      " |          Additional dimension(s) over which to aggregate while fitting. For example,\n",
      " |          calling `ds.curvefit(coords='time', reduce_dims=['lat', 'lon'], ...)` will\n",
      " |          aggregate all lat and lon points and fit the specified function along the\n",
      " |          time dimension.\n",
      " |      skipna : bool, default: True\n",
      " |          Whether to skip missing values when fitting. Default is True.\n",
      " |      p0 : dict-like, optional\n",
      " |          Optional dictionary of parameter names to initial guesses passed to the\n",
      " |          `curve_fit` `p0` arg. If none or only some parameters are passed, the rest will\n",
      " |          be assigned initial values following the default scipy behavior.\n",
      " |      bounds : dict-like, optional\n",
      " |          Optional dictionary of parameter names to bounding values passed to the\n",
      " |          `curve_fit` `bounds` arg. If none or only some parameters are passed, the rest\n",
      " |          will be unbounded following the default scipy behavior.\n",
      " |      param_names : sequence of hashable, optional\n",
      " |          Sequence of names for the fittable parameters of `func`. If not supplied,\n",
      " |          this will be automatically determined by arguments of `func`. `param_names`\n",
      " |          should be manually supplied when fitting a function that takes a variable\n",
      " |          number of parameters.\n",
      " |      **kwargs : optional\n",
      " |          Additional keyword arguments to passed to scipy curve_fit.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      curvefit_results : Dataset\n",
      " |          A single dataset which contains:\n",
      " |      \n",
      " |          [var]_curvefit_coefficients\n",
      " |              The coefficients of the best fit.\n",
      " |          [var]_curvefit_covariance\n",
      " |              The covariance matrix of the coefficient estimates.\n",
      " |      \n",
      " |      See Also\n",
      " |      --------\n",
      " |      Dataset.polyfit\n",
      " |      scipy.optimize.curve_fit\n",
      " |  \n",
      " |  diff(self: 'T_Dataset', dim: 'Hashable', n: 'int' = 1, label: \"Literal['upper', 'lower']\" = 'upper') -> 'T_Dataset'\n",
      " |      Calculate the n-th order discrete difference along given axis.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      dim : Hashable\n",
      " |          Dimension over which to calculate the finite difference.\n",
      " |      n : int, default: 1\n",
      " |          The number of times values are differenced.\n",
      " |      label : {\"upper\", \"lower\"}, default: \"upper\"\n",
      " |          The new coordinate in dimension ``dim`` will have the\n",
      " |          values of either the minuend's or subtrahend's coordinate\n",
      " |          for values 'upper' and 'lower', respectively.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      difference : Dataset\n",
      " |          The n-th order finite difference of this object.\n",
      " |      \n",
      " |      Notes\n",
      " |      -----\n",
      " |      `n` matches numpy's behavior and is different from pandas' first argument named\n",
      " |      `periods`.\n",
      " |      \n",
      " |      Examples\n",
      " |      --------\n",
      " |      >>> ds = xr.Dataset({\"foo\": (\"x\", [5, 5, 6, 6])})\n",
      " |      >>> ds.diff(\"x\")\n",
      " |      <xarray.Dataset>\n",
      " |      Dimensions:  (x: 3)\n",
      " |      Dimensions without coordinates: x\n",
      " |      Data variables:\n",
      " |          foo      (x) int64 0 1 0\n",
      " |      >>> ds.diff(\"x\", 2)\n",
      " |      <xarray.Dataset>\n",
      " |      Dimensions:  (x: 2)\n",
      " |      Dimensions without coordinates: x\n",
      " |      Data variables:\n",
      " |          foo      (x) int64 1 -1\n",
      " |      \n",
      " |      See Also\n",
      " |      --------\n",
      " |      Dataset.differentiate\n",
      " |  \n",
      " |  differentiate(self: 'T_Dataset', coord: 'Hashable', edge_order: 'Literal[1, 2]' = 1, datetime_unit: 'DatetimeUnitOptions | None' = None) -> 'T_Dataset'\n",
      " |      Differentiate with the second order accurate central\n",
      " |      differences.\n",
      " |      \n",
      " |      .. note::\n",
      " |          This feature is limited to simple cartesian geometry, i.e. coord\n",
      " |          must be one dimensional.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      coord : Hashable\n",
      " |          The coordinate to be used to compute the gradient.\n",
      " |      edge_order : {1, 2}, default: 1\n",
      " |          N-th order accurate differences at the boundaries.\n",
      " |      datetime_unit : None or {\"Y\", \"M\", \"W\", \"D\", \"h\", \"m\", \"s\", \"ms\",             \"us\", \"ns\", \"ps\", \"fs\", \"as\", None}, default: None\n",
      " |          Unit to compute gradient. Only valid for datetime coordinate.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      differentiated: Dataset\n",
      " |      \n",
      " |      See also\n",
      " |      --------\n",
      " |      numpy.gradient: corresponding numpy function\n",
      " |  \n",
      " |  drop(self: 'T_Dataset', labels=None, dim=None, *, errors: 'ErrorOptions' = 'raise', **labels_kwargs) -> 'T_Dataset'\n",
      " |      Backward compatible method based on `drop_vars` and `drop_sel`\n",
      " |      \n",
      " |      Using either `drop_vars` or `drop_sel` is encouraged\n",
      " |      \n",
      " |      See Also\n",
      " |      --------\n",
      " |      Dataset.drop_vars\n",
      " |      Dataset.drop_sel\n",
      " |  \n",
      " |  drop_dims(self: 'T_Dataset', drop_dims: 'Hashable | Iterable[Hashable]', *, errors: 'ErrorOptions' = 'raise') -> 'T_Dataset'\n",
      " |      Drop dimensions and associated variables from this dataset.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      drop_dims : hashable or iterable of hashable\n",
      " |          Dimension or dimensions to drop.\n",
      " |      errors : {\"raise\", \"ignore\"}, default: \"raise\"\n",
      " |          If 'raise', raises a ValueError error if any of the\n",
      " |          dimensions passed are not in the dataset. If 'ignore', any given\n",
      " |          dimensions that are in the dataset are dropped and no error is raised.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      obj : Dataset\n",
      " |          The dataset without the given dimensions (or any variables\n",
      " |          containing those dimensions).\n",
      " |  \n",
      " |  drop_duplicates(self: 'T_Dataset', dim: 'Hashable | Iterable[Hashable]', keep: \"Literal['first', 'last', False]\" = 'first') -> 'T_Dataset'\n",
      " |      Returns a new Dataset with duplicate dimension values removed.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      dim : dimension label or labels\n",
      " |          Pass `...` to drop duplicates along all dimensions.\n",
      " |      keep : {\"first\", \"last\", False}, default: \"first\"\n",
      " |          Determines which duplicates (if any) to keep.\n",
      " |          - ``\"first\"`` : Drop duplicates except for the first occurrence.\n",
      " |          - ``\"last\"`` : Drop duplicates except for the last occurrence.\n",
      " |          - False : Drop all duplicates.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      Dataset\n",
      " |      \n",
      " |      See Also\n",
      " |      --------\n",
      " |      DataArray.drop_duplicates\n",
      " |  \n",
      " |  drop_isel(self: 'T_Dataset', indexers=None, **indexers_kwargs) -> 'T_Dataset'\n",
      " |      Drop index positions from this Dataset.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      indexers : mapping of hashable to Any\n",
      " |          Index locations to drop\n",
      " |      **indexers_kwargs : {dim: position, ...}, optional\n",
      " |          The keyword arguments form of ``dim`` and ``positions``\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      dropped : Dataset\n",
      " |      \n",
      " |      Raises\n",
      " |      ------\n",
      " |      IndexError\n",
      " |      \n",
      " |      Examples\n",
      " |      --------\n",
      " |      >>> data = np.arange(6).reshape(2, 3)\n",
      " |      >>> labels = [\"a\", \"b\", \"c\"]\n",
      " |      >>> ds = xr.Dataset({\"A\": ([\"x\", \"y\"], data), \"y\": labels})\n",
      " |      >>> ds\n",
      " |      <xarray.Dataset>\n",
      " |      Dimensions:  (x: 2, y: 3)\n",
      " |      Coordinates:\n",
      " |        * y        (y) <U1 'a' 'b' 'c'\n",
      " |      Dimensions without coordinates: x\n",
      " |      Data variables:\n",
      " |          A        (x, y) int64 0 1 2 3 4 5\n",
      " |      >>> ds.drop_isel(y=[0, 2])\n",
      " |      <xarray.Dataset>\n",
      " |      Dimensions:  (x: 2, y: 1)\n",
      " |      Coordinates:\n",
      " |        * y        (y) <U1 'b'\n",
      " |      Dimensions without coordinates: x\n",
      " |      Data variables:\n",
      " |          A        (x, y) int64 1 4\n",
      " |      >>> ds.drop_isel(y=1)\n",
      " |      <xarray.Dataset>\n",
      " |      Dimensions:  (x: 2, y: 2)\n",
      " |      Coordinates:\n",
      " |        * y        (y) <U1 'a' 'c'\n",
      " |      Dimensions without coordinates: x\n",
      " |      Data variables:\n",
      " |          A        (x, y) int64 0 2 3 5\n",
      " |  \n",
      " |  drop_sel(self: 'T_Dataset', labels=None, *, errors: 'ErrorOptions' = 'raise', **labels_kwargs) -> 'T_Dataset'\n",
      " |      Drop index labels from this dataset.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      labels : mapping of hashable to Any\n",
      " |          Index labels to drop\n",
      " |      errors : {\"raise\", \"ignore\"}, default: \"raise\"\n",
      " |          If 'raise', raises a ValueError error if\n",
      " |          any of the index labels passed are not\n",
      " |          in the dataset. If 'ignore', any given labels that are in the\n",
      " |          dataset are dropped and no error is raised.\n",
      " |      **labels_kwargs : {dim: label, ...}, optional\n",
      " |          The keyword arguments form of ``dim`` and ``labels``\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      dropped : Dataset\n",
      " |      \n",
      " |      Examples\n",
      " |      --------\n",
      " |      >>> data = np.arange(6).reshape(2, 3)\n",
      " |      >>> labels = [\"a\", \"b\", \"c\"]\n",
      " |      >>> ds = xr.Dataset({\"A\": ([\"x\", \"y\"], data), \"y\": labels})\n",
      " |      >>> ds\n",
      " |      <xarray.Dataset>\n",
      " |      Dimensions:  (x: 2, y: 3)\n",
      " |      Coordinates:\n",
      " |        * y        (y) <U1 'a' 'b' 'c'\n",
      " |      Dimensions without coordinates: x\n",
      " |      Data variables:\n",
      " |          A        (x, y) int64 0 1 2 3 4 5\n",
      " |      >>> ds.drop_sel(y=[\"a\", \"c\"])\n",
      " |      <xarray.Dataset>\n",
      " |      Dimensions:  (x: 2, y: 1)\n",
      " |      Coordinates:\n",
      " |        * y        (y) <U1 'b'\n",
      " |      Dimensions without coordinates: x\n",
      " |      Data variables:\n",
      " |          A        (x, y) int64 1 4\n",
      " |      >>> ds.drop_sel(y=\"b\")\n",
      " |      <xarray.Dataset>\n",
      " |      Dimensions:  (x: 2, y: 2)\n",
      " |      Coordinates:\n",
      " |        * y        (y) <U1 'a' 'c'\n",
      " |      Dimensions without coordinates: x\n",
      " |      Data variables:\n",
      " |          A        (x, y) int64 0 2 3 5\n",
      " |  \n",
      " |  drop_vars(self: 'T_Dataset', names: 'Hashable | Iterable[Hashable]', *, errors: 'ErrorOptions' = 'raise') -> 'T_Dataset'\n",
      " |      Drop variables from this dataset.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      names : hashable or iterable of hashable\n",
      " |          Name(s) of variables to drop.\n",
      " |      errors : {\"raise\", \"ignore\"}, default: \"raise\"\n",
      " |          If 'raise', raises a ValueError error if any of the variable\n",
      " |          passed are not in the dataset. If 'ignore', any given names that are in the\n",
      " |          dataset are dropped and no error is raised.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      dropped : Dataset\n",
      " |  \n",
      " |  dropna(self: 'T_Dataset', dim: 'Hashable', how: \"Literal['any', 'all']\" = 'any', thresh: 'int | None' = None, subset: 'Iterable[Hashable] | None' = None) -> 'T_Dataset'\n",
      " |      Returns a new dataset with dropped labels for missing values along\n",
      " |      the provided dimension.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      dim : hashable\n",
      " |          Dimension along which to drop missing values. Dropping along\n",
      " |          multiple dimensions simultaneously is not yet supported.\n",
      " |      how : {\"any\", \"all\"}, default: \"any\"\n",
      " |          - any : if any NA values are present, drop that label\n",
      " |          - all : if all values are NA, drop that label\n",
      " |      \n",
      " |      thresh : int or None, optional\n",
      " |          If supplied, require this many non-NA values.\n",
      " |      subset : iterable of hashable or None, optional\n",
      " |          Which variables to check for missing values. By default, all\n",
      " |          variables in the dataset are checked.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      Dataset\n",
      " |  \n",
      " |  dump_to_store(self, store: 'AbstractDataStore', **kwargs) -> 'None'\n",
      " |      Store dataset contents to a backends.*DataStore object.\n",
      " |  \n",
      " |  equals(self, other: 'Dataset') -> 'bool'\n",
      " |      Two Datasets are equal if they have matching variables and\n",
      " |      coordinates, all of which are equal.\n",
      " |      \n",
      " |      Datasets can still be equal (like pandas objects) if they have NaN\n",
      " |      values in the same locations.\n",
      " |      \n",
      " |      This method is necessary because `v1 == v2` for ``Dataset``\n",
      " |      does element-wise comparisons (like numpy.ndarrays).\n",
      " |      \n",
      " |      See Also\n",
      " |      --------\n",
      " |      Dataset.broadcast_equals\n",
      " |      Dataset.identical\n",
      " |  \n",
      " |  expand_dims(self, dim: 'None | Hashable | Sequence[Hashable] | Mapping[Any, Any]' = None, axis: 'None | int | Sequence[int]' = None, **dim_kwargs: 'Any') -> 'Dataset'\n",
      " |      Return a new object with an additional axis (or axes) inserted at\n",
      " |      the corresponding position in the array shape.  The new object is a\n",
      " |      view into the underlying array, not a copy.\n",
      " |      \n",
      " |      If dim is already a scalar coordinate, it will be promoted to a 1D\n",
      " |      coordinate consisting of a single value.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      dim : hashable, sequence of hashable, mapping, or None\n",
      " |          Dimensions to include on the new variable. If provided as hashable\n",
      " |          or sequence of hashable, then dimensions are inserted with length\n",
      " |          1. If provided as a mapping, then the keys are the new dimensions\n",
      " |          and the values are either integers (giving the length of the new\n",
      " |          dimensions) or array-like (giving the coordinates of the new\n",
      " |          dimensions).\n",
      " |      axis : int, sequence of int, or None, default: None\n",
      " |          Axis position(s) where new axis is to be inserted (position(s) on\n",
      " |          the result array). If a sequence of integers is passed,\n",
      " |          multiple axes are inserted. In this case, dim arguments should be\n",
      " |          same length list. If axis=None is passed, all the axes will be\n",
      " |          inserted to the start of the result array.\n",
      " |      **dim_kwargs : int or sequence or ndarray\n",
      " |          The keywords are arbitrary dimensions being inserted and the values\n",
      " |          are either the lengths of the new dims (if int is given), or their\n",
      " |          coordinates. Note, this is an alternative to passing a dict to the\n",
      " |          dim kwarg and will only be used if dim is None.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      expanded : Dataset\n",
      " |          This object, but with additional dimension(s).\n",
      " |  \n",
      " |  ffill(self: 'T_Dataset', dim: 'Hashable', limit: 'int | None' = None) -> 'T_Dataset'\n",
      " |      Fill NaN values by propagating values forward\n",
      " |      \n",
      " |      *Requires bottleneck.*\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      dim : Hashable\n",
      " |          Specifies the dimension along which to propagate values when\n",
      " |          filling.\n",
      " |      limit : int or None, optional\n",
      " |          The maximum number of consecutive NaN values to forward fill. In\n",
      " |          other words, if there is a gap with more than this number of\n",
      " |          consecutive NaNs, it will only be partially filled. Must be greater\n",
      " |          than 0 or None for no limit. Must be None or greater than or equal\n",
      " |          to axis length if filling along chunked axes (dimensions).\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      Dataset\n",
      " |  \n",
      " |  fillna(self: 'T_Dataset', value: 'Any') -> 'T_Dataset'\n",
      " |      Fill missing values in this object.\n",
      " |      \n",
      " |      This operation follows the normal broadcasting and alignment rules that\n",
      " |      xarray uses for binary arithmetic, except the result is aligned to this\n",
      " |      object (``join='left'``) instead of aligned to the intersection of\n",
      " |      index coordinates (``join='inner'``).\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      value : scalar, ndarray, DataArray, dict or Dataset\n",
      " |          Used to fill all matching missing values in this dataset's data\n",
      " |          variables. Scalars, ndarrays or DataArrays arguments are used to\n",
      " |          fill all data with aligned coordinates (for DataArrays).\n",
      " |          Dictionaries or datasets match data variables and then align\n",
      " |          coordinates if necessary.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      Dataset\n",
      " |      \n",
      " |      Examples\n",
      " |      --------\n",
      " |      >>> ds = xr.Dataset(\n",
      " |      ...     {\n",
      " |      ...         \"A\": (\"x\", [np.nan, 2, np.nan, 0]),\n",
      " |      ...         \"B\": (\"x\", [3, 4, np.nan, 1]),\n",
      " |      ...         \"C\": (\"x\", [np.nan, np.nan, np.nan, 5]),\n",
      " |      ...         \"D\": (\"x\", [np.nan, 3, np.nan, 4]),\n",
      " |      ...     },\n",
      " |      ...     coords={\"x\": [0, 1, 2, 3]},\n",
      " |      ... )\n",
      " |      >>> ds\n",
      " |      <xarray.Dataset>\n",
      " |      Dimensions:  (x: 4)\n",
      " |      Coordinates:\n",
      " |        * x        (x) int64 0 1 2 3\n",
      " |      Data variables:\n",
      " |          A        (x) float64 nan 2.0 nan 0.0\n",
      " |          B        (x) float64 3.0 4.0 nan 1.0\n",
      " |          C        (x) float64 nan nan nan 5.0\n",
      " |          D        (x) float64 nan 3.0 nan 4.0\n",
      " |      \n",
      " |      Replace all `NaN` values with 0s.\n",
      " |      \n",
      " |      >>> ds.fillna(0)\n",
      " |      <xarray.Dataset>\n",
      " |      Dimensions:  (x: 4)\n",
      " |      Coordinates:\n",
      " |        * x        (x) int64 0 1 2 3\n",
      " |      Data variables:\n",
      " |          A        (x) float64 0.0 2.0 0.0 0.0\n",
      " |          B        (x) float64 3.0 4.0 0.0 1.0\n",
      " |          C        (x) float64 0.0 0.0 0.0 5.0\n",
      " |          D        (x) float64 0.0 3.0 0.0 4.0\n",
      " |      \n",
      " |      Replace all `NaN` elements in column A, B, C, and D, with 0, 1, 2, and 3 respectively.\n",
      " |      \n",
      " |      >>> values = {\"A\": 0, \"B\": 1, \"C\": 2, \"D\": 3}\n",
      " |      >>> ds.fillna(value=values)\n",
      " |      <xarray.Dataset>\n",
      " |      Dimensions:  (x: 4)\n",
      " |      Coordinates:\n",
      " |        * x        (x) int64 0 1 2 3\n",
      " |      Data variables:\n",
      " |          A        (x) float64 0.0 2.0 0.0 0.0\n",
      " |          B        (x) float64 3.0 4.0 1.0 1.0\n",
      " |          C        (x) float64 2.0 2.0 2.0 5.0\n",
      " |          D        (x) float64 3.0 3.0 3.0 4.0\n",
      " |  \n",
      " |  filter_by_attrs(self: 'T_Dataset', **kwargs) -> 'T_Dataset'\n",
      " |      Returns a ``Dataset`` with variables that match specific conditions.\n",
      " |      \n",
      " |      Can pass in ``key=value`` or ``key=callable``.  A Dataset is returned\n",
      " |      containing only the variables for which all the filter tests pass.\n",
      " |      These tests are either ``key=value`` for which the attribute ``key``\n",
      " |      has the exact value ``value`` or the callable passed into\n",
      " |      ``key=callable`` returns True. The callable will be passed a single\n",
      " |      value, either the value of the attribute ``key`` or ``None`` if the\n",
      " |      DataArray does not have an attribute with the name ``key``.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      **kwargs\n",
      " |          key : str\n",
      " |              Attribute name.\n",
      " |          value : callable or obj\n",
      " |              If value is a callable, it should return a boolean in the form\n",
      " |              of bool = func(attr) where attr is da.attrs[key].\n",
      " |              Otherwise, value will be compared to the each\n",
      " |              DataArray's attrs[key].\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      new : Dataset\n",
      " |          New dataset with variables filtered by attribute.\n",
      " |      \n",
      " |      Examples\n",
      " |      --------\n",
      " |      >>> temp = 15 + 8 * np.random.randn(2, 2, 3)\n",
      " |      >>> precip = 10 * np.random.rand(2, 2, 3)\n",
      " |      >>> lon = [[-99.83, -99.32], [-99.79, -99.23]]\n",
      " |      >>> lat = [[42.25, 42.21], [42.63, 42.59]]\n",
      " |      >>> dims = [\"x\", \"y\", \"time\"]\n",
      " |      >>> temp_attr = dict(standard_name=\"air_potential_temperature\")\n",
      " |      >>> precip_attr = dict(standard_name=\"convective_precipitation_flux\")\n",
      " |      \n",
      " |      >>> ds = xr.Dataset(\n",
      " |      ...     dict(\n",
      " |      ...         temperature=(dims, temp, temp_attr),\n",
      " |      ...         precipitation=(dims, precip, precip_attr),\n",
      " |      ...     ),\n",
      " |      ...     coords=dict(\n",
      " |      ...         lon=([\"x\", \"y\"], lon),\n",
      " |      ...         lat=([\"x\", \"y\"], lat),\n",
      " |      ...         time=pd.date_range(\"2014-09-06\", periods=3),\n",
      " |      ...         reference_time=pd.Timestamp(\"2014-09-05\"),\n",
      " |      ...     ),\n",
      " |      ... )\n",
      " |      \n",
      " |      Get variables matching a specific standard_name:\n",
      " |      \n",
      " |      >>> ds.filter_by_attrs(standard_name=\"convective_precipitation_flux\")\n",
      " |      <xarray.Dataset>\n",
      " |      Dimensions:         (x: 2, y: 2, time: 3)\n",
      " |      Coordinates:\n",
      " |          lon             (x, y) float64 -99.83 -99.32 -99.79 -99.23\n",
      " |          lat             (x, y) float64 42.25 42.21 42.63 42.59\n",
      " |        * time            (time) datetime64[ns] 2014-09-06 2014-09-07 2014-09-08\n",
      " |          reference_time  datetime64[ns] 2014-09-05\n",
      " |      Dimensions without coordinates: x, y\n",
      " |      Data variables:\n",
      " |          precipitation   (x, y, time) float64 5.68 9.256 0.7104 ... 7.992 4.615 7.805\n",
      " |      \n",
      " |      Get all variables that have a standard_name attribute:\n",
      " |      \n",
      " |      >>> standard_name = lambda v: v is not None\n",
      " |      >>> ds.filter_by_attrs(standard_name=standard_name)\n",
      " |      <xarray.Dataset>\n",
      " |      Dimensions:         (x: 2, y: 2, time: 3)\n",
      " |      Coordinates:\n",
      " |          lon             (x, y) float64 -99.83 -99.32 -99.79 -99.23\n",
      " |          lat             (x, y) float64 42.25 42.21 42.63 42.59\n",
      " |        * time            (time) datetime64[ns] 2014-09-06 2014-09-07 2014-09-08\n",
      " |          reference_time  datetime64[ns] 2014-09-05\n",
      " |      Dimensions without coordinates: x, y\n",
      " |      Data variables:\n",
      " |          temperature     (x, y, time) float64 29.11 18.2 22.83 ... 18.28 16.15 26.63\n",
      " |          precipitation   (x, y, time) float64 5.68 9.256 0.7104 ... 7.992 4.615 7.805\n",
      " |  \n",
      " |  groupby(self, group: 'Hashable | DataArray | IndexVariable', squeeze: 'bool' = True, restore_coord_dims: 'bool' = False) -> 'DatasetGroupBy'\n",
      " |      Returns a DatasetGroupBy object for performing grouped operations.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      group : Hashable, DataArray or IndexVariable\n",
      " |          Array whose unique values should be used to group this array. If a\n",
      " |          string, must be the name of a variable contained in this dataset.\n",
      " |      squeeze : bool, default: True\n",
      " |          If \"group\" is a dimension of any arrays in this dataset, `squeeze`\n",
      " |          controls whether the subarrays have a dimension of length 1 along\n",
      " |          that dimension or if the dimension is squeezed out.\n",
      " |      restore_coord_dims : bool, default: False\n",
      " |          If True, also restore the dimension order of multi-dimensional\n",
      " |          coordinates.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      grouped : DatasetGroupBy\n",
      " |          A `DatasetGroupBy` object patterned after `pandas.GroupBy` that can be\n",
      " |          iterated over in the form of `(unique_value, grouped_array)` pairs.\n",
      " |      \n",
      " |      See Also\n",
      " |      --------\n",
      " |      Dataset.groupby_bins\n",
      " |      DataArray.groupby\n",
      " |      core.groupby.DatasetGroupBy\n",
      " |      pandas.DataFrame.groupby\n",
      " |  \n",
      " |  groupby_bins(self, group: 'Hashable | DataArray | IndexVariable', bins: 'ArrayLike', right: 'bool' = True, labels: 'ArrayLike | None' = None, precision: 'int' = 3, include_lowest: 'bool' = False, squeeze: 'bool' = True, restore_coord_dims: 'bool' = False) -> 'DatasetGroupBy'\n",
      " |      Returns a DatasetGroupBy object for performing grouped operations.\n",
      " |      \n",
      " |      Rather than using all unique values of `group`, the values are discretized\n",
      " |      first by applying `pandas.cut` [1]_ to `group`.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      group : Hashable, DataArray or IndexVariable\n",
      " |          Array whose binned values should be used to group this array. If a\n",
      " |          string, must be the name of a variable contained in this dataset.\n",
      " |      bins : int or array-like\n",
      " |          If bins is an int, it defines the number of equal-width bins in the\n",
      " |          range of x. However, in this case, the range of x is extended by .1%\n",
      " |          on each side to include the min or max values of x. If bins is a\n",
      " |          sequence it defines the bin edges allowing for non-uniform bin\n",
      " |          width. No extension of the range of x is done in this case.\n",
      " |      right : bool, default: True\n",
      " |          Indicates whether the bins include the rightmost edge or not. If\n",
      " |          right == True (the default), then the bins [1,2,3,4] indicate\n",
      " |          (1,2], (2,3], (3,4].\n",
      " |      labels : array-like or bool, default: None\n",
      " |          Used as labels for the resulting bins. Must be of the same length as\n",
      " |          the resulting bins. If False, string bin labels are assigned by\n",
      " |          `pandas.cut`.\n",
      " |      precision : int, default: 3\n",
      " |          The precision at which to store and display the bins labels.\n",
      " |      include_lowest : bool, default: False\n",
      " |          Whether the first interval should be left-inclusive or not.\n",
      " |      squeeze : bool, default: True\n",
      " |          If \"group\" is a dimension of any arrays in this dataset, `squeeze`\n",
      " |          controls whether the subarrays have a dimension of length 1 along\n",
      " |          that dimension or if the dimension is squeezed out.\n",
      " |      restore_coord_dims : bool, default: False\n",
      " |          If True, also restore the dimension order of multi-dimensional\n",
      " |          coordinates.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      grouped : DatasetGroupBy\n",
      " |          A `DatasetGroupBy` object patterned after `pandas.GroupBy` that can be\n",
      " |          iterated over in the form of `(unique_value, grouped_array)` pairs.\n",
      " |          The name of the group has the added suffix `_bins` in order to\n",
      " |          distinguish it from the original variable.\n",
      " |      \n",
      " |      See Also\n",
      " |      --------\n",
      " |      Dataset.groupby\n",
      " |      DataArray.groupby_bins\n",
      " |      core.groupby.DatasetGroupBy\n",
      " |      pandas.DataFrame.groupby\n",
      " |      \n",
      " |      References\n",
      " |      ----------\n",
      " |      .. [1] http://pandas.pydata.org/pandas-docs/stable/generated/pandas.cut.html\n",
      " |  \n",
      " |  head(self: 'T_Dataset', indexers: 'Mapping[Any, int] | int | None' = None, **indexers_kwargs: 'Any') -> 'T_Dataset'\n",
      " |      Returns a new dataset with the first `n` values of each array\n",
      " |      for the specified dimension(s).\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      indexers : dict or int, default: 5\n",
      " |          A dict with keys matching dimensions and integer values `n`\n",
      " |          or a single integer `n` applied over all dimensions.\n",
      " |          One of indexers or indexers_kwargs must be provided.\n",
      " |      **indexers_kwargs : {dim: n, ...}, optional\n",
      " |          The keyword arguments form of ``indexers``.\n",
      " |          One of indexers or indexers_kwargs must be provided.\n",
      " |      \n",
      " |      See Also\n",
      " |      --------\n",
      " |      Dataset.tail\n",
      " |      Dataset.thin\n",
      " |      DataArray.head\n",
      " |  \n",
      " |  identical(self, other: 'Dataset') -> 'bool'\n",
      " |      Like equals, but also checks all dataset attributes and the\n",
      " |      attributes on all variables and coordinates.\n",
      " |      \n",
      " |      See Also\n",
      " |      --------\n",
      " |      Dataset.broadcast_equals\n",
      " |      Dataset.equals\n",
      " |  \n",
      " |  idxmax(self: 'T_Dataset', dim: 'Hashable | None' = None, skipna: 'bool | None' = None, fill_value: 'Any' = <NA>, keep_attrs: 'bool | None' = None) -> 'T_Dataset'\n",
      " |      Return the coordinate label of the maximum value along a dimension.\n",
      " |      \n",
      " |      Returns a new `Dataset` named after the dimension with the values of\n",
      " |      the coordinate labels along that dimension corresponding to maximum\n",
      " |      values along that dimension.\n",
      " |      \n",
      " |      In comparison to :py:meth:`~Dataset.argmax`, this returns the\n",
      " |      coordinate label while :py:meth:`~Dataset.argmax` returns the index.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      dim : str, optional\n",
      " |          Dimension over which to apply `idxmax`.  This is optional for 1D\n",
      " |          variables, but required for variables with 2 or more dimensions.\n",
      " |      skipna : bool or None, optional\n",
      " |          If True, skip missing values (as marked by NaN). By default, only\n",
      " |          skips missing values for ``float``, ``complex``, and ``object``\n",
      " |          dtypes; other dtypes either do not have a sentinel missing value\n",
      " |          (``int``) or ``skipna=True`` has not been implemented\n",
      " |          (``datetime64`` or ``timedelta64``).\n",
      " |      fill_value : Any, default: NaN\n",
      " |          Value to be filled in case all of the values along a dimension are\n",
      " |          null.  By default this is NaN.  The fill value and result are\n",
      " |          automatically converted to a compatible dtype if possible.\n",
      " |          Ignored if ``skipna`` is False.\n",
      " |      keep_attrs : bool or None, optional\n",
      " |          If True, the attributes (``attrs``) will be copied from the\n",
      " |          original object to the new one. If False, the new object\n",
      " |          will be returned without attributes.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      reduced : Dataset\n",
      " |          New `Dataset` object with `idxmax` applied to its data and the\n",
      " |          indicated dimension removed.\n",
      " |      \n",
      " |      See Also\n",
      " |      --------\n",
      " |      DataArray.idxmax, Dataset.idxmin, Dataset.max, Dataset.argmax\n",
      " |      \n",
      " |      Examples\n",
      " |      --------\n",
      " |      >>> array1 = xr.DataArray(\n",
      " |      ...     [0, 2, 1, 0, -2], dims=\"x\", coords={\"x\": [\"a\", \"b\", \"c\", \"d\", \"e\"]}\n",
      " |      ... )\n",
      " |      >>> array2 = xr.DataArray(\n",
      " |      ...     [\n",
      " |      ...         [2.0, 1.0, 2.0, 0.0, -2.0],\n",
      " |      ...         [-4.0, np.NaN, 2.0, np.NaN, -2.0],\n",
      " |      ...         [np.NaN, np.NaN, 1.0, np.NaN, np.NaN],\n",
      " |      ...     ],\n",
      " |      ...     dims=[\"y\", \"x\"],\n",
      " |      ...     coords={\"y\": [-1, 0, 1], \"x\": [\"a\", \"b\", \"c\", \"d\", \"e\"]},\n",
      " |      ... )\n",
      " |      >>> ds = xr.Dataset({\"int\": array1, \"float\": array2})\n",
      " |      >>> ds.max(dim=\"x\")\n",
      " |      <xarray.Dataset>\n",
      " |      Dimensions:  (y: 3)\n",
      " |      Coordinates:\n",
      " |        * y        (y) int64 -1 0 1\n",
      " |      Data variables:\n",
      " |          int      int64 2\n",
      " |          float    (y) float64 2.0 2.0 1.0\n",
      " |      >>> ds.argmax(dim=\"x\")\n",
      " |      <xarray.Dataset>\n",
      " |      Dimensions:  (y: 3)\n",
      " |      Coordinates:\n",
      " |        * y        (y) int64 -1 0 1\n",
      " |      Data variables:\n",
      " |          int      int64 1\n",
      " |          float    (y) int64 0 2 2\n",
      " |      >>> ds.idxmax(dim=\"x\")\n",
      " |      <xarray.Dataset>\n",
      " |      Dimensions:  (y: 3)\n",
      " |      Coordinates:\n",
      " |        * y        (y) int64 -1 0 1\n",
      " |      Data variables:\n",
      " |          int      <U1 'b'\n",
      " |          float    (y) object 'a' 'c' 'c'\n",
      " |  \n",
      " |  idxmin(self: 'T_Dataset', dim: 'Hashable | None' = None, skipna: 'bool | None' = None, fill_value: 'Any' = <NA>, keep_attrs: 'bool | None' = None) -> 'T_Dataset'\n",
      " |      Return the coordinate label of the minimum value along a dimension.\n",
      " |      \n",
      " |      Returns a new `Dataset` named after the dimension with the values of\n",
      " |      the coordinate labels along that dimension corresponding to minimum\n",
      " |      values along that dimension.\n",
      " |      \n",
      " |      In comparison to :py:meth:`~Dataset.argmin`, this returns the\n",
      " |      coordinate label while :py:meth:`~Dataset.argmin` returns the index.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      dim : Hashable, optional\n",
      " |          Dimension over which to apply `idxmin`.  This is optional for 1D\n",
      " |          variables, but required for variables with 2 or more dimensions.\n",
      " |      skipna : bool or None, optional\n",
      " |          If True, skip missing values (as marked by NaN). By default, only\n",
      " |          skips missing values for ``float``, ``complex``, and ``object``\n",
      " |          dtypes; other dtypes either do not have a sentinel missing value\n",
      " |          (``int``) or ``skipna=True`` has not been implemented\n",
      " |          (``datetime64`` or ``timedelta64``).\n",
      " |      fill_value : Any, default: NaN\n",
      " |          Value to be filled in case all of the values along a dimension are\n",
      " |          null.  By default this is NaN.  The fill value and result are\n",
      " |          automatically converted to a compatible dtype if possible.\n",
      " |          Ignored if ``skipna`` is False.\n",
      " |      keep_attrs : bool or None, optional\n",
      " |          If True, the attributes (``attrs``) will be copied from the\n",
      " |          original object to the new one. If False, the new object\n",
      " |          will be returned without attributes.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      reduced : Dataset\n",
      " |          New `Dataset` object with `idxmin` applied to its data and the\n",
      " |          indicated dimension removed.\n",
      " |      \n",
      " |      See Also\n",
      " |      --------\n",
      " |      DataArray.idxmin, Dataset.idxmax, Dataset.min, Dataset.argmin\n",
      " |      \n",
      " |      Examples\n",
      " |      --------\n",
      " |      >>> array1 = xr.DataArray(\n",
      " |      ...     [0, 2, 1, 0, -2], dims=\"x\", coords={\"x\": [\"a\", \"b\", \"c\", \"d\", \"e\"]}\n",
      " |      ... )\n",
      " |      >>> array2 = xr.DataArray(\n",
      " |      ...     [\n",
      " |      ...         [2.0, 1.0, 2.0, 0.0, -2.0],\n",
      " |      ...         [-4.0, np.NaN, 2.0, np.NaN, -2.0],\n",
      " |      ...         [np.NaN, np.NaN, 1.0, np.NaN, np.NaN],\n",
      " |      ...     ],\n",
      " |      ...     dims=[\"y\", \"x\"],\n",
      " |      ...     coords={\"y\": [-1, 0, 1], \"x\": [\"a\", \"b\", \"c\", \"d\", \"e\"]},\n",
      " |      ... )\n",
      " |      >>> ds = xr.Dataset({\"int\": array1, \"float\": array2})\n",
      " |      >>> ds.min(dim=\"x\")\n",
      " |      <xarray.Dataset>\n",
      " |      Dimensions:  (y: 3)\n",
      " |      Coordinates:\n",
      " |        * y        (y) int64 -1 0 1\n",
      " |      Data variables:\n",
      " |          int      int64 -2\n",
      " |          float    (y) float64 -2.0 -4.0 1.0\n",
      " |      >>> ds.argmin(dim=\"x\")\n",
      " |      <xarray.Dataset>\n",
      " |      Dimensions:  (y: 3)\n",
      " |      Coordinates:\n",
      " |        * y        (y) int64 -1 0 1\n",
      " |      Data variables:\n",
      " |          int      int64 4\n",
      " |          float    (y) int64 4 0 2\n",
      " |      >>> ds.idxmin(dim=\"x\")\n",
      " |      <xarray.Dataset>\n",
      " |      Dimensions:  (y: 3)\n",
      " |      Coordinates:\n",
      " |        * y        (y) int64 -1 0 1\n",
      " |      Data variables:\n",
      " |          int      <U1 'e'\n",
      " |          float    (y) object 'e' 'a' 'c'\n",
      " |  \n",
      " |  info(self, buf: 'IO | None' = None) -> 'None'\n",
      " |      Concise summary of a Dataset variables and attributes.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      buf : file-like, default: sys.stdout\n",
      " |          writable buffer\n",
      " |      \n",
      " |      See Also\n",
      " |      --------\n",
      " |      pandas.DataFrame.assign\n",
      " |      ncdump : netCDF's ncdump\n",
      " |  \n",
      " |  integrate(self: 'T_Dataset', coord: 'Hashable | Sequence[Hashable]', datetime_unit: 'DatetimeUnitOptions' = None) -> 'T_Dataset'\n",
      " |      Integrate along the given coordinate using the trapezoidal rule.\n",
      " |      \n",
      " |      .. note::\n",
      " |          This feature is limited to simple cartesian geometry, i.e. coord\n",
      " |          must be one dimensional.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      coord : hashable, or sequence of hashable\n",
      " |          Coordinate(s) used for the integration.\n",
      " |      datetime_unit : {'Y', 'M', 'W', 'D', 'h', 'm', 's', 'ms', 'us', 'ns',                         'ps', 'fs', 'as', None}, optional\n",
      " |          Specify the unit if datetime coordinate is used.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      integrated : Dataset\n",
      " |      \n",
      " |      See also\n",
      " |      --------\n",
      " |      DataArray.integrate\n",
      " |      numpy.trapz : corresponding numpy function\n",
      " |      \n",
      " |      Examples\n",
      " |      --------\n",
      " |      >>> ds = xr.Dataset(\n",
      " |      ...     data_vars={\"a\": (\"x\", [5, 5, 6, 6]), \"b\": (\"x\", [1, 2, 1, 0])},\n",
      " |      ...     coords={\"x\": [0, 1, 2, 3], \"y\": (\"x\", [1, 7, 3, 5])},\n",
      " |      ... )\n",
      " |      >>> ds\n",
      " |      <xarray.Dataset>\n",
      " |      Dimensions:  (x: 4)\n",
      " |      Coordinates:\n",
      " |        * x        (x) int64 0 1 2 3\n",
      " |          y        (x) int64 1 7 3 5\n",
      " |      Data variables:\n",
      " |          a        (x) int64 5 5 6 6\n",
      " |          b        (x) int64 1 2 1 0\n",
      " |      >>> ds.integrate(\"x\")\n",
      " |      <xarray.Dataset>\n",
      " |      Dimensions:  ()\n",
      " |      Data variables:\n",
      " |          a        float64 16.5\n",
      " |          b        float64 3.5\n",
      " |      >>> ds.integrate(\"y\")\n",
      " |      <xarray.Dataset>\n",
      " |      Dimensions:  ()\n",
      " |      Data variables:\n",
      " |          a        float64 20.0\n",
      " |          b        float64 4.0\n",
      " |  \n",
      " |  interp(self: 'T_Dataset', coords: 'Mapping[Any, Any] | None' = None, method: 'InterpOptions' = 'linear', assume_sorted: 'bool' = False, kwargs: 'Mapping[str, Any]' = None, method_non_numeric: 'str' = 'nearest', **coords_kwargs: 'Any') -> 'T_Dataset'\n",
      " |      Interpolate a Dataset onto new coordinates\n",
      " |      \n",
      " |      Performs univariate or multivariate interpolation of a Dataset onto\n",
      " |      new coordinates using scipy's interpolation routines. If interpolating\n",
      " |      along an existing dimension, :py:class:`scipy.interpolate.interp1d` is\n",
      " |      called.  When interpolating along multiple existing dimensions, an\n",
      " |      attempt is made to decompose the interpolation into multiple\n",
      " |      1-dimensional interpolations. If this is possible,\n",
      " |      :py:class:`scipy.interpolate.interp1d` is called. Otherwise,\n",
      " |      :py:func:`scipy.interpolate.interpn` is called.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      coords : dict, optional\n",
      " |          Mapping from dimension names to the new coordinates.\n",
      " |          New coordinate can be a scalar, array-like or DataArray.\n",
      " |          If DataArrays are passed as new coordinates, their dimensions are\n",
      " |          used for the broadcasting. Missing values are skipped.\n",
      " |      method : {\"linear\", \"nearest\", \"zero\", \"slinear\", \"quadratic\", \"cubic\", \"polynomial\",             \"barycentric\", \"krog\", \"pchip\", \"spline\", \"akima\"}, default: \"linear\"\n",
      " |          String indicating which method to use for interpolation:\n",
      " |      \n",
      " |          - 'linear': linear interpolation. Additional keyword\n",
      " |            arguments are passed to :py:func:`numpy.interp`\n",
      " |          - 'nearest', 'zero', 'slinear', 'quadratic', 'cubic', 'polynomial':\n",
      " |            are passed to :py:func:`scipy.interpolate.interp1d`. If\n",
      " |            ``method='polynomial'``, the ``order`` keyword argument must also be\n",
      " |            provided.\n",
      " |          - 'barycentric', 'krog', 'pchip', 'spline', 'akima': use their\n",
      " |            respective :py:class:`scipy.interpolate` classes.\n",
      " |      \n",
      " |      assume_sorted : bool, default: False\n",
      " |          If False, values of coordinates that are interpolated over can be\n",
      " |          in any order and they are sorted first. If True, interpolated\n",
      " |          coordinates are assumed to be an array of monotonically increasing\n",
      " |          values.\n",
      " |      kwargs : dict, optional\n",
      " |          Additional keyword arguments passed to scipy's interpolator. Valid\n",
      " |          options and their behavior depend whether ``interp1d`` or\n",
      " |          ``interpn`` is used.\n",
      " |      method_non_numeric : {\"nearest\", \"pad\", \"ffill\", \"backfill\", \"bfill\"}, optional\n",
      " |          Method for non-numeric types. Passed on to :py:meth:`Dataset.reindex`.\n",
      " |          ``\"nearest\"`` is used by default.\n",
      " |      **coords_kwargs : {dim: coordinate, ...}, optional\n",
      " |          The keyword arguments form of ``coords``.\n",
      " |          One of coords or coords_kwargs must be provided.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      interpolated : Dataset\n",
      " |          New dataset on the new coordinates.\n",
      " |      \n",
      " |      Notes\n",
      " |      -----\n",
      " |      scipy is required.\n",
      " |      \n",
      " |      See Also\n",
      " |      --------\n",
      " |      scipy.interpolate.interp1d\n",
      " |      scipy.interpolate.interpn\n",
      " |      \n",
      " |      Examples\n",
      " |      --------\n",
      " |      >>> ds = xr.Dataset(\n",
      " |      ...     data_vars={\n",
      " |      ...         \"a\": (\"x\", [5, 7, 4]),\n",
      " |      ...         \"b\": (\n",
      " |      ...             (\"x\", \"y\"),\n",
      " |      ...             [[1, 4, 2, 9], [2, 7, 6, np.nan], [6, np.nan, 5, 8]],\n",
      " |      ...         ),\n",
      " |      ...     },\n",
      " |      ...     coords={\"x\": [0, 1, 2], \"y\": [10, 12, 14, 16]},\n",
      " |      ... )\n",
      " |      >>> ds\n",
      " |      <xarray.Dataset>\n",
      " |      Dimensions:  (x: 3, y: 4)\n",
      " |      Coordinates:\n",
      " |        * x        (x) int64 0 1 2\n",
      " |        * y        (y) int64 10 12 14 16\n",
      " |      Data variables:\n",
      " |          a        (x) int64 5 7 4\n",
      " |          b        (x, y) float64 1.0 4.0 2.0 9.0 2.0 7.0 6.0 nan 6.0 nan 5.0 8.0\n",
      " |      \n",
      " |      1D interpolation with the default method (linear):\n",
      " |      \n",
      " |      >>> ds.interp(x=[0, 0.75, 1.25, 1.75])\n",
      " |      <xarray.Dataset>\n",
      " |      Dimensions:  (x: 4, y: 4)\n",
      " |      Coordinates:\n",
      " |        * y        (y) int64 10 12 14 16\n",
      " |        * x        (x) float64 0.0 0.75 1.25 1.75\n",
      " |      Data variables:\n",
      " |          a        (x) float64 5.0 6.5 6.25 4.75\n",
      " |          b        (x, y) float64 1.0 4.0 2.0 nan 1.75 6.25 ... nan 5.0 nan 5.25 nan\n",
      " |      \n",
      " |      1D interpolation with a different method:\n",
      " |      \n",
      " |      >>> ds.interp(x=[0, 0.75, 1.25, 1.75], method=\"nearest\")\n",
      " |      <xarray.Dataset>\n",
      " |      Dimensions:  (x: 4, y: 4)\n",
      " |      Coordinates:\n",
      " |        * y        (y) int64 10 12 14 16\n",
      " |        * x        (x) float64 0.0 0.75 1.25 1.75\n",
      " |      Data variables:\n",
      " |          a        (x) float64 5.0 7.0 7.0 4.0\n",
      " |          b        (x, y) float64 1.0 4.0 2.0 9.0 2.0 7.0 ... 6.0 nan 6.0 nan 5.0 8.0\n",
      " |      \n",
      " |      1D extrapolation:\n",
      " |      \n",
      " |      >>> ds.interp(\n",
      " |      ...     x=[1, 1.5, 2.5, 3.5],\n",
      " |      ...     method=\"linear\",\n",
      " |      ...     kwargs={\"fill_value\": \"extrapolate\"},\n",
      " |      ... )\n",
      " |      <xarray.Dataset>\n",
      " |      Dimensions:  (x: 4, y: 4)\n",
      " |      Coordinates:\n",
      " |        * y        (y) int64 10 12 14 16\n",
      " |        * x        (x) float64 1.0 1.5 2.5 3.5\n",
      " |      Data variables:\n",
      " |          a        (x) float64 7.0 5.5 2.5 -0.5\n",
      " |          b        (x, y) float64 2.0 7.0 6.0 nan 4.0 nan ... 4.5 nan 12.0 nan 3.5 nan\n",
      " |      \n",
      " |      2D interpolation:\n",
      " |      \n",
      " |      >>> ds.interp(x=[0, 0.75, 1.25, 1.75], y=[11, 13, 15], method=\"linear\")\n",
      " |      <xarray.Dataset>\n",
      " |      Dimensions:  (x: 4, y: 3)\n",
      " |      Coordinates:\n",
      " |        * x        (x) float64 0.0 0.75 1.25 1.75\n",
      " |        * y        (y) int64 11 13 15\n",
      " |      Data variables:\n",
      " |          a        (x) float64 5.0 6.5 6.25 4.75\n",
      " |          b        (x, y) float64 2.5 3.0 nan 4.0 5.625 nan nan nan nan nan nan nan\n",
      " |  \n",
      " |  interp_calendar(self: 'T_Dataset', target: 'pd.DatetimeIndex | CFTimeIndex | DataArray', dim: 'Hashable' = 'time') -> 'T_Dataset'\n",
      " |      Interpolates the Dataset to another calendar based on decimal year measure.\n",
      " |      \n",
      " |      Each timestamp in `source` and `target` are first converted to their decimal\n",
      " |      year equivalent then `source` is interpolated on the target coordinate.\n",
      " |      The decimal year of a timestamp is its year plus its sub-year component\n",
      " |      converted to the fraction of its year. For example \"2000-03-01 12:00\" is\n",
      " |      2000.1653 in a standard calendar or 2000.16301 in a `\"noleap\"` calendar.\n",
      " |      \n",
      " |      This method should only be used when the time (HH:MM:SS) information of\n",
      " |      time coordinate is not important.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      target: DataArray or DatetimeIndex or CFTimeIndex\n",
      " |          The target time coordinate of a valid dtype\n",
      " |          (np.datetime64 or cftime objects)\n",
      " |      dim : Hashable, default: \"time\"\n",
      " |          The time coordinate name.\n",
      " |      \n",
      " |      Return\n",
      " |      ------\n",
      " |      DataArray\n",
      " |          The source interpolated on the decimal years of target,\n",
      " |  \n",
      " |  interp_like(self, other: 'Dataset | DataArray', method: 'InterpOptions' = 'linear', assume_sorted: 'bool' = False, kwargs: 'Mapping[str, Any] | None' = None, method_non_numeric: 'str' = 'nearest') -> 'Dataset'\n",
      " |      Interpolate this object onto the coordinates of another object,\n",
      " |      filling the out of range values with NaN.\n",
      " |      \n",
      " |      If interpolating along a single existing dimension,\n",
      " |      :py:class:`scipy.interpolate.interp1d` is called. When interpolating\n",
      " |      along multiple existing dimensions, an attempt is made to decompose the\n",
      " |      interpolation into multiple 1-dimensional interpolations. If this is\n",
      " |      possible, :py:class:`scipy.interpolate.interp1d` is called. Otherwise,\n",
      " |      :py:func:`scipy.interpolate.interpn` is called.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      other : Dataset or DataArray\n",
      " |          Object with an 'indexes' attribute giving a mapping from dimension\n",
      " |          names to an 1d array-like, which provides coordinates upon\n",
      " |          which to index the variables in this dataset. Missing values are skipped.\n",
      " |      method : {\"linear\", \"nearest\", \"zero\", \"slinear\", \"quadratic\", \"cubic\", \"polynomial\",             \"barycentric\", \"krog\", \"pchip\", \"spline\", \"akima\"}, default: \"linear\"\n",
      " |          String indicating which method to use for interpolation:\n",
      " |      \n",
      " |          - 'linear': linear interpolation. Additional keyword\n",
      " |            arguments are passed to :py:func:`numpy.interp`\n",
      " |          - 'nearest', 'zero', 'slinear', 'quadratic', 'cubic', 'polynomial':\n",
      " |            are passed to :py:func:`scipy.interpolate.interp1d`. If\n",
      " |            ``method='polynomial'``, the ``order`` keyword argument must also be\n",
      " |            provided.\n",
      " |          - 'barycentric', 'krog', 'pchip', 'spline', 'akima': use their\n",
      " |            respective :py:class:`scipy.interpolate` classes.\n",
      " |      \n",
      " |      assume_sorted : bool, default: False\n",
      " |          If False, values of coordinates that are interpolated over can be\n",
      " |          in any order and they are sorted first. If True, interpolated\n",
      " |          coordinates are assumed to be an array of monotonically increasing\n",
      " |          values.\n",
      " |      kwargs : dict, optional\n",
      " |          Additional keyword passed to scipy's interpolator.\n",
      " |      method_non_numeric : {\"nearest\", \"pad\", \"ffill\", \"backfill\", \"bfill\"}, optional\n",
      " |          Method for non-numeric types. Passed on to :py:meth:`Dataset.reindex`.\n",
      " |          ``\"nearest\"`` is used by default.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      interpolated : Dataset\n",
      " |          Another dataset by interpolating this dataset's data along the\n",
      " |          coordinates of the other object.\n",
      " |      \n",
      " |      Notes\n",
      " |      -----\n",
      " |      scipy is required.\n",
      " |      If the dataset has object-type coordinates, reindex is used for these\n",
      " |      coordinates instead of the interpolation.\n",
      " |      \n",
      " |      See Also\n",
      " |      --------\n",
      " |      Dataset.interp\n",
      " |      Dataset.reindex_like\n",
      " |  \n",
      " |  interpolate_na(self: 'T_Dataset', dim: 'Hashable | None' = None, method: 'InterpOptions' = 'linear', limit: 'int' = None, use_coordinate: 'bool | Hashable' = True, max_gap: 'int | float | str | pd.Timedelta | np.timedelta64 | datetime.timedelta' = None, **kwargs: 'Any') -> 'T_Dataset'\n",
      " |      Fill in NaNs by interpolating according to different methods.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      dim : Hashable or None, optional\n",
      " |          Specifies the dimension along which to interpolate.\n",
      " |      method : {\"linear\", \"nearest\", \"zero\", \"slinear\", \"quadratic\", \"cubic\", \"polynomial\",             \"barycentric\", \"krog\", \"pchip\", \"spline\", \"akima\"}, default: \"linear\"\n",
      " |          String indicating which method to use for interpolation:\n",
      " |      \n",
      " |          - 'linear': linear interpolation. Additional keyword\n",
      " |            arguments are passed to :py:func:`numpy.interp`\n",
      " |          - 'nearest', 'zero', 'slinear', 'quadratic', 'cubic', 'polynomial':\n",
      " |            are passed to :py:func:`scipy.interpolate.interp1d`. If\n",
      " |            ``method='polynomial'``, the ``order`` keyword argument must also be\n",
      " |            provided.\n",
      " |          - 'barycentric', 'krog', 'pchip', 'spline', 'akima': use their\n",
      " |            respective :py:class:`scipy.interpolate` classes.\n",
      " |      \n",
      " |      use_coordinate : bool or Hashable, default: True\n",
      " |          Specifies which index to use as the x values in the interpolation\n",
      " |          formulated as `y = f(x)`. If False, values are treated as if\n",
      " |          eqaully-spaced along ``dim``. If True, the IndexVariable `dim` is\n",
      " |          used. If ``use_coordinate`` is a string, it specifies the name of a\n",
      " |          coordinate variariable to use as the index.\n",
      " |      limit : int, default: None\n",
      " |          Maximum number of consecutive NaNs to fill. Must be greater than 0\n",
      " |          or None for no limit. This filling is done regardless of the size of\n",
      " |          the gap in the data. To only interpolate over gaps less than a given length,\n",
      " |          see ``max_gap``.\n",
      " |      max_gap : int, float, str, pandas.Timedelta, numpy.timedelta64, datetime.timedelta, default: None\n",
      " |          Maximum size of gap, a continuous sequence of NaNs, that will be filled.\n",
      " |          Use None for no limit. When interpolating along a datetime64 dimension\n",
      " |          and ``use_coordinate=True``, ``max_gap`` can be one of the following:\n",
      " |      \n",
      " |          - a string that is valid input for pandas.to_timedelta\n",
      " |          - a :py:class:`numpy.timedelta64` object\n",
      " |          - a :py:class:`pandas.Timedelta` object\n",
      " |          - a :py:class:`datetime.timedelta` object\n",
      " |      \n",
      " |          Otherwise, ``max_gap`` must be an int or a float. Use of ``max_gap`` with unlabeled\n",
      " |          dimensions has not been implemented yet. Gap length is defined as the difference\n",
      " |          between coordinate values at the first data point after a gap and the last value\n",
      " |          before a gap. For gaps at the beginning (end), gap length is defined as the difference\n",
      " |          between coordinate values at the first (last) valid data point and the first (last) NaN.\n",
      " |          For example, consider::\n",
      " |      \n",
      " |              <xarray.DataArray (x: 9)>\n",
      " |              array([nan, nan, nan,  1., nan, nan,  4., nan, nan])\n",
      " |              Coordinates:\n",
      " |                * x        (x) int64 0 1 2 3 4 5 6 7 8\n",
      " |      \n",
      " |          The gap lengths are 3-0 = 3; 6-3 = 3; and 8-6 = 2 respectively\n",
      " |      **kwargs : dict, optional\n",
      " |          parameters passed verbatim to the underlying interpolation function\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      interpolated: Dataset\n",
      " |          Filled in Dataset.\n",
      " |      \n",
      " |      See Also\n",
      " |      --------\n",
      " |      numpy.interp\n",
      " |      scipy.interpolate\n",
      " |      \n",
      " |      Examples\n",
      " |      --------\n",
      " |      >>> ds = xr.Dataset(\n",
      " |      ...     {\n",
      " |      ...         \"A\": (\"x\", [np.nan, 2, 3, np.nan, 0]),\n",
      " |      ...         \"B\": (\"x\", [3, 4, np.nan, 1, 7]),\n",
      " |      ...         \"C\": (\"x\", [np.nan, np.nan, np.nan, 5, 0]),\n",
      " |      ...         \"D\": (\"x\", [np.nan, 3, np.nan, -1, 4]),\n",
      " |      ...     },\n",
      " |      ...     coords={\"x\": [0, 1, 2, 3, 4]},\n",
      " |      ... )\n",
      " |      >>> ds\n",
      " |      <xarray.Dataset>\n",
      " |      Dimensions:  (x: 5)\n",
      " |      Coordinates:\n",
      " |        * x        (x) int64 0 1 2 3 4\n",
      " |      Data variables:\n",
      " |          A        (x) float64 nan 2.0 3.0 nan 0.0\n",
      " |          B        (x) float64 3.0 4.0 nan 1.0 7.0\n",
      " |          C        (x) float64 nan nan nan 5.0 0.0\n",
      " |          D        (x) float64 nan 3.0 nan -1.0 4.0\n",
      " |      \n",
      " |      >>> ds.interpolate_na(dim=\"x\", method=\"linear\")\n",
      " |      <xarray.Dataset>\n",
      " |      Dimensions:  (x: 5)\n",
      " |      Coordinates:\n",
      " |        * x        (x) int64 0 1 2 3 4\n",
      " |      Data variables:\n",
      " |          A        (x) float64 nan 2.0 3.0 1.5 0.0\n",
      " |          B        (x) float64 3.0 4.0 2.5 1.0 7.0\n",
      " |          C        (x) float64 nan nan nan 5.0 0.0\n",
      " |          D        (x) float64 nan 3.0 1.0 -1.0 4.0\n",
      " |      \n",
      " |      >>> ds.interpolate_na(dim=\"x\", method=\"linear\", fill_value=\"extrapolate\")\n",
      " |      <xarray.Dataset>\n",
      " |      Dimensions:  (x: 5)\n",
      " |      Coordinates:\n",
      " |        * x        (x) int64 0 1 2 3 4\n",
      " |      Data variables:\n",
      " |          A        (x) float64 1.0 2.0 3.0 1.5 0.0\n",
      " |          B        (x) float64 3.0 4.0 2.5 1.0 7.0\n",
      " |          C        (x) float64 20.0 15.0 10.0 5.0 0.0\n",
      " |          D        (x) float64 5.0 3.0 1.0 -1.0 4.0\n",
      " |  \n",
      " |  isel(self: 'T_Dataset', indexers: 'Mapping[Any, Any] | None' = None, drop: 'bool' = False, missing_dims: 'ErrorOptionsWithWarn' = 'raise', **indexers_kwargs: 'Any') -> 'T_Dataset'\n",
      " |      Returns a new dataset with each array indexed along the specified\n",
      " |      dimension(s).\n",
      " |      \n",
      " |      This method selects values from each array using its `__getitem__`\n",
      " |      method, except this method does not require knowing the order of\n",
      " |      each array's dimensions.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      indexers : dict, optional\n",
      " |          A dict with keys matching dimensions and values given\n",
      " |          by integers, slice objects or arrays.\n",
      " |          indexer can be a integer, slice, array-like or DataArray.\n",
      " |          If DataArrays are passed as indexers, xarray-style indexing will be\n",
      " |          carried out. See :ref:`indexing` for the details.\n",
      " |          One of indexers or indexers_kwargs must be provided.\n",
      " |      drop : bool, default: False\n",
      " |          If ``drop=True``, drop coordinates variables indexed by integers\n",
      " |          instead of making them scalar.\n",
      " |      missing_dims : {\"raise\", \"warn\", \"ignore\"}, default: \"raise\"\n",
      " |          What to do if dimensions that should be selected from are not present in the\n",
      " |          Dataset:\n",
      " |          - \"raise\": raise an exception\n",
      " |          - \"warn\": raise a warning, and ignore the missing dimensions\n",
      " |          - \"ignore\": ignore the missing dimensions\n",
      " |      \n",
      " |      **indexers_kwargs : {dim: indexer, ...}, optional\n",
      " |          The keyword arguments form of ``indexers``.\n",
      " |          One of indexers or indexers_kwargs must be provided.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      obj : Dataset\n",
      " |          A new Dataset with the same contents as this dataset, except each\n",
      " |          array and dimension is indexed by the appropriate indexers.\n",
      " |          If indexer DataArrays have coordinates that do not conflict with\n",
      " |          this object, then these coordinates will be attached.\n",
      " |          In general, each array's data will be a view of the array's data\n",
      " |          in this dataset, unless vectorized indexing was triggered by using\n",
      " |          an array indexer, in which case the data will be a copy.\n",
      " |      \n",
      " |      See Also\n",
      " |      --------\n",
      " |      Dataset.sel\n",
      " |      DataArray.isel\n",
      " |  \n",
      " |  load(self: 'T_Dataset', **kwargs) -> 'T_Dataset'\n",
      " |      Manually trigger loading and/or computation of this dataset's data\n",
      " |      from disk or a remote source into memory and return this dataset.\n",
      " |      Unlike compute, the original dataset is modified and returned.\n",
      " |      \n",
      " |      Normally, it should not be necessary to call this method in user code,\n",
      " |      because all xarray functions should either work on deferred data or\n",
      " |      load data automatically. However, this method can be necessary when\n",
      " |      working with many file objects on disk.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      **kwargs : dict\n",
      " |          Additional keyword arguments passed on to ``dask.compute``.\n",
      " |      \n",
      " |      See Also\n",
      " |      --------\n",
      " |      dask.compute\n",
      " |  \n",
      " |  map(self: 'T_Dataset', func: 'Callable', keep_attrs: 'bool | None' = None, args: 'Iterable[Any]' = (), **kwargs: 'Any') -> 'T_Dataset'\n",
      " |      Apply a function to each data variable in this dataset\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      func : callable\n",
      " |          Function which can be called in the form `func(x, *args, **kwargs)`\n",
      " |          to transform each DataArray `x` in this dataset into another\n",
      " |          DataArray.\n",
      " |      keep_attrs : bool or None, optional\n",
      " |          If True, both the dataset's and variables' attributes (`attrs`) will be\n",
      " |          copied from the original objects to the new ones. If False, the new dataset\n",
      " |          and variables will be returned without copying the attributes.\n",
      " |      args : iterable, optional\n",
      " |          Positional arguments passed on to `func`.\n",
      " |      **kwargs : Any\n",
      " |          Keyword arguments passed on to `func`.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      applied : Dataset\n",
      " |          Resulting dataset from applying ``func`` to each data variable.\n",
      " |      \n",
      " |      Examples\n",
      " |      --------\n",
      " |      >>> da = xr.DataArray(np.random.randn(2, 3))\n",
      " |      >>> ds = xr.Dataset({\"foo\": da, \"bar\": (\"x\", [-1, 2])})\n",
      " |      >>> ds\n",
      " |      <xarray.Dataset>\n",
      " |      Dimensions:  (dim_0: 2, dim_1: 3, x: 2)\n",
      " |      Dimensions without coordinates: dim_0, dim_1, x\n",
      " |      Data variables:\n",
      " |          foo      (dim_0, dim_1) float64 1.764 0.4002 0.9787 2.241 1.868 -0.9773\n",
      " |          bar      (x) int64 -1 2\n",
      " |      >>> ds.map(np.fabs)\n",
      " |      <xarray.Dataset>\n",
      " |      Dimensions:  (dim_0: 2, dim_1: 3, x: 2)\n",
      " |      Dimensions without coordinates: dim_0, dim_1, x\n",
      " |      Data variables:\n",
      " |          foo      (dim_0, dim_1) float64 1.764 0.4002 0.9787 2.241 1.868 0.9773\n",
      " |          bar      (x) float64 1.0 2.0\n",
      " |  \n",
      " |  map_blocks(self, func: 'Callable[..., T_Xarray]', args: 'Sequence[Any]' = (), kwargs: 'Mapping[str, Any] | None' = None, template: 'DataArray | Dataset | None' = None) -> 'T_Xarray'\n",
      " |      Apply a function to each block of this Dataset.\n",
      " |      \n",
      " |      .. warning::\n",
      " |          This method is experimental and its signature may change.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      func : callable\n",
      " |          User-provided function that accepts a Dataset as its first\n",
      " |          parameter. The function will receive a subset or 'block' of this Dataset (see below),\n",
      " |          corresponding to one chunk along each chunked dimension. ``func`` will be\n",
      " |          executed as ``func(subset_dataset, *subset_args, **kwargs)``.\n",
      " |      \n",
      " |          This function must return either a single DataArray or a single Dataset.\n",
      " |      \n",
      " |          This function cannot add a new chunked dimension.\n",
      " |      args : sequence\n",
      " |          Passed to func after unpacking and subsetting any xarray objects by blocks.\n",
      " |          xarray objects in args must be aligned with obj, otherwise an error is raised.\n",
      " |      kwargs : Mapping or None\n",
      " |          Passed verbatim to func after unpacking. xarray objects, if any, will not be\n",
      " |          subset to blocks. Passing dask collections in kwargs is not allowed.\n",
      " |      template : DataArray, Dataset or None, optional\n",
      " |          xarray object representing the final result after compute is called. If not provided,\n",
      " |          the function will be first run on mocked-up data, that looks like this object but\n",
      " |          has sizes 0, to determine properties of the returned object such as dtype,\n",
      " |          variable names, attributes, new dimensions and new indexes (if any).\n",
      " |          ``template`` must be provided if the function changes the size of existing dimensions.\n",
      " |          When provided, ``attrs`` on variables in `template` are copied over to the result. Any\n",
      " |          ``attrs`` set by ``func`` will be ignored.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      A single DataArray or Dataset with dask backend, reassembled from the outputs of the\n",
      " |      function.\n",
      " |      \n",
      " |      Notes\n",
      " |      -----\n",
      " |      This function is designed for when ``func`` needs to manipulate a whole xarray object\n",
      " |      subset to each block. Each block is loaded into memory. In the more common case where\n",
      " |      ``func`` can work on numpy arrays, it is recommended to use ``apply_ufunc``.\n",
      " |      \n",
      " |      If none of the variables in this object is backed by dask arrays, calling this function is\n",
      " |      equivalent to calling ``func(obj, *args, **kwargs)``.\n",
      " |      \n",
      " |      See Also\n",
      " |      --------\n",
      " |      dask.array.map_blocks, xarray.apply_ufunc, xarray.Dataset.map_blocks\n",
      " |      xarray.DataArray.map_blocks\n",
      " |      \n",
      " |      Examples\n",
      " |      --------\n",
      " |      Calculate an anomaly from climatology using ``.groupby()``. Using\n",
      " |      ``xr.map_blocks()`` allows for parallel operations with knowledge of ``xarray``,\n",
      " |      its indices, and its methods like ``.groupby()``.\n",
      " |      \n",
      " |      >>> def calculate_anomaly(da, groupby_type=\"time.month\"):\n",
      " |      ...     gb = da.groupby(groupby_type)\n",
      " |      ...     clim = gb.mean(dim=\"time\")\n",
      " |      ...     return gb - clim\n",
      " |      ...\n",
      " |      >>> time = xr.cftime_range(\"1990-01\", \"1992-01\", freq=\"M\")\n",
      " |      >>> month = xr.DataArray(time.month, coords={\"time\": time}, dims=[\"time\"])\n",
      " |      >>> np.random.seed(123)\n",
      " |      >>> array = xr.DataArray(\n",
      " |      ...     np.random.rand(len(time)),\n",
      " |      ...     dims=[\"time\"],\n",
      " |      ...     coords={\"time\": time, \"month\": month},\n",
      " |      ... ).chunk()\n",
      " |      >>> ds = xr.Dataset({\"a\": array})\n",
      " |      >>> ds.map_blocks(calculate_anomaly, template=ds).compute()\n",
      " |      <xarray.Dataset>\n",
      " |      Dimensions:  (time: 24)\n",
      " |      Coordinates:\n",
      " |        * time     (time) object 1990-01-31 00:00:00 ... 1991-12-31 00:00:00\n",
      " |          month    (time) int64 1 2 3 4 5 6 7 8 9 10 11 12 1 2 3 4 5 6 7 8 9 10 11 12\n",
      " |      Data variables:\n",
      " |          a        (time) float64 0.1289 0.1132 -0.0856 ... 0.2287 0.1906 -0.05901\n",
      " |      \n",
      " |      Note that one must explicitly use ``args=[]`` and ``kwargs={}`` to pass arguments\n",
      " |      to the function being applied in ``xr.map_blocks()``:\n",
      " |      \n",
      " |      >>> ds.map_blocks(\n",
      " |      ...     calculate_anomaly,\n",
      " |      ...     kwargs={\"groupby_type\": \"time.year\"},\n",
      " |      ...     template=ds,\n",
      " |      ... )\n",
      " |      <xarray.Dataset>\n",
      " |      Dimensions:  (time: 24)\n",
      " |      Coordinates:\n",
      " |        * time     (time) object 1990-01-31 00:00:00 ... 1991-12-31 00:00:00\n",
      " |          month    (time) int64 dask.array<chunksize=(24,), meta=np.ndarray>\n",
      " |      Data variables:\n",
      " |          a        (time) float64 dask.array<chunksize=(24,), meta=np.ndarray>\n",
      " |  \n",
      " |  merge(self: 'T_Dataset', other: 'CoercibleMapping | DataArray', overwrite_vars: 'Hashable | Iterable[Hashable]' = frozenset(), compat: 'CompatOptions' = 'no_conflicts', join: 'JoinOptions' = 'outer', fill_value: 'Any' = <NA>, combine_attrs: 'CombineAttrsOptions' = 'override') -> 'T_Dataset'\n",
      " |      Merge the arrays of two datasets into a single dataset.\n",
      " |      \n",
      " |      This method generally does not allow for overriding data, with the\n",
      " |      exception of attributes, which are ignored on the second dataset.\n",
      " |      Variables with the same name are checked for conflicts via the equals\n",
      " |      or identical methods.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      other : Dataset or mapping\n",
      " |          Dataset or variables to merge with this dataset.\n",
      " |      overwrite_vars : hashable or iterable of hashable, optional\n",
      " |          If provided, update variables of these name(s) without checking for\n",
      " |          conflicts in this dataset.\n",
      " |      compat : {\"broadcast_equals\", \"equals\", \"identical\",                   \"no_conflicts\"}, optional\n",
      " |          String indicating how to compare variables of the same name for\n",
      " |          potential conflicts:\n",
      " |      \n",
      " |          - 'broadcast_equals': all values must be equal when variables are\n",
      " |            broadcast against each other to ensure common dimensions.\n",
      " |          - 'equals': all values and dimensions must be the same.\n",
      " |          - 'identical': all values, dimensions and attributes must be the\n",
      " |            same.\n",
      " |          - 'no_conflicts': only values which are not null in both datasets\n",
      " |            must be equal. The returned dataset then contains the combination\n",
      " |            of all non-null values.\n",
      " |      \n",
      " |      join : {\"outer\", \"inner\", \"left\", \"right\", \"exact\"}, optional\n",
      " |          Method for joining ``self`` and ``other`` along shared dimensions:\n",
      " |      \n",
      " |          - 'outer': use the union of the indexes\n",
      " |          - 'inner': use the intersection of the indexes\n",
      " |          - 'left': use indexes from ``self``\n",
      " |          - 'right': use indexes from ``other``\n",
      " |          - 'exact': error instead of aligning non-equal indexes\n",
      " |      \n",
      " |      fill_value : scalar or dict-like, optional\n",
      " |          Value to use for newly missing values. If a dict-like, maps\n",
      " |          variable names (including coordinates) to fill values.\n",
      " |      combine_attrs : {\"drop\", \"identical\", \"no_conflicts\", \"drop_conflicts\",                         \"override\"} or callable, default: \"override\"\n",
      " |          A callable or a string indicating how to combine attrs of the objects being\n",
      " |          merged:\n",
      " |      \n",
      " |          - \"drop\": empty attrs on returned Dataset.\n",
      " |          - \"identical\": all attrs must be the same on every object.\n",
      " |          - \"no_conflicts\": attrs from all objects are combined, any that have\n",
      " |            the same name must also have the same value.\n",
      " |          - \"drop_conflicts\": attrs from all objects are combined, any that have\n",
      " |            the same name but different values are dropped.\n",
      " |          - \"override\": skip comparing and copy attrs from the first dataset to\n",
      " |            the result.\n",
      " |      \n",
      " |          If a callable, it must expect a sequence of ``attrs`` dicts and a context object\n",
      " |          as its only parameters.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      merged : Dataset\n",
      " |          Merged dataset.\n",
      " |      \n",
      " |      Raises\n",
      " |      ------\n",
      " |      MergeError\n",
      " |          If any variables conflict (see ``compat``).\n",
      " |      \n",
      " |      See Also\n",
      " |      --------\n",
      " |      Dataset.update\n",
      " |  \n",
      " |  pad(self: 'T_Dataset', pad_width: 'Mapping[Any, int | tuple[int, int]]' = None, mode: 'PadModeOptions' = 'constant', stat_length: 'int | tuple[int, int] | Mapping[Any, tuple[int, int]] | None' = None, constant_values: 'float | tuple[float, float] | Mapping[Any, tuple[float, float]] | None' = None, end_values: 'int | tuple[int, int] | Mapping[Any, tuple[int, int]] | None' = None, reflect_type: 'PadReflectOptions' = None, **pad_width_kwargs: 'Any') -> 'T_Dataset'\n",
      " |      Pad this dataset along one or more dimensions.\n",
      " |      \n",
      " |      .. warning::\n",
      " |          This function is experimental and its behaviour is likely to change\n",
      " |          especially regarding padding of dimension coordinates (or IndexVariables).\n",
      " |      \n",
      " |      When using one of the modes (\"edge\", \"reflect\", \"symmetric\", \"wrap\"),\n",
      " |      coordinates will be padded with the same mode, otherwise coordinates\n",
      " |      are padded using the \"constant\" mode with fill_value dtypes.NA.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      pad_width : mapping of hashable to tuple of int\n",
      " |          Mapping with the form of {dim: (pad_before, pad_after)}\n",
      " |          describing the number of values padded along each dimension.\n",
      " |          {dim: pad} is a shortcut for pad_before = pad_after = pad\n",
      " |      mode : {\"constant\", \"edge\", \"linear_ramp\", \"maximum\", \"mean\", \"median\",             \"minimum\", \"reflect\", \"symmetric\", \"wrap\"}, default: \"constant\"\n",
      " |          How to pad the DataArray (taken from numpy docs):\n",
      " |      \n",
      " |          - \"constant\": Pads with a constant value.\n",
      " |          - \"edge\": Pads with the edge values of array.\n",
      " |          - \"linear_ramp\": Pads with the linear ramp between end_value and the\n",
      " |            array edge value.\n",
      " |          - \"maximum\": Pads with the maximum value of all or part of the\n",
      " |            vector along each axis.\n",
      " |          - \"mean\": Pads with the mean value of all or part of the\n",
      " |            vector along each axis.\n",
      " |          - \"median\": Pads with the median value of all or part of the\n",
      " |            vector along each axis.\n",
      " |          - \"minimum\": Pads with the minimum value of all or part of the\n",
      " |            vector along each axis.\n",
      " |          - \"reflect\": Pads with the reflection of the vector mirrored on\n",
      " |            the first and last values of the vector along each axis.\n",
      " |          - \"symmetric\": Pads with the reflection of the vector mirrored\n",
      " |            along the edge of the array.\n",
      " |          - \"wrap\": Pads with the wrap of the vector along the axis.\n",
      " |            The first values are used to pad the end and the\n",
      " |            end values are used to pad the beginning.\n",
      " |      \n",
      " |      stat_length : int, tuple or mapping of hashable to tuple, default: None\n",
      " |          Used in 'maximum', 'mean', 'median', and 'minimum'.  Number of\n",
      " |          values at edge of each axis used to calculate the statistic value.\n",
      " |          {dim_1: (before_1, after_1), ... dim_N: (before_N, after_N)} unique\n",
      " |          statistic lengths along each dimension.\n",
      " |          ((before, after),) yields same before and after statistic lengths\n",
      " |          for each dimension.\n",
      " |          (stat_length,) or int is a shortcut for before = after = statistic\n",
      " |          length for all axes.\n",
      " |          Default is ``None``, to use the entire axis.\n",
      " |      constant_values : scalar, tuple or mapping of hashable to tuple, default: 0\n",
      " |          Used in 'constant'.  The values to set the padded values for each\n",
      " |          axis.\n",
      " |          ``{dim_1: (before_1, after_1), ... dim_N: (before_N, after_N)}`` unique\n",
      " |          pad constants along each dimension.\n",
      " |          ``((before, after),)`` yields same before and after constants for each\n",
      " |          dimension.\n",
      " |          ``(constant,)`` or ``constant`` is a shortcut for ``before = after = constant`` for\n",
      " |          all dimensions.\n",
      " |          Default is 0.\n",
      " |      end_values : scalar, tuple or mapping of hashable to tuple, default: 0\n",
      " |          Used in 'linear_ramp'.  The values used for the ending value of the\n",
      " |          linear_ramp and that will form the edge of the padded array.\n",
      " |          ``{dim_1: (before_1, after_1), ... dim_N: (before_N, after_N)}`` unique\n",
      " |          end values along each dimension.\n",
      " |          ``((before, after),)`` yields same before and after end values for each\n",
      " |          axis.\n",
      " |          ``(constant,)`` or ``constant`` is a shortcut for ``before = after = constant`` for\n",
      " |          all axes.\n",
      " |          Default is 0.\n",
      " |      reflect_type : {\"even\", \"odd\", None}, optional\n",
      " |          Used in \"reflect\", and \"symmetric\".  The \"even\" style is the\n",
      " |          default with an unaltered reflection around the edge value.  For\n",
      " |          the \"odd\" style, the extended part of the array is created by\n",
      " |          subtracting the reflected values from two times the edge value.\n",
      " |      **pad_width_kwargs\n",
      " |          The keyword arguments form of ``pad_width``.\n",
      " |          One of ``pad_width`` or ``pad_width_kwargs`` must be provided.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      padded : Dataset\n",
      " |          Dataset with the padded coordinates and data.\n",
      " |      \n",
      " |      See Also\n",
      " |      --------\n",
      " |      Dataset.shift, Dataset.roll, Dataset.bfill, Dataset.ffill, numpy.pad, dask.array.pad\n",
      " |      \n",
      " |      Notes\n",
      " |      -----\n",
      " |      By default when ``mode=\"constant\"`` and ``constant_values=None``, integer types will be\n",
      " |      promoted to ``float`` and padded with ``np.nan``. To avoid type promotion\n",
      " |      specify ``constant_values=np.nan``\n",
      " |      \n",
      " |      Padding coordinates will drop their corresponding index (if any) and will reset default\n",
      " |      indexes for dimension coordinates.\n",
      " |      \n",
      " |      Examples\n",
      " |      --------\n",
      " |      >>> ds = xr.Dataset({\"foo\": (\"x\", range(5))})\n",
      " |      >>> ds.pad(x=(1, 2))\n",
      " |      <xarray.Dataset>\n",
      " |      Dimensions:  (x: 8)\n",
      " |      Dimensions without coordinates: x\n",
      " |      Data variables:\n",
      " |          foo      (x) float64 nan 0.0 1.0 2.0 3.0 4.0 nan nan\n",
      " |  \n",
      " |  persist(self: 'T_Dataset', **kwargs) -> 'T_Dataset'\n",
      " |      Trigger computation, keeping data as dask arrays\n",
      " |      \n",
      " |      This operation can be used to trigger computation on underlying dask\n",
      " |      arrays, similar to ``.compute()`` or ``.load()``.  However this\n",
      " |      operation keeps the data as dask arrays. This is particularly useful\n",
      " |      when using the dask.distributed scheduler and you want to load a large\n",
      " |      amount of data into distributed memory.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      **kwargs : dict\n",
      " |          Additional keyword arguments passed on to ``dask.persist``.\n",
      " |      \n",
      " |      See Also\n",
      " |      --------\n",
      " |      dask.persist\n",
      " |  \n",
      " |  polyfit(self: 'T_Dataset', dim: 'Hashable', deg: 'int', skipna: 'bool | None' = None, rcond: 'float | None' = None, w: 'Hashable | Any' = None, full: 'bool' = False, cov: \"bool | Literal['unscaled']\" = False) -> 'T_Dataset'\n",
      " |      Least squares polynomial fit.\n",
      " |      \n",
      " |      This replicates the behaviour of `numpy.polyfit` but differs by skipping\n",
      " |      invalid values when `skipna = True`.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      dim : hashable\n",
      " |          Coordinate along which to fit the polynomials.\n",
      " |      deg : int\n",
      " |          Degree of the fitting polynomial.\n",
      " |      skipna : bool or None, optional\n",
      " |          If True, removes all invalid values before fitting each 1D slices of the array.\n",
      " |          Default is True if data is stored in a dask.array or if there is any\n",
      " |          invalid values, False otherwise.\n",
      " |      rcond : float or None, optional\n",
      " |          Relative condition number to the fit.\n",
      " |      w : hashable or Any, optional\n",
      " |          Weights to apply to the y-coordinate of the sample points.\n",
      " |          Can be an array-like object or the name of a coordinate in the dataset.\n",
      " |      full : bool, default: False\n",
      " |          Whether to return the residuals, matrix rank and singular values in addition\n",
      " |          to the coefficients.\n",
      " |      cov : bool or \"unscaled\", default: False\n",
      " |          Whether to return to the covariance matrix in addition to the coefficients.\n",
      " |          The matrix is not scaled if `cov='unscaled'`.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      polyfit_results : Dataset\n",
      " |          A single dataset which contains (for each \"var\" in the input dataset):\n",
      " |      \n",
      " |          [var]_polyfit_coefficients\n",
      " |              The coefficients of the best fit for each variable in this dataset.\n",
      " |          [var]_polyfit_residuals\n",
      " |              The residuals of the least-square computation for each variable (only included if `full=True`)\n",
      " |              When the matrix rank is deficient, np.nan is returned.\n",
      " |          [dim]_matrix_rank\n",
      " |              The effective rank of the scaled Vandermonde coefficient matrix (only included if `full=True`)\n",
      " |              The rank is computed ignoring the NaN values that might be skipped.\n",
      " |          [dim]_singular_values\n",
      " |              The singular values of the scaled Vandermonde coefficient matrix (only included if `full=True`)\n",
      " |          [var]_polyfit_covariance\n",
      " |              The covariance matrix of the polynomial coefficient estimates (only included if `full=False` and `cov=True`)\n",
      " |      \n",
      " |      Warns\n",
      " |      -----\n",
      " |      RankWarning\n",
      " |          The rank of the coefficient matrix in the least-squares fit is deficient.\n",
      " |          The warning is not raised with in-memory (not dask) data and `full=True`.\n",
      " |      \n",
      " |      See Also\n",
      " |      --------\n",
      " |      numpy.polyfit\n",
      " |      numpy.polyval\n",
      " |      xarray.polyval\n",
      " |  \n",
      " |  quantile(self: 'T_Dataset', q: 'ArrayLike', dim: 'str | Iterable[Hashable] | None' = None, method: 'QUANTILE_METHODS' = 'linear', numeric_only: 'bool' = False, keep_attrs: 'bool' = None, skipna: 'bool' = None, interpolation: 'QUANTILE_METHODS' = None) -> 'T_Dataset'\n",
      " |      Compute the qth quantile of the data along the specified dimension.\n",
      " |      \n",
      " |      Returns the qth quantiles(s) of the array elements for each variable\n",
      " |      in the Dataset.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      q : float or array-like of float\n",
      " |          Quantile to compute, which must be between 0 and 1 inclusive.\n",
      " |      dim : str or Iterable of Hashable, optional\n",
      " |          Dimension(s) over which to apply quantile.\n",
      " |      method : str, default: \"linear\"\n",
      " |          This optional parameter specifies the interpolation method to use when the\n",
      " |          desired quantile lies between two data points. The options sorted by their R\n",
      " |          type as summarized in the H&F paper [1]_ are:\n",
      " |      \n",
      " |              1. \"inverted_cdf\" (*)\n",
      " |              2. \"averaged_inverted_cdf\" (*)\n",
      " |              3. \"closest_observation\" (*)\n",
      " |              4. \"interpolated_inverted_cdf\" (*)\n",
      " |              5. \"hazen\" (*)\n",
      " |              6. \"weibull\" (*)\n",
      " |              7. \"linear\"  (default)\n",
      " |              8. \"median_unbiased\" (*)\n",
      " |              9. \"normal_unbiased\" (*)\n",
      " |      \n",
      " |          The first three methods are discontiuous.  The following discontinuous\n",
      " |          variations of the default \"linear\" (7.) option are also available:\n",
      " |      \n",
      " |              * \"lower\"\n",
      " |              * \"higher\"\n",
      " |              * \"midpoint\"\n",
      " |              * \"nearest\"\n",
      " |      \n",
      " |          See :py:func:`numpy.quantile` or [1]_ for a description. Methods marked with\n",
      " |          an asterix require numpy version 1.22 or newer. The \"method\" argument was\n",
      " |          previously called \"interpolation\", renamed in accordance with numpy\n",
      " |          version 1.22.0.\n",
      " |      \n",
      " |      keep_attrs : bool, optional\n",
      " |          If True, the dataset's attributes (`attrs`) will be copied from\n",
      " |          the original object to the new one.  If False (default), the new\n",
      " |          object will be returned without attributes.\n",
      " |      numeric_only : bool, optional\n",
      " |          If True, only apply ``func`` to variables with a numeric dtype.\n",
      " |      skipna : bool, optional\n",
      " |          If True, skip missing values (as marked by NaN). By default, only\n",
      " |          skips missing values for float dtypes; other dtypes either do not\n",
      " |          have a sentinel missing value (int) or skipna=True has not been\n",
      " |          implemented (object, datetime64 or timedelta64).\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      quantiles : Dataset\n",
      " |          If `q` is a single quantile, then the result is a scalar for each\n",
      " |          variable in data_vars. If multiple percentiles are given, first\n",
      " |          axis of the result corresponds to the quantile and a quantile\n",
      " |          dimension is added to the return Dataset. The other dimensions are\n",
      " |          the dimensions that remain after the reduction of the array.\n",
      " |      \n",
      " |      See Also\n",
      " |      --------\n",
      " |      numpy.nanquantile, numpy.quantile, pandas.Series.quantile, DataArray.quantile\n",
      " |      \n",
      " |      Examples\n",
      " |      --------\n",
      " |      >>> ds = xr.Dataset(\n",
      " |      ...     {\"a\": ((\"x\", \"y\"), [[0.7, 4.2, 9.4, 1.5], [6.5, 7.3, 2.6, 1.9]])},\n",
      " |      ...     coords={\"x\": [7, 9], \"y\": [1, 1.5, 2, 2.5]},\n",
      " |      ... )\n",
      " |      >>> ds.quantile(0)  # or ds.quantile(0, dim=...)\n",
      " |      <xarray.Dataset>\n",
      " |      Dimensions:   ()\n",
      " |      Coordinates:\n",
      " |          quantile  float64 0.0\n",
      " |      Data variables:\n",
      " |          a         float64 0.7\n",
      " |      >>> ds.quantile(0, dim=\"x\")\n",
      " |      <xarray.Dataset>\n",
      " |      Dimensions:   (y: 4)\n",
      " |      Coordinates:\n",
      " |        * y         (y) float64 1.0 1.5 2.0 2.5\n",
      " |          quantile  float64 0.0\n",
      " |      Data variables:\n",
      " |          a         (y) float64 0.7 4.2 2.6 1.5\n",
      " |      >>> ds.quantile([0, 0.5, 1])\n",
      " |      <xarray.Dataset>\n",
      " |      Dimensions:   (quantile: 3)\n",
      " |      Coordinates:\n",
      " |        * quantile  (quantile) float64 0.0 0.5 1.0\n",
      " |      Data variables:\n",
      " |          a         (quantile) float64 0.7 3.4 9.4\n",
      " |      >>> ds.quantile([0, 0.5, 1], dim=\"x\")\n",
      " |      <xarray.Dataset>\n",
      " |      Dimensions:   (quantile: 3, y: 4)\n",
      " |      Coordinates:\n",
      " |        * y         (y) float64 1.0 1.5 2.0 2.5\n",
      " |        * quantile  (quantile) float64 0.0 0.5 1.0\n",
      " |      Data variables:\n",
      " |          a         (quantile, y) float64 0.7 4.2 2.6 1.5 3.6 ... 1.7 6.5 7.3 9.4 1.9\n",
      " |      \n",
      " |      References\n",
      " |      ----------\n",
      " |      .. [1] R. J. Hyndman and Y. Fan,\n",
      " |         \"Sample quantiles in statistical packages,\"\n",
      " |         The American Statistician, 50(4), pp. 361-365, 1996\n",
      " |  \n",
      " |  query(self: 'T_Dataset', queries: 'Mapping[Any, Any] | None' = None, parser: 'QueryParserOptions' = 'pandas', engine: 'QueryEngineOptions' = None, missing_dims: 'ErrorOptionsWithWarn' = 'raise', **queries_kwargs: 'Any') -> 'T_Dataset'\n",
      " |      Return a new dataset with each array indexed along the specified\n",
      " |      dimension(s), where the indexers are given as strings containing\n",
      " |      Python expressions to be evaluated against the data variables in the\n",
      " |      dataset.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      queries : dict-like, optional\n",
      " |          A dict-like with keys matching dimensions and values given by strings\n",
      " |          containing Python expressions to be evaluated against the data variables\n",
      " |          in the dataset. The expressions will be evaluated using the pandas\n",
      " |          eval() function, and can contain any valid Python expressions but cannot\n",
      " |          contain any Python statements.\n",
      " |      parser : {\"pandas\", \"python\"}, default: \"pandas\"\n",
      " |          The parser to use to construct the syntax tree from the expression.\n",
      " |          The default of 'pandas' parses code slightly different than standard\n",
      " |          Python. Alternatively, you can parse an expression using the 'python'\n",
      " |          parser to retain strict Python semantics.\n",
      " |      engine : {\"python\", \"numexpr\", None}, default: None\n",
      " |          The engine used to evaluate the expression. Supported engines are:\n",
      " |      \n",
      " |          - None: tries to use numexpr, falls back to python\n",
      " |          - \"numexpr\": evaluates expressions using numexpr\n",
      " |          - \"python\": performs operations as if you had evald in top level python\n",
      " |      \n",
      " |      missing_dims : {\"raise\", \"warn\", \"ignore\"}, default: \"raise\"\n",
      " |          What to do if dimensions that should be selected from are not present in the\n",
      " |          Dataset:\n",
      " |      \n",
      " |          - \"raise\": raise an exception\n",
      " |          - \"warn\": raise a warning, and ignore the missing dimensions\n",
      " |          - \"ignore\": ignore the missing dimensions\n",
      " |      \n",
      " |      **queries_kwargs : {dim: query, ...}, optional\n",
      " |          The keyword arguments form of ``queries``.\n",
      " |          One of queries or queries_kwargs must be provided.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      obj : Dataset\n",
      " |          A new Dataset with the same contents as this dataset, except each\n",
      " |          array and dimension is indexed by the results of the appropriate\n",
      " |          queries.\n",
      " |      \n",
      " |      See Also\n",
      " |      --------\n",
      " |      Dataset.isel\n",
      " |      pandas.eval\n",
      " |      \n",
      " |      Examples\n",
      " |      --------\n",
      " |      >>> a = np.arange(0, 5, 1)\n",
      " |      >>> b = np.linspace(0, 1, 5)\n",
      " |      >>> ds = xr.Dataset({\"a\": (\"x\", a), \"b\": (\"x\", b)})\n",
      " |      >>> ds\n",
      " |      <xarray.Dataset>\n",
      " |      Dimensions:  (x: 5)\n",
      " |      Dimensions without coordinates: x\n",
      " |      Data variables:\n",
      " |          a        (x) int64 0 1 2 3 4\n",
      " |          b        (x) float64 0.0 0.25 0.5 0.75 1.0\n",
      " |      >>> ds.query(x=\"a > 2\")\n",
      " |      <xarray.Dataset>\n",
      " |      Dimensions:  (x: 2)\n",
      " |      Dimensions without coordinates: x\n",
      " |      Data variables:\n",
      " |          a        (x) int64 3 4\n",
      " |          b        (x) float64 0.75 1.0\n",
      " |  \n",
      " |  rank(self: 'T_Dataset', dim: 'Hashable', pct: 'bool' = False, keep_attrs: 'bool | None' = None) -> 'T_Dataset'\n",
      " |      Ranks the data.\n",
      " |      \n",
      " |      Equal values are assigned a rank that is the average of the ranks that\n",
      " |      would have been otherwise assigned to all of the values within\n",
      " |      that set.\n",
      " |      Ranks begin at 1, not 0. If pct is True, computes percentage ranks.\n",
      " |      \n",
      " |      NaNs in the input array are returned as NaNs.\n",
      " |      \n",
      " |      The `bottleneck` library is required.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      dim : Hashable\n",
      " |          Dimension over which to compute rank.\n",
      " |      pct : bool, default: False\n",
      " |          If True, compute percentage ranks, otherwise compute integer ranks.\n",
      " |      keep_attrs : bool or None, optional\n",
      " |          If True, the dataset's attributes (`attrs`) will be copied from\n",
      " |          the original object to the new one.  If False, the new\n",
      " |          object will be returned without attributes.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      ranked : Dataset\n",
      " |          Variables that do not depend on `dim` are dropped.\n",
      " |  \n",
      " |  reduce(self: 'T_Dataset', func: 'Callable', dim: 'Hashable | Iterable[Hashable]' = None, *, keep_attrs: 'bool | None' = None, keepdims: 'bool' = False, numeric_only: 'bool' = False, **kwargs: 'Any') -> 'T_Dataset'\n",
      " |      Reduce this dataset by applying `func` along some dimension(s).\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      func : callable\n",
      " |          Function which can be called in the form\n",
      " |          `f(x, axis=axis, **kwargs)` to return the result of reducing an\n",
      " |          np.ndarray over an integer valued axis.\n",
      " |      dim : str or sequence of str, optional\n",
      " |          Dimension(s) over which to apply `func`.  By default `func` is\n",
      " |          applied over all dimensions.\n",
      " |      keep_attrs : bool or None, optional\n",
      " |          If True, the dataset's attributes (`attrs`) will be copied from\n",
      " |          the original object to the new one.  If False (default), the new\n",
      " |          object will be returned without attributes.\n",
      " |      keepdims : bool, default: False\n",
      " |          If True, the dimensions which are reduced are left in the result\n",
      " |          as dimensions of size one. Coordinates that use these dimensions\n",
      " |          are removed.\n",
      " |      numeric_only : bool, default: False\n",
      " |          If True, only apply ``func`` to variables with a numeric dtype.\n",
      " |      **kwargs : Any\n",
      " |          Additional keyword arguments passed on to ``func``.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      reduced : Dataset\n",
      " |          Dataset with this object's DataArrays replaced with new DataArrays\n",
      " |          of summarized data and the indicated dimension(s) removed.\n",
      " |  \n",
      " |  reindex(self: 'T_Dataset', indexers: 'Mapping[Any, Any] | None' = None, method: 'ReindexMethodOptions' = None, tolerance: 'int | float | Iterable[int | float] | None' = None, copy: 'bool' = True, fill_value: 'Any' = <NA>, **indexers_kwargs: 'Any') -> 'T_Dataset'\n",
      " |      Conform this object onto a new set of indexes, filling in\n",
      " |      missing values with ``fill_value``. The default fill value is NaN.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      indexers : dict, optional\n",
      " |          Dictionary with keys given by dimension names and values given by\n",
      " |          arrays of coordinates tick labels. Any mis-matched coordinate\n",
      " |          values will be filled in with NaN, and any mis-matched dimension\n",
      " |          names will simply be ignored.\n",
      " |          One of indexers or indexers_kwargs must be provided.\n",
      " |      method : {None, \"nearest\", \"pad\", \"ffill\", \"backfill\", \"bfill\", None}, optional\n",
      " |          Method to use for filling index values in ``indexers`` not found in\n",
      " |          this dataset:\n",
      " |      \n",
      " |          - None (default): don't fill gaps\n",
      " |          - \"pad\" / \"ffill\": propagate last valid index value forward\n",
      " |          - \"backfill\" / \"bfill\": propagate next valid index value backward\n",
      " |          - \"nearest\": use nearest valid index value\n",
      " |      \n",
      " |      tolerance : optional\n",
      " |          Maximum distance between original and new labels for inexact\n",
      " |          matches. The values of the index at the matching locations must\n",
      " |          satisfy the equation ``abs(index[indexer] - target) <= tolerance``.\n",
      " |          Tolerance may be a scalar value, which applies the same tolerance\n",
      " |          to all values, or list-like, which applies variable tolerance per\n",
      " |          element. List-like must be the same size as the index and its dtype\n",
      " |          must exactly match the indexs type.\n",
      " |      copy : bool, default: True\n",
      " |          If ``copy=True``, data in the return value is always copied. If\n",
      " |          ``copy=False`` and reindexing is unnecessary, or can be performed\n",
      " |          with only slice operations, then the output may share memory with\n",
      " |          the input. In either case, a new xarray object is always returned.\n",
      " |      fill_value : scalar or dict-like, optional\n",
      " |          Value to use for newly missing values. If a dict-like,\n",
      " |          maps variable names (including coordinates) to fill values.\n",
      " |      sparse : bool, default: False\n",
      " |          use sparse-array.\n",
      " |      **indexers_kwargs : {dim: indexer, ...}, optional\n",
      " |          Keyword arguments in the same form as ``indexers``.\n",
      " |          One of indexers or indexers_kwargs must be provided.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      reindexed : Dataset\n",
      " |          Another dataset, with this dataset's data but replaced coordinates.\n",
      " |      \n",
      " |      See Also\n",
      " |      --------\n",
      " |      Dataset.reindex_like\n",
      " |      align\n",
      " |      pandas.Index.get_indexer\n",
      " |      \n",
      " |      Examples\n",
      " |      --------\n",
      " |      Create a dataset with some fictional data.\n",
      " |      \n",
      " |      >>> x = xr.Dataset(\n",
      " |      ...     {\n",
      " |      ...         \"temperature\": (\"station\", 20 * np.random.rand(4)),\n",
      " |      ...         \"pressure\": (\"station\", 500 * np.random.rand(4)),\n",
      " |      ...     },\n",
      " |      ...     coords={\"station\": [\"boston\", \"nyc\", \"seattle\", \"denver\"]},\n",
      " |      ... )\n",
      " |      >>> x\n",
      " |      <xarray.Dataset>\n",
      " |      Dimensions:      (station: 4)\n",
      " |      Coordinates:\n",
      " |        * station      (station) <U7 'boston' 'nyc' 'seattle' 'denver'\n",
      " |      Data variables:\n",
      " |          temperature  (station) float64 10.98 14.3 12.06 10.9\n",
      " |          pressure     (station) float64 211.8 322.9 218.8 445.9\n",
      " |      >>> x.indexes\n",
      " |      Indexes:\n",
      " |      station: Index(['boston', 'nyc', 'seattle', 'denver'], dtype='object', name='station')\n",
      " |      \n",
      " |      Create a new index and reindex the dataset. By default values in the new index that\n",
      " |      do not have corresponding records in the dataset are assigned `NaN`.\n",
      " |      \n",
      " |      >>> new_index = [\"boston\", \"austin\", \"seattle\", \"lincoln\"]\n",
      " |      >>> x.reindex({\"station\": new_index})\n",
      " |      <xarray.Dataset>\n",
      " |      Dimensions:      (station: 4)\n",
      " |      Coordinates:\n",
      " |        * station      (station) <U7 'boston' 'austin' 'seattle' 'lincoln'\n",
      " |      Data variables:\n",
      " |          temperature  (station) float64 10.98 nan 12.06 nan\n",
      " |          pressure     (station) float64 211.8 nan 218.8 nan\n",
      " |      \n",
      " |      We can fill in the missing values by passing a value to the keyword `fill_value`.\n",
      " |      \n",
      " |      >>> x.reindex({\"station\": new_index}, fill_value=0)\n",
      " |      <xarray.Dataset>\n",
      " |      Dimensions:      (station: 4)\n",
      " |      Coordinates:\n",
      " |        * station      (station) <U7 'boston' 'austin' 'seattle' 'lincoln'\n",
      " |      Data variables:\n",
      " |          temperature  (station) float64 10.98 0.0 12.06 0.0\n",
      " |          pressure     (station) float64 211.8 0.0 218.8 0.0\n",
      " |      \n",
      " |      We can also use different fill values for each variable.\n",
      " |      \n",
      " |      >>> x.reindex(\n",
      " |      ...     {\"station\": new_index}, fill_value={\"temperature\": 0, \"pressure\": 100}\n",
      " |      ... )\n",
      " |      <xarray.Dataset>\n",
      " |      Dimensions:      (station: 4)\n",
      " |      Coordinates:\n",
      " |        * station      (station) <U7 'boston' 'austin' 'seattle' 'lincoln'\n",
      " |      Data variables:\n",
      " |          temperature  (station) float64 10.98 0.0 12.06 0.0\n",
      " |          pressure     (station) float64 211.8 100.0 218.8 100.0\n",
      " |      \n",
      " |      Because the index is not monotonically increasing or decreasing, we cannot use arguments\n",
      " |      to the keyword method to fill the `NaN` values.\n",
      " |      \n",
      " |      >>> x.reindex({\"station\": new_index}, method=\"nearest\")\n",
      " |      Traceback (most recent call last):\n",
      " |      ...\n",
      " |          raise ValueError('index must be monotonic increasing or decreasing')\n",
      " |      ValueError: index must be monotonic increasing or decreasing\n",
      " |      \n",
      " |      To further illustrate the filling functionality in reindex, we will create a\n",
      " |      dataset with a monotonically increasing index (for example, a sequence of dates).\n",
      " |      \n",
      " |      >>> x2 = xr.Dataset(\n",
      " |      ...     {\n",
      " |      ...         \"temperature\": (\n",
      " |      ...             \"time\",\n",
      " |      ...             [15.57, 12.77, np.nan, 0.3081, 16.59, 15.12],\n",
      " |      ...         ),\n",
      " |      ...         \"pressure\": (\"time\", 500 * np.random.rand(6)),\n",
      " |      ...     },\n",
      " |      ...     coords={\"time\": pd.date_range(\"01/01/2019\", periods=6, freq=\"D\")},\n",
      " |      ... )\n",
      " |      >>> x2\n",
      " |      <xarray.Dataset>\n",
      " |      Dimensions:      (time: 6)\n",
      " |      Coordinates:\n",
      " |        * time         (time) datetime64[ns] 2019-01-01 2019-01-02 ... 2019-01-06\n",
      " |      Data variables:\n",
      " |          temperature  (time) float64 15.57 12.77 nan 0.3081 16.59 15.12\n",
      " |          pressure     (time) float64 481.8 191.7 395.9 264.4 284.0 462.8\n",
      " |      \n",
      " |      Suppose we decide to expand the dataset to cover a wider date range.\n",
      " |      \n",
      " |      >>> time_index2 = pd.date_range(\"12/29/2018\", periods=10, freq=\"D\")\n",
      " |      >>> x2.reindex({\"time\": time_index2})\n",
      " |      <xarray.Dataset>\n",
      " |      Dimensions:      (time: 10)\n",
      " |      Coordinates:\n",
      " |        * time         (time) datetime64[ns] 2018-12-29 2018-12-30 ... 2019-01-07\n",
      " |      Data variables:\n",
      " |          temperature  (time) float64 nan nan nan 15.57 ... 0.3081 16.59 15.12 nan\n",
      " |          pressure     (time) float64 nan nan nan 481.8 ... 264.4 284.0 462.8 nan\n",
      " |      \n",
      " |      The index entries that did not have a value in the original data frame (for example, `2018-12-29`)\n",
      " |      are by default filled with NaN. If desired, we can fill in the missing values using one of several options.\n",
      " |      \n",
      " |      For example, to back-propagate the last valid value to fill the `NaN` values,\n",
      " |      pass `bfill` as an argument to the `method` keyword.\n",
      " |      \n",
      " |      >>> x3 = x2.reindex({\"time\": time_index2}, method=\"bfill\")\n",
      " |      >>> x3\n",
      " |      <xarray.Dataset>\n",
      " |      Dimensions:      (time: 10)\n",
      " |      Coordinates:\n",
      " |        * time         (time) datetime64[ns] 2018-12-29 2018-12-30 ... 2019-01-07\n",
      " |      Data variables:\n",
      " |          temperature  (time) float64 15.57 15.57 15.57 15.57 ... 16.59 15.12 nan\n",
      " |          pressure     (time) float64 481.8 481.8 481.8 481.8 ... 284.0 462.8 nan\n",
      " |      \n",
      " |      Please note that the `NaN` value present in the original dataset (at index value `2019-01-03`)\n",
      " |      will not be filled by any of the value propagation schemes.\n",
      " |      \n",
      " |      >>> x2.where(x2.temperature.isnull(), drop=True)\n",
      " |      <xarray.Dataset>\n",
      " |      Dimensions:      (time: 1)\n",
      " |      Coordinates:\n",
      " |        * time         (time) datetime64[ns] 2019-01-03\n",
      " |      Data variables:\n",
      " |          temperature  (time) float64 nan\n",
      " |          pressure     (time) float64 395.9\n",
      " |      >>> x3.where(x3.temperature.isnull(), drop=True)\n",
      " |      <xarray.Dataset>\n",
      " |      Dimensions:      (time: 2)\n",
      " |      Coordinates:\n",
      " |        * time         (time) datetime64[ns] 2019-01-03 2019-01-07\n",
      " |      Data variables:\n",
      " |          temperature  (time) float64 nan nan\n",
      " |          pressure     (time) float64 395.9 nan\n",
      " |      \n",
      " |      This is because filling while reindexing does not look at dataset values, but only compares\n",
      " |      the original and desired indexes. If you do want to fill in the `NaN` values present in the\n",
      " |      original dataset, use the :py:meth:`~Dataset.fillna()` method.\n",
      " |  \n",
      " |  reindex_like(self: 'T_Dataset', other: 'Dataset | DataArray', method: 'ReindexMethodOptions' = None, tolerance: 'int | float | Iterable[int | float] | None' = None, copy: 'bool' = True, fill_value: 'Any' = <NA>) -> 'T_Dataset'\n",
      " |      Conform this object onto the indexes of another object, filling in\n",
      " |      missing values with ``fill_value``. The default fill value is NaN.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      other : Dataset or DataArray\n",
      " |          Object with an 'indexes' attribute giving a mapping from dimension\n",
      " |          names to pandas.Index objects, which provides coordinates upon\n",
      " |          which to index the variables in this dataset. The indexes on this\n",
      " |          other object need not be the same as the indexes on this\n",
      " |          dataset. Any mis-matched index values will be filled in with\n",
      " |          NaN, and any mis-matched dimension names will simply be ignored.\n",
      " |      method : {None, \"nearest\", \"pad\", \"ffill\", \"backfill\", \"bfill\", None}, optional\n",
      " |          Method to use for filling index values from other not found in this\n",
      " |          dataset:\n",
      " |      \n",
      " |          - None (default): don't fill gaps\n",
      " |          - \"pad\" / \"ffill\": propagate last valid index value forward\n",
      " |          - \"backfill\" / \"bfill\": propagate next valid index value backward\n",
      " |          - \"nearest\": use nearest valid index value\n",
      " |      \n",
      " |      tolerance : optional\n",
      " |          Maximum distance between original and new labels for inexact\n",
      " |          matches. The values of the index at the matching locations must\n",
      " |          satisfy the equation ``abs(index[indexer] - target) <= tolerance``.\n",
      " |          Tolerance may be a scalar value, which applies the same tolerance\n",
      " |          to all values, or list-like, which applies variable tolerance per\n",
      " |          element. List-like must be the same size as the index and its dtype\n",
      " |          must exactly match the indexs type.\n",
      " |      copy : bool, default: True\n",
      " |          If ``copy=True``, data in the return value is always copied. If\n",
      " |          ``copy=False`` and reindexing is unnecessary, or can be performed\n",
      " |          with only slice operations, then the output may share memory with\n",
      " |          the input. In either case, a new xarray object is always returned.\n",
      " |      fill_value : scalar or dict-like, optional\n",
      " |          Value to use for newly missing values. If a dict-like maps\n",
      " |          variable names to fill values.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      reindexed : Dataset\n",
      " |          Another dataset, with this dataset's data but coordinates from the\n",
      " |          other object.\n",
      " |      \n",
      " |      See Also\n",
      " |      --------\n",
      " |      Dataset.reindex\n",
      " |      align\n",
      " |  \n",
      " |  rename(self: 'T_Dataset', name_dict: 'Mapping[Any, Hashable] | None' = None, **names: 'Hashable') -> 'T_Dataset'\n",
      " |      Returns a new object with renamed variables, coordinates and dimensions.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      name_dict : dict-like, optional\n",
      " |          Dictionary whose keys are current variable, coordinate or dimension names and\n",
      " |          whose values are the desired names.\n",
      " |      **names : optional\n",
      " |          Keyword form of ``name_dict``.\n",
      " |          One of name_dict or names must be provided.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      renamed : Dataset\n",
      " |          Dataset with renamed variables, coordinates and dimensions.\n",
      " |      \n",
      " |      See Also\n",
      " |      --------\n",
      " |      Dataset.swap_dims\n",
      " |      Dataset.rename_vars\n",
      " |      Dataset.rename_dims\n",
      " |      DataArray.rename\n",
      " |  \n",
      " |  rename_dims(self: 'T_Dataset', dims_dict: 'Mapping[Any, Hashable] | None' = None, **dims: 'Hashable') -> 'T_Dataset'\n",
      " |      Returns a new object with renamed dimensions only.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      dims_dict : dict-like, optional\n",
      " |          Dictionary whose keys are current dimension names and\n",
      " |          whose values are the desired names. The desired names must\n",
      " |          not be the name of an existing dimension or Variable in the Dataset.\n",
      " |      **dims : optional\n",
      " |          Keyword form of ``dims_dict``.\n",
      " |          One of dims_dict or dims must be provided.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      renamed : Dataset\n",
      " |          Dataset with renamed dimensions.\n",
      " |      \n",
      " |      See Also\n",
      " |      --------\n",
      " |      Dataset.swap_dims\n",
      " |      Dataset.rename\n",
      " |      Dataset.rename_vars\n",
      " |      DataArray.rename\n",
      " |  \n",
      " |  rename_vars(self: 'T_Dataset', name_dict: 'Mapping[Any, Hashable]' = None, **names: 'Hashable') -> 'T_Dataset'\n",
      " |      Returns a new object with renamed variables including coordinates\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      name_dict : dict-like, optional\n",
      " |          Dictionary whose keys are current variable or coordinate names and\n",
      " |          whose values are the desired names.\n",
      " |      **names : optional\n",
      " |          Keyword form of ``name_dict``.\n",
      " |          One of name_dict or names must be provided.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      renamed : Dataset\n",
      " |          Dataset with renamed variables including coordinates\n",
      " |      \n",
      " |      See Also\n",
      " |      --------\n",
      " |      Dataset.swap_dims\n",
      " |      Dataset.rename\n",
      " |      Dataset.rename_dims\n",
      " |      DataArray.rename\n",
      " |  \n",
      " |  reorder_levels(self: 'T_Dataset', dim_order: 'Mapping[Any, Sequence[int | Hashable]] | None' = None, **dim_order_kwargs: 'Sequence[int | Hashable]') -> 'T_Dataset'\n",
      " |      Rearrange index levels using input order.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      dim_order : dict-like of Hashable to Sequence of int or Hashable, optional\n",
      " |          Mapping from names matching dimensions and values given\n",
      " |          by lists representing new level orders. Every given dimension\n",
      " |          must have a multi-index.\n",
      " |      **dim_order_kwargs : Sequence of int or Hashable, optional\n",
      " |          The keyword arguments form of ``dim_order``.\n",
      " |          One of dim_order or dim_order_kwargs must be provided.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      obj : Dataset\n",
      " |          Another dataset, with this dataset's data but replaced\n",
      " |          coordinates.\n",
      " |  \n",
      " |  resample(self, indexer: 'Mapping[Any, str] | None' = None, skipna: 'bool | None' = None, closed: 'SideOptions | None' = None, label: 'SideOptions | None' = None, base: 'int' = 0, keep_attrs: 'bool | None' = None, loffset: 'datetime.timedelta | str | None' = None, restore_coord_dims: 'bool | None' = None, **indexer_kwargs: 'str') -> 'DatasetResample'\n",
      " |      Returns a Resample object for performing resampling operations.\n",
      " |      \n",
      " |      Handles both downsampling and upsampling. The resampled\n",
      " |      dimension must be a datetime-like coordinate. If any intervals\n",
      " |      contain no values from the original object, they will be given\n",
      " |      the value ``NaN``.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      indexer : Mapping of Hashable to str, optional\n",
      " |          Mapping from the dimension name to resample frequency [1]_. The\n",
      " |          dimension must be datetime-like.\n",
      " |      skipna : bool, optional\n",
      " |          Whether to skip missing values when aggregating in downsampling.\n",
      " |      closed : {\"left\", \"right\"}, optional\n",
      " |          Side of each interval to treat as closed.\n",
      " |      label : {\"left\", \"right\"}, optional\n",
      " |          Side of each interval to use for labeling.\n",
      " |      base : int, default = 0\n",
      " |          For frequencies that evenly subdivide 1 day, the \"origin\" of the\n",
      " |          aggregated intervals. For example, for \"24H\" frequency, base could\n",
      " |          range from 0 through 23.\n",
      " |      loffset : timedelta or str, optional\n",
      " |          Offset used to adjust the resampled time labels. Some pandas date\n",
      " |          offset strings are supported.\n",
      " |      restore_coord_dims : bool, optional\n",
      " |          If True, also restore the dimension order of multi-dimensional\n",
      " |          coordinates.\n",
      " |      **indexer_kwargs : str\n",
      " |          The keyword arguments form of ``indexer``.\n",
      " |          One of indexer or indexer_kwargs must be provided.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      resampled : core.resample.DataArrayResample\n",
      " |          This object resampled.\n",
      " |      \n",
      " |      See Also\n",
      " |      --------\n",
      " |      DataArray.resample\n",
      " |      pandas.Series.resample\n",
      " |      pandas.DataFrame.resample\n",
      " |      \n",
      " |      References\n",
      " |      ----------\n",
      " |      .. [1] http://pandas.pydata.org/pandas-docs/stable/timeseries.html#offset-aliases\n",
      " |  \n",
      " |  reset_coords(self: 'T_Dataset', names: 'Hashable | Iterable[Hashable] | None' = None, drop: 'bool' = False) -> 'T_Dataset'\n",
      " |      Given names of coordinates, reset them to become variables\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      names : hashable or iterable of hashable, optional\n",
      " |          Name(s) of non-index coordinates in this dataset to reset into\n",
      " |          variables. By default, all non-index coordinates are reset.\n",
      " |      drop : bool, default: False\n",
      " |          If True, remove coordinates instead of converting them into\n",
      " |          variables.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      Dataset\n",
      " |  \n",
      " |  reset_index(self: 'T_Dataset', dims_or_levels: 'Hashable | Sequence[Hashable]', drop: 'bool' = False) -> 'T_Dataset'\n",
      " |      Reset the specified index(es) or multi-index level(s).\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      dims_or_levels : Hashable or Sequence of Hashable\n",
      " |          Name(s) of the dimension(s) and/or multi-index level(s) that will\n",
      " |          be reset.\n",
      " |      drop : bool, default: False\n",
      " |          If True, remove the specified indexes and/or multi-index levels\n",
      " |          instead of extracting them as new coordinates (default: False).\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      obj : Dataset\n",
      " |          Another dataset, with this dataset's data but replaced coordinates.\n",
      " |      \n",
      " |      See Also\n",
      " |      --------\n",
      " |      Dataset.set_index\n",
      " |  \n",
      " |  roll(self: 'T_Dataset', shifts: 'Mapping[Any, int] | None' = None, roll_coords: 'bool' = False, **shifts_kwargs: 'int') -> 'T_Dataset'\n",
      " |      Roll this dataset by an offset along one or more dimensions.\n",
      " |      \n",
      " |      Unlike shift, roll treats the given dimensions as periodic, so will not\n",
      " |      create any missing values to be filled.\n",
      " |      \n",
      " |      Also unlike shift, roll may rotate all variables, including coordinates\n",
      " |      if specified. The direction of rotation is consistent with\n",
      " |      :py:func:`numpy.roll`.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      shifts : mapping of hashable to int, optional\n",
      " |          A dict with keys matching dimensions and values given\n",
      " |          by integers to rotate each of the given dimensions. Positive\n",
      " |          offsets roll to the right; negative offsets roll to the left.\n",
      " |      roll_coords : bool, default: False\n",
      " |          Indicates whether to roll the coordinates by the offset too.\n",
      " |      **shifts_kwargs : {dim: offset, ...}, optional\n",
      " |          The keyword arguments form of ``shifts``.\n",
      " |          One of shifts or shifts_kwargs must be provided.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      rolled : Dataset\n",
      " |          Dataset with the same attributes but rolled data and coordinates.\n",
      " |      \n",
      " |      See Also\n",
      " |      --------\n",
      " |      shift\n",
      " |      \n",
      " |      Examples\n",
      " |      --------\n",
      " |      >>> ds = xr.Dataset({\"foo\": (\"x\", list(\"abcde\"))}, coords={\"x\": np.arange(5)})\n",
      " |      >>> ds.roll(x=2)\n",
      " |      <xarray.Dataset>\n",
      " |      Dimensions:  (x: 5)\n",
      " |      Coordinates:\n",
      " |        * x        (x) int64 0 1 2 3 4\n",
      " |      Data variables:\n",
      " |          foo      (x) <U1 'd' 'e' 'a' 'b' 'c'\n",
      " |      \n",
      " |      >>> ds.roll(x=2, roll_coords=True)\n",
      " |      <xarray.Dataset>\n",
      " |      Dimensions:  (x: 5)\n",
      " |      Coordinates:\n",
      " |        * x        (x) int64 3 4 0 1 2\n",
      " |      Data variables:\n",
      " |          foo      (x) <U1 'd' 'e' 'a' 'b' 'c'\n",
      " |  \n",
      " |  rolling(self, dim: 'Mapping[Any, int] | None' = None, min_periods: 'int | None' = None, center: 'bool | Mapping[Any, bool]' = False, **window_kwargs: 'int') -> 'DatasetRolling'\n",
      " |      Rolling window object for Datasets.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      dim : dict, optional\n",
      " |          Mapping from the dimension name to create the rolling iterator\n",
      " |          along (e.g. `time`) to its moving window size.\n",
      " |      min_periods : int or None, default: None\n",
      " |          Minimum number of observations in window required to have a value\n",
      " |          (otherwise result is NA). The default, None, is equivalent to\n",
      " |          setting min_periods equal to the size of the window.\n",
      " |      center : bool or Mapping to int, default: False\n",
      " |          Set the labels at the center of the window.\n",
      " |      **window_kwargs : optional\n",
      " |          The keyword arguments form of ``dim``.\n",
      " |          One of dim or window_kwargs must be provided.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      core.rolling.DatasetRolling\n",
      " |      \n",
      " |      See Also\n",
      " |      --------\n",
      " |      core.rolling.DatasetRolling\n",
      " |      DataArray.rolling\n",
      " |  \n",
      " |  sel(self: 'T_Dataset', indexers: 'Mapping[Any, Any]' = None, method: 'str' = None, tolerance: 'int | float | Iterable[int | float] | None' = None, drop: 'bool' = False, **indexers_kwargs: 'Any') -> 'T_Dataset'\n",
      " |      Returns a new dataset with each array indexed by tick labels\n",
      " |      along the specified dimension(s).\n",
      " |      \n",
      " |      In contrast to `Dataset.isel`, indexers for this method should use\n",
      " |      labels instead of integers.\n",
      " |      \n",
      " |      Under the hood, this method is powered by using pandas's powerful Index\n",
      " |      objects. This makes label based indexing essentially just as fast as\n",
      " |      using integer indexing.\n",
      " |      \n",
      " |      It also means this method uses pandas's (well documented) logic for\n",
      " |      indexing. This means you can use string shortcuts for datetime indexes\n",
      " |      (e.g., '2000-01' to select all values in January 2000). It also means\n",
      " |      that slices are treated as inclusive of both the start and stop values,\n",
      " |      unlike normal Python indexing.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      indexers : dict, optional\n",
      " |          A dict with keys matching dimensions and values given\n",
      " |          by scalars, slices or arrays of tick labels. For dimensions with\n",
      " |          multi-index, the indexer may also be a dict-like object with keys\n",
      " |          matching index level names.\n",
      " |          If DataArrays are passed as indexers, xarray-style indexing will be\n",
      " |          carried out. See :ref:`indexing` for the details.\n",
      " |          One of indexers or indexers_kwargs must be provided.\n",
      " |      method : {None, \"nearest\", \"pad\", \"ffill\", \"backfill\", \"bfill\"}, optional\n",
      " |          Method to use for inexact matches:\n",
      " |      \n",
      " |          * None (default): only exact matches\n",
      " |          * pad / ffill: propagate last valid index value forward\n",
      " |          * backfill / bfill: propagate next valid index value backward\n",
      " |          * nearest: use nearest valid index value\n",
      " |      tolerance : optional\n",
      " |          Maximum distance between original and new labels for inexact\n",
      " |          matches. The values of the index at the matching locations must\n",
      " |          satisfy the equation ``abs(index[indexer] - target) <= tolerance``.\n",
      " |      drop : bool, optional\n",
      " |          If ``drop=True``, drop coordinates variables in `indexers` instead\n",
      " |          of making them scalar.\n",
      " |      **indexers_kwargs : {dim: indexer, ...}, optional\n",
      " |          The keyword arguments form of ``indexers``.\n",
      " |          One of indexers or indexers_kwargs must be provided.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      obj : Dataset\n",
      " |          A new Dataset with the same contents as this dataset, except each\n",
      " |          variable and dimension is indexed by the appropriate indexers.\n",
      " |          If indexer DataArrays have coordinates that do not conflict with\n",
      " |          this object, then these coordinates will be attached.\n",
      " |          In general, each array's data will be a view of the array's data\n",
      " |          in this dataset, unless vectorized indexing was triggered by using\n",
      " |          an array indexer, in which case the data will be a copy.\n",
      " |      \n",
      " |      See Also\n",
      " |      --------\n",
      " |      Dataset.isel\n",
      " |      DataArray.sel\n",
      " |  \n",
      " |  set_coords(self: 'T_Dataset', names: 'Hashable | Iterable[Hashable]') -> 'T_Dataset'\n",
      " |      Given names of one or more variables, set them as coordinates\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      names : hashable or iterable of hashable\n",
      " |          Name(s) of variables in this dataset to convert into coordinates.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      Dataset\n",
      " |      \n",
      " |      See Also\n",
      " |      --------\n",
      " |      Dataset.swap_dims\n",
      " |  \n",
      " |  set_index(self, indexes: 'Mapping[Any, Hashable | Sequence[Hashable]] | None' = None, append: 'bool' = False, **indexes_kwargs: 'Hashable | Sequence[Hashable]') -> 'Dataset'\n",
      " |      Set Dataset (multi-)indexes using one or more existing coordinates\n",
      " |      or variables.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      indexes : {dim: index, ...}\n",
      " |          Mapping from names matching dimensions and values given\n",
      " |          by (lists of) the names of existing coordinates or variables to set\n",
      " |          as new (multi-)index.\n",
      " |      append : bool, default: False\n",
      " |          If True, append the supplied index(es) to the existing index(es).\n",
      " |          Otherwise replace the existing index(es) (default).\n",
      " |      **indexes_kwargs : optional\n",
      " |          The keyword arguments form of ``indexes``.\n",
      " |          One of indexes or indexes_kwargs must be provided.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      obj : Dataset\n",
      " |          Another dataset, with this dataset's data but replaced coordinates.\n",
      " |      \n",
      " |      Examples\n",
      " |      --------\n",
      " |      >>> arr = xr.DataArray(\n",
      " |      ...     data=np.ones((2, 3)),\n",
      " |      ...     dims=[\"x\", \"y\"],\n",
      " |      ...     coords={\"x\": range(2), \"y\": range(3), \"a\": (\"x\", [3, 4])},\n",
      " |      ... )\n",
      " |      >>> ds = xr.Dataset({\"v\": arr})\n",
      " |      >>> ds\n",
      " |      <xarray.Dataset>\n",
      " |      Dimensions:  (x: 2, y: 3)\n",
      " |      Coordinates:\n",
      " |        * x        (x) int64 0 1\n",
      " |        * y        (y) int64 0 1 2\n",
      " |          a        (x) int64 3 4\n",
      " |      Data variables:\n",
      " |          v        (x, y) float64 1.0 1.0 1.0 1.0 1.0 1.0\n",
      " |      >>> ds.set_index(x=\"a\")\n",
      " |      <xarray.Dataset>\n",
      " |      Dimensions:  (x: 2, y: 3)\n",
      " |      Coordinates:\n",
      " |        * x        (x) int64 3 4\n",
      " |        * y        (y) int64 0 1 2\n",
      " |      Data variables:\n",
      " |          v        (x, y) float64 1.0 1.0 1.0 1.0 1.0 1.0\n",
      " |      \n",
      " |      See Also\n",
      " |      --------\n",
      " |      Dataset.reset_index\n",
      " |      Dataset.swap_dims\n",
      " |  \n",
      " |  shift(self: 'T_Dataset', shifts: 'Mapping[Any, int] | None' = None, fill_value: 'Any' = <NA>, **shifts_kwargs: 'int') -> 'T_Dataset'\n",
      " |      Shift this dataset by an offset along one or more dimensions.\n",
      " |      \n",
      " |      Only data variables are moved; coordinates stay in place. This is\n",
      " |      consistent with the behavior of ``shift`` in pandas.\n",
      " |      \n",
      " |      Values shifted from beyond array bounds will appear at one end of\n",
      " |      each dimension, which are filled according to `fill_value`. For periodic\n",
      " |      offsets instead see `roll`.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      shifts : mapping of hashable to int\n",
      " |          Integer offset to shift along each of the given dimensions.\n",
      " |          Positive offsets shift to the right; negative offsets shift to the\n",
      " |          left.\n",
      " |      fill_value : scalar or dict-like, optional\n",
      " |          Value to use for newly missing values. If a dict-like, maps\n",
      " |          variable names (including coordinates) to fill values.\n",
      " |      **shifts_kwargs\n",
      " |          The keyword arguments form of ``shifts``.\n",
      " |          One of shifts or shifts_kwargs must be provided.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      shifted : Dataset\n",
      " |          Dataset with the same coordinates and attributes but shifted data\n",
      " |          variables.\n",
      " |      \n",
      " |      See Also\n",
      " |      --------\n",
      " |      roll\n",
      " |      \n",
      " |      Examples\n",
      " |      --------\n",
      " |      >>> ds = xr.Dataset({\"foo\": (\"x\", list(\"abcde\"))})\n",
      " |      >>> ds.shift(x=2)\n",
      " |      <xarray.Dataset>\n",
      " |      Dimensions:  (x: 5)\n",
      " |      Dimensions without coordinates: x\n",
      " |      Data variables:\n",
      " |          foo      (x) object nan nan 'a' 'b' 'c'\n",
      " |  \n",
      " |  sortby(self: 'T_Dataset', variables: 'Hashable | DataArray | list[Hashable | DataArray]', ascending: 'bool' = True) -> 'T_Dataset'\n",
      " |      Sort object by labels or values (along an axis).\n",
      " |      \n",
      " |      Sorts the dataset, either along specified dimensions,\n",
      " |      or according to values of 1-D dataarrays that share dimension\n",
      " |      with calling object.\n",
      " |      \n",
      " |      If the input variables are dataarrays, then the dataarrays are aligned\n",
      " |      (via left-join) to the calling object prior to sorting by cell values.\n",
      " |      NaNs are sorted to the end, following Numpy convention.\n",
      " |      \n",
      " |      If multiple sorts along the same dimension is\n",
      " |      given, numpy's lexsort is performed along that dimension:\n",
      " |      https://numpy.org/doc/stable/reference/generated/numpy.lexsort.html\n",
      " |      and the FIRST key in the sequence is used as the primary sort key,\n",
      " |      followed by the 2nd key, etc.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      variables : Hashable, DataArray, or list of hashable or DataArray\n",
      " |          1D DataArray objects or name(s) of 1D variable(s) in\n",
      " |          coords/data_vars whose values are used to sort the dataset.\n",
      " |      ascending : bool, default: True\n",
      " |          Whether to sort by ascending or descending order.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      sorted : Dataset\n",
      " |          A new dataset where all the specified dims are sorted by dim\n",
      " |          labels.\n",
      " |      \n",
      " |      See Also\n",
      " |      --------\n",
      " |      DataArray.sortby\n",
      " |      numpy.sort\n",
      " |      pandas.sort_values\n",
      " |      pandas.sort_index\n",
      " |      \n",
      " |      Examples\n",
      " |      --------\n",
      " |      >>> ds = xr.Dataset(\n",
      " |      ...     {\n",
      " |      ...         \"A\": ((\"x\", \"y\"), [[1, 2], [3, 4]]),\n",
      " |      ...         \"B\": ((\"x\", \"y\"), [[5, 6], [7, 8]]),\n",
      " |      ...     },\n",
      " |      ...     coords={\"x\": [\"b\", \"a\"], \"y\": [1, 0]},\n",
      " |      ... )\n",
      " |      >>> ds.sortby(\"x\")\n",
      " |      <xarray.Dataset>\n",
      " |      Dimensions:  (x: 2, y: 2)\n",
      " |      Coordinates:\n",
      " |        * x        (x) <U1 'a' 'b'\n",
      " |        * y        (y) int64 1 0\n",
      " |      Data variables:\n",
      " |          A        (x, y) int64 3 4 1 2\n",
      " |          B        (x, y) int64 7 8 5 6\n",
      " |  \n",
      " |  stack(self: 'T_Dataset', dimensions: 'Mapping[Any, Sequence[Hashable]] | None' = None, create_index: 'bool | None' = True, index_cls: 'type[Index]' = <class 'xarray.core.indexes.PandasMultiIndex'>, **dimensions_kwargs: 'Sequence[Hashable]') -> 'T_Dataset'\n",
      " |      Stack any number of existing dimensions into a single new dimension.\n",
      " |      \n",
      " |      New dimensions will be added at the end, and by default the corresponding\n",
      " |      coordinate variables will be combined into a MultiIndex.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      dimensions : mapping of hashable to sequence of hashable\n",
      " |          Mapping of the form `new_name=(dim1, dim2, ...)`. Names of new\n",
      " |          dimensions, and the existing dimensions that they replace. An\n",
      " |          ellipsis (`...`) will be replaced by all unlisted dimensions.\n",
      " |          Passing a list containing an ellipsis (`stacked_dim=[...]`) will stack over\n",
      " |          all dimensions.\n",
      " |      create_index : bool or None, default: True\n",
      " |      \n",
      " |          - True: create a multi-index for each of the stacked dimensions.\n",
      " |          - False: don't create any index.\n",
      " |          - None. create a multi-index only if exactly one single (1-d) coordinate\n",
      " |            index is found for every dimension to stack.\n",
      " |      \n",
      " |      index_cls: Index-class, default: PandasMultiIndex\n",
      " |          Can be used to pass a custom multi-index type (must be an Xarray index that\n",
      " |          implements `.stack()`). By default, a pandas multi-index wrapper is used.\n",
      " |      **dimensions_kwargs\n",
      " |          The keyword arguments form of ``dimensions``.\n",
      " |          One of dimensions or dimensions_kwargs must be provided.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      stacked : Dataset\n",
      " |          Dataset with stacked data.\n",
      " |      \n",
      " |      See Also\n",
      " |      --------\n",
      " |      Dataset.unstack\n",
      " |  \n",
      " |  swap_dims(self: 'T_Dataset', dims_dict: 'Mapping[Any, Hashable]' = None, **dims_kwargs) -> 'T_Dataset'\n",
      " |      Returns a new object with swapped dimensions.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      dims_dict : dict-like\n",
      " |          Dictionary whose keys are current dimension names and whose values\n",
      " |          are new names.\n",
      " |      **dims_kwargs : {existing_dim: new_dim, ...}, optional\n",
      " |          The keyword arguments form of ``dims_dict``.\n",
      " |          One of dims_dict or dims_kwargs must be provided.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      swapped : Dataset\n",
      " |          Dataset with swapped dimensions.\n",
      " |      \n",
      " |      Examples\n",
      " |      --------\n",
      " |      >>> ds = xr.Dataset(\n",
      " |      ...     data_vars={\"a\": (\"x\", [5, 7]), \"b\": (\"x\", [0.1, 2.4])},\n",
      " |      ...     coords={\"x\": [\"a\", \"b\"], \"y\": (\"x\", [0, 1])},\n",
      " |      ... )\n",
      " |      >>> ds\n",
      " |      <xarray.Dataset>\n",
      " |      Dimensions:  (x: 2)\n",
      " |      Coordinates:\n",
      " |        * x        (x) <U1 'a' 'b'\n",
      " |          y        (x) int64 0 1\n",
      " |      Data variables:\n",
      " |          a        (x) int64 5 7\n",
      " |          b        (x) float64 0.1 2.4\n",
      " |      \n",
      " |      >>> ds.swap_dims({\"x\": \"y\"})\n",
      " |      <xarray.Dataset>\n",
      " |      Dimensions:  (y: 2)\n",
      " |      Coordinates:\n",
      " |          x        (y) <U1 'a' 'b'\n",
      " |        * y        (y) int64 0 1\n",
      " |      Data variables:\n",
      " |          a        (y) int64 5 7\n",
      " |          b        (y) float64 0.1 2.4\n",
      " |      \n",
      " |      >>> ds.swap_dims({\"x\": \"z\"})\n",
      " |      <xarray.Dataset>\n",
      " |      Dimensions:  (z: 2)\n",
      " |      Coordinates:\n",
      " |          x        (z) <U1 'a' 'b'\n",
      " |          y        (z) int64 0 1\n",
      " |      Dimensions without coordinates: z\n",
      " |      Data variables:\n",
      " |          a        (z) int64 5 7\n",
      " |          b        (z) float64 0.1 2.4\n",
      " |      \n",
      " |      See Also\n",
      " |      --------\n",
      " |      Dataset.rename\n",
      " |      DataArray.swap_dims\n",
      " |  \n",
      " |  tail(self: 'T_Dataset', indexers: 'Mapping[Any, int] | int | None' = None, **indexers_kwargs: 'Any') -> 'T_Dataset'\n",
      " |      Returns a new dataset with the last `n` values of each array\n",
      " |      for the specified dimension(s).\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      indexers : dict or int, default: 5\n",
      " |          A dict with keys matching dimensions and integer values `n`\n",
      " |          or a single integer `n` applied over all dimensions.\n",
      " |          One of indexers or indexers_kwargs must be provided.\n",
      " |      **indexers_kwargs : {dim: n, ...}, optional\n",
      " |          The keyword arguments form of ``indexers``.\n",
      " |          One of indexers or indexers_kwargs must be provided.\n",
      " |      \n",
      " |      See Also\n",
      " |      --------\n",
      " |      Dataset.head\n",
      " |      Dataset.thin\n",
      " |      DataArray.tail\n",
      " |  \n",
      " |  thin(self: 'T_Dataset', indexers: 'Mapping[Any, int] | int | None' = None, **indexers_kwargs: 'Any') -> 'T_Dataset'\n",
      " |      Returns a new dataset with each array indexed along every `n`-th\n",
      " |      value for the specified dimension(s)\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      indexers : dict or int\n",
      " |          A dict with keys matching dimensions and integer values `n`\n",
      " |          or a single integer `n` applied over all dimensions.\n",
      " |          One of indexers or indexers_kwargs must be provided.\n",
      " |      **indexers_kwargs : {dim: n, ...}, optional\n",
      " |          The keyword arguments form of ``indexers``.\n",
      " |          One of indexers or indexers_kwargs must be provided.\n",
      " |      \n",
      " |      Examples\n",
      " |      --------\n",
      " |      >>> x_arr = np.arange(0, 26)\n",
      " |      >>> x_arr\n",
      " |      array([ 0,  1,  2,  3,  4,  5,  6,  7,  8,  9, 10, 11, 12, 13, 14, 15, 16,\n",
      " |             17, 18, 19, 20, 21, 22, 23, 24, 25])\n",
      " |      >>> x = xr.DataArray(\n",
      " |      ...     np.reshape(x_arr, (2, 13)),\n",
      " |      ...     dims=(\"x\", \"y\"),\n",
      " |      ...     coords={\"x\": [0, 1], \"y\": np.arange(0, 13)},\n",
      " |      ... )\n",
      " |      >>> x_ds = xr.Dataset({\"foo\": x})\n",
      " |      >>> x_ds\n",
      " |      <xarray.Dataset>\n",
      " |      Dimensions:  (x: 2, y: 13)\n",
      " |      Coordinates:\n",
      " |        * x        (x) int64 0 1\n",
      " |        * y        (y) int64 0 1 2 3 4 5 6 7 8 9 10 11 12\n",
      " |      Data variables:\n",
      " |          foo      (x, y) int64 0 1 2 3 4 5 6 7 8 9 ... 16 17 18 19 20 21 22 23 24 25\n",
      " |      \n",
      " |      >>> x_ds.thin(3)\n",
      " |      <xarray.Dataset>\n",
      " |      Dimensions:  (x: 1, y: 5)\n",
      " |      Coordinates:\n",
      " |        * x        (x) int64 0\n",
      " |        * y        (y) int64 0 3 6 9 12\n",
      " |      Data variables:\n",
      " |          foo      (x, y) int64 0 3 6 9 12\n",
      " |      >>> x.thin({\"x\": 2, \"y\": 5})\n",
      " |      <xarray.DataArray (x: 1, y: 3)>\n",
      " |      array([[ 0,  5, 10]])\n",
      " |      Coordinates:\n",
      " |        * x        (x) int64 0\n",
      " |        * y        (y) int64 0 5 10\n",
      " |      \n",
      " |      See Also\n",
      " |      --------\n",
      " |      Dataset.head\n",
      " |      Dataset.tail\n",
      " |      DataArray.thin\n",
      " |  \n",
      " |  to_array(self, dim: 'Hashable' = 'variable', name: 'Hashable | None' = None) -> 'DataArray'\n",
      " |      Convert this dataset into an xarray.DataArray\n",
      " |      \n",
      " |      The data variables of this dataset will be broadcast against each other\n",
      " |      and stacked along the first axis of the new array. All coordinates of\n",
      " |      this dataset will remain coordinates.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      dim : Hashable, default: \"variable\"\n",
      " |          Name of the new dimension.\n",
      " |      name : Hashable or None, optional\n",
      " |          Name of the new data array.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      array : xarray.DataArray\n",
      " |  \n",
      " |  to_dask_dataframe(self, dim_order: 'Sequence[Hashable] | None' = None, set_index: 'bool' = False) -> 'DaskDataFrame'\n",
      " |      Convert this dataset into a dask.dataframe.DataFrame.\n",
      " |      \n",
      " |      The dimensions, coordinates and data variables in this dataset form\n",
      " |      the columns of the DataFrame.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      dim_order : list, optional\n",
      " |          Hierarchical dimension order for the resulting dataframe. All\n",
      " |          arrays are transposed to this order and then written out as flat\n",
      " |          vectors in contiguous order, so the last dimension in this list\n",
      " |          will be contiguous in the resulting DataFrame. This has a major\n",
      " |          influence on which operations are efficient on the resulting dask\n",
      " |          dataframe.\n",
      " |      \n",
      " |          If provided, must include all dimensions of this dataset. By\n",
      " |          default, dimensions are sorted alphabetically.\n",
      " |      set_index : bool, default: False\n",
      " |          If set_index=True, the dask DataFrame is indexed by this dataset's\n",
      " |          coordinate. Since dask DataFrames do not support multi-indexes,\n",
      " |          set_index only works if the dataset only contains one dimension.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      dask.dataframe.DataFrame\n",
      " |  \n",
      " |  to_dataframe(self, dim_order: 'Sequence[Hashable] | None' = None) -> 'pd.DataFrame'\n",
      " |      Convert this dataset into a pandas.DataFrame.\n",
      " |      \n",
      " |      Non-index variables in this dataset form the columns of the\n",
      " |      DataFrame. The DataFrame is indexed by the Cartesian product of\n",
      " |      this dataset's indices.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      dim_order: Sequence of Hashable or None, optional\n",
      " |          Hierarchical dimension order for the resulting dataframe. All\n",
      " |          arrays are transposed to this order and then written out as flat\n",
      " |          vectors in contiguous order, so the last dimension in this list\n",
      " |          will be contiguous in the resulting DataFrame. This has a major\n",
      " |          influence on which operations are efficient on the resulting\n",
      " |          dataframe.\n",
      " |      \n",
      " |          If provided, must include all dimensions of this dataset. By\n",
      " |          default, dimensions are sorted alphabetically.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      result : DataFrame\n",
      " |          Dataset as a pandas DataFrame.\n",
      " |  \n",
      " |  to_dict(self, data: 'bool' = True, encoding: 'bool' = False) -> 'dict[str, Any]'\n",
      " |      Convert this dataset to a dictionary following xarray naming\n",
      " |      conventions.\n",
      " |      \n",
      " |      Converts all variables and attributes to native Python objects\n",
      " |      Useful for converting to json. To avoid datetime incompatibility\n",
      " |      use decode_times=False kwarg in xarrray.open_dataset.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      data : bool, default: True\n",
      " |          Whether to include the actual data in the dictionary. When set to\n",
      " |          False, returns just the schema.\n",
      " |      encoding : bool, default: False\n",
      " |          Whether to include the Dataset's encoding in the dictionary.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      d : dict\n",
      " |          Dict with keys: \"coords\", \"attrs\", \"dims\", \"data_vars\" and optionally\n",
      " |          \"encoding\".\n",
      " |      \n",
      " |      See Also\n",
      " |      --------\n",
      " |      Dataset.from_dict\n",
      " |      DataArray.to_dict\n",
      " |  \n",
      " |  to_netcdf(self, path: 'str | PathLike | None' = None, mode: \"Literal['w', 'a']\" = 'w', format: 'T_NetcdfTypes | None' = None, group: 'str | None' = None, engine: 'T_NetcdfEngine | None' = None, encoding: 'Mapping[Hashable, Mapping[str, Any]] | None' = None, unlimited_dims: 'Iterable[Hashable] | None' = None, compute: 'bool' = True, invalid_netcdf: 'bool' = False) -> 'bytes | Delayed | None'\n",
      " |      Write dataset contents to a netCDF file.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      path : str, path-like or file-like, optional\n",
      " |          Path to which to save this dataset. File-like objects are only\n",
      " |          supported by the scipy engine. If no path is provided, this\n",
      " |          function returns the resulting netCDF file as bytes; in this case,\n",
      " |          we need to use scipy, which does not support netCDF version 4 (the\n",
      " |          default format becomes NETCDF3_64BIT).\n",
      " |      mode : {\"w\", \"a\"}, default: \"w\"\n",
      " |          Write ('w') or append ('a') mode. If mode='w', any existing file at\n",
      " |          this location will be overwritten. If mode='a', existing variables\n",
      " |          will be overwritten.\n",
      " |      format : {\"NETCDF4\", \"NETCDF4_CLASSIC\", \"NETCDF3_64BIT\",                   \"NETCDF3_CLASSIC\"}, optional\n",
      " |          File format for the resulting netCDF file:\n",
      " |      \n",
      " |          * NETCDF4: Data is stored in an HDF5 file, using netCDF4 API\n",
      " |            features.\n",
      " |          * NETCDF4_CLASSIC: Data is stored in an HDF5 file, using only\n",
      " |            netCDF 3 compatible API features.\n",
      " |          * NETCDF3_64BIT: 64-bit offset version of the netCDF 3 file format,\n",
      " |            which fully supports 2+ GB files, but is only compatible with\n",
      " |            clients linked against netCDF version 3.6.0 or later.\n",
      " |          * NETCDF3_CLASSIC: The classic netCDF 3 file format. It does not\n",
      " |            handle 2+ GB files very well.\n",
      " |      \n",
      " |          All formats are supported by the netCDF4-python library.\n",
      " |          scipy.io.netcdf only supports the last two formats.\n",
      " |      \n",
      " |          The default format is NETCDF4 if you are saving a file to disk and\n",
      " |          have the netCDF4-python library available. Otherwise, xarray falls\n",
      " |          back to using scipy to write netCDF files and defaults to the\n",
      " |          NETCDF3_64BIT format (scipy does not support netCDF4).\n",
      " |      group : str, optional\n",
      " |          Path to the netCDF4 group in the given file to open (only works for\n",
      " |          format='NETCDF4'). The group(s) will be created if necessary.\n",
      " |      engine : {\"netcdf4\", \"scipy\", \"h5netcdf\"}, optional\n",
      " |          Engine to use when writing netCDF files. If not provided, the\n",
      " |          default engine is chosen based on available dependencies, with a\n",
      " |          preference for 'netcdf4' if writing to a file on disk.\n",
      " |      encoding : dict, optional\n",
      " |          Nested dictionary with variable names as keys and dictionaries of\n",
      " |          variable specific encodings as values, e.g.,\n",
      " |          ``{\"my_variable\": {\"dtype\": \"int16\", \"scale_factor\": 0.1,\n",
      " |          \"zlib\": True}, ...}``\n",
      " |      \n",
      " |          The `h5netcdf` engine supports both the NetCDF4-style compression\n",
      " |          encoding parameters ``{\"zlib\": True, \"complevel\": 9}`` and the h5py\n",
      " |          ones ``{\"compression\": \"gzip\", \"compression_opts\": 9}``.\n",
      " |          This allows using any compression plugin installed in the HDF5\n",
      " |          library, e.g. LZF.\n",
      " |      \n",
      " |      unlimited_dims : iterable of hashable, optional\n",
      " |          Dimension(s) that should be serialized as unlimited dimensions.\n",
      " |          By default, no dimensions are treated as unlimited dimensions.\n",
      " |          Note that unlimited_dims may also be set via\n",
      " |          ``dataset.encoding[\"unlimited_dims\"]``.\n",
      " |      compute: bool, default: True\n",
      " |          If true compute immediately, otherwise return a\n",
      " |          ``dask.delayed.Delayed`` object that can be computed later.\n",
      " |      invalid_netcdf: bool, default: False\n",
      " |          Only valid along with ``engine=\"h5netcdf\"``. If True, allow writing\n",
      " |          hdf5 files which are invalid netcdf as described in\n",
      " |          https://github.com/h5netcdf/h5netcdf.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |          * ``bytes`` if path is None\n",
      " |          * ``dask.delayed.Delayed`` if compute is False\n",
      " |          * None otherwise\n",
      " |      \n",
      " |      See Also\n",
      " |      --------\n",
      " |      DataArray.to_netcdf\n",
      " |  \n",
      " |  to_pandas(self) -> 'pd.Series | pd.DataFrame'\n",
      " |      Convert this dataset into a pandas object without changing the number of dimensions.\n",
      " |      \n",
      " |      The type of the returned object depends on the number of Dataset\n",
      " |      dimensions:\n",
      " |      \n",
      " |      * 0D -> `pandas.Series`\n",
      " |      * 1D -> `pandas.DataFrame`\n",
      " |      \n",
      " |      Only works for Datasets with 1 or fewer dimensions.\n",
      " |  \n",
      " |  to_stacked_array(self, new_dim: 'Hashable', sample_dims: 'Collection[Hashable]', variable_dim: 'Hashable' = 'variable', name: 'Hashable | None' = None) -> 'DataArray'\n",
      " |      Combine variables of differing dimensionality into a DataArray\n",
      " |      without broadcasting.\n",
      " |      \n",
      " |      This method is similar to Dataset.to_array but does not broadcast the\n",
      " |      variables.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      new_dim : hashable\n",
      " |          Name of the new stacked coordinate\n",
      " |      sample_dims : Collection of hashables\n",
      " |          List of dimensions that **will not** be stacked. Each array in the\n",
      " |          dataset must share these dimensions. For machine learning\n",
      " |          applications, these define the dimensions over which samples are\n",
      " |          drawn.\n",
      " |      variable_dim : hashable, default: \"variable\"\n",
      " |          Name of the level in the stacked coordinate which corresponds to\n",
      " |          the variables.\n",
      " |      name : hashable, optional\n",
      " |          Name of the new data array.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      stacked : DataArray\n",
      " |          DataArray with the specified dimensions and data variables\n",
      " |          stacked together. The stacked coordinate is named ``new_dim``\n",
      " |          and represented by a MultiIndex object with a level containing the\n",
      " |          data variable names. The name of this level is controlled using\n",
      " |          the ``variable_dim`` argument.\n",
      " |      \n",
      " |      See Also\n",
      " |      --------\n",
      " |      Dataset.to_array\n",
      " |      Dataset.stack\n",
      " |      DataArray.to_unstacked_dataset\n",
      " |      \n",
      " |      Examples\n",
      " |      --------\n",
      " |      >>> data = xr.Dataset(\n",
      " |      ...     data_vars={\n",
      " |      ...         \"a\": ((\"x\", \"y\"), [[0, 1, 2], [3, 4, 5]]),\n",
      " |      ...         \"b\": (\"x\", [6, 7]),\n",
      " |      ...     },\n",
      " |      ...     coords={\"y\": [\"u\", \"v\", \"w\"]},\n",
      " |      ... )\n",
      " |      \n",
      " |      >>> data\n",
      " |      <xarray.Dataset>\n",
      " |      Dimensions:  (x: 2, y: 3)\n",
      " |      Coordinates:\n",
      " |        * y        (y) <U1 'u' 'v' 'w'\n",
      " |      Dimensions without coordinates: x\n",
      " |      Data variables:\n",
      " |          a        (x, y) int64 0 1 2 3 4 5\n",
      " |          b        (x) int64 6 7\n",
      " |      \n",
      " |      >>> data.to_stacked_array(\"z\", sample_dims=[\"x\"])\n",
      " |      <xarray.DataArray 'a' (x: 2, z: 4)>\n",
      " |      array([[0, 1, 2, 6],\n",
      " |             [3, 4, 5, 7]])\n",
      " |      Coordinates:\n",
      " |        * z         (z) object MultiIndex\n",
      " |        * variable  (z) object 'a' 'a' 'a' 'b'\n",
      " |        * y         (z) object 'u' 'v' 'w' nan\n",
      " |      Dimensions without coordinates: x\n",
      " |  \n",
      " |  to_zarr(self, store: 'MutableMapping | str | PathLike[str] | None' = None, chunk_store: 'MutableMapping | str | PathLike | None' = None, mode: \"Literal['w', 'w-', 'a', 'r+', None]\" = None, synchronizer=None, group: 'str | None' = None, encoding: 'Mapping | None' = None, compute: 'bool' = True, consolidated: 'bool | None' = None, append_dim: 'Hashable | None' = None, region: 'Mapping[str, slice] | None' = None, safe_chunks: 'bool' = True, storage_options: 'dict[str, str] | None' = None) -> 'ZarrStore | Delayed'\n",
      " |      Write dataset contents to a zarr group.\n",
      " |      \n",
      " |      Zarr chunks are determined in the following way:\n",
      " |      \n",
      " |      - From the ``chunks`` attribute in each variable's ``encoding``\n",
      " |        (can be set via `Dataset.chunk`).\n",
      " |      - If the variable is a Dask array, from the dask chunks\n",
      " |      - If neither Dask chunks nor encoding chunks are present, chunks will\n",
      " |        be determined automatically by Zarr\n",
      " |      - If both Dask chunks and encoding chunks are present, encoding chunks\n",
      " |        will be used, provided that there is a many-to-one relationship between\n",
      " |        encoding chunks and dask chunks (i.e. Dask chunks are bigger than and\n",
      " |        evenly divide encoding chunks); otherwise raise a ``ValueError``.\n",
      " |        This restriction ensures that no synchronization / locks are required\n",
      " |        when writing. To disable this restriction, use ``safe_chunks=False``.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      store : MutableMapping, str or path-like, optional\n",
      " |          Store or path to directory in local or remote file system.\n",
      " |      chunk_store : MutableMapping, str or path-like, optional\n",
      " |          Store or path to directory in local or remote file system only for Zarr\n",
      " |          array chunks. Requires zarr-python v2.4.0 or later.\n",
      " |      mode : {\"w\", \"w-\", \"a\", \"r+\", None}, optional\n",
      " |          Persistence mode: \"w\" means create (overwrite if exists);\n",
      " |          \"w-\" means create (fail if exists);\n",
      " |          \"a\" means override existing variables (create if does not exist);\n",
      " |          \"r+\" means modify existing array *values* only (raise an error if\n",
      " |          any metadata or shapes would change).\n",
      " |          The default mode is \"a\" if ``append_dim`` is set. Otherwise, it is\n",
      " |          \"r+\" if ``region`` is set and ``w-`` otherwise.\n",
      " |      synchronizer : object, optional\n",
      " |          Zarr array synchronizer.\n",
      " |      group : str, optional\n",
      " |          Group path. (a.k.a. `path` in zarr terminology.)\n",
      " |      encoding : dict, optional\n",
      " |          Nested dictionary with variable names as keys and dictionaries of\n",
      " |          variable specific encodings as values, e.g.,\n",
      " |          ``{\"my_variable\": {\"dtype\": \"int16\", \"scale_factor\": 0.1,}, ...}``\n",
      " |      compute : bool, optional\n",
      " |          If True write array data immediately, otherwise return a\n",
      " |          ``dask.delayed.Delayed`` object that can be computed to write\n",
      " |          array data later. Metadata is always updated eagerly.\n",
      " |      consolidated : bool, optional\n",
      " |          If True, apply zarr's `consolidate_metadata` function to the store\n",
      " |          after writing metadata and read existing stores with consolidated\n",
      " |          metadata; if False, do not. The default (`consolidated=None`) means\n",
      " |          write consolidated metadata and attempt to read consolidated\n",
      " |          metadata for existing stores (falling back to non-consolidated).\n",
      " |      append_dim : hashable, optional\n",
      " |          If set, the dimension along which the data will be appended. All\n",
      " |          other dimensions on overridden variables must remain the same size.\n",
      " |      region : dict, optional\n",
      " |          Optional mapping from dimension names to integer slices along\n",
      " |          dataset dimensions to indicate the region of existing zarr array(s)\n",
      " |          in which to write this dataset's data. For example,\n",
      " |          ``{'x': slice(0, 1000), 'y': slice(10000, 11000)}`` would indicate\n",
      " |          that values should be written to the region ``0:1000`` along ``x``\n",
      " |          and ``10000:11000`` along ``y``.\n",
      " |      \n",
      " |          Two restrictions apply to the use of ``region``:\n",
      " |      \n",
      " |          - If ``region`` is set, _all_ variables in a dataset must have at\n",
      " |            least one dimension in common with the region. Other variables\n",
      " |            should be written in a separate call to ``to_zarr()``.\n",
      " |          - Dimensions cannot be included in both ``region`` and\n",
      " |            ``append_dim`` at the same time. To create empty arrays to fill\n",
      " |            in with ``region``, use a separate call to ``to_zarr()`` with\n",
      " |            ``compute=False``. See \"Appending to existing Zarr stores\" in\n",
      " |            the reference documentation for full details.\n",
      " |      safe_chunks : bool, optional\n",
      " |          If True, only allow writes to when there is a many-to-one relationship\n",
      " |          between Zarr chunks (specified in encoding) and Dask chunks.\n",
      " |          Set False to override this restriction; however, data may become corrupted\n",
      " |          if Zarr arrays are written in parallel. This option may be useful in combination\n",
      " |          with ``compute=False`` to initialize a Zarr from an existing\n",
      " |          Dataset with arbitrary chunk structure.\n",
      " |      storage_options : dict, optional\n",
      " |          Any additional parameters for the storage backend (ignored for local\n",
      " |          paths).\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |          * ``dask.delayed.Delayed`` if compute is False\n",
      " |          * ZarrStore otherwise\n",
      " |      \n",
      " |      References\n",
      " |      ----------\n",
      " |      https://zarr.readthedocs.io/\n",
      " |      \n",
      " |      Notes\n",
      " |      -----\n",
      " |      Zarr chunking behavior:\n",
      " |          If chunks are found in the encoding argument or attribute\n",
      " |          corresponding to any DataArray, those chunks are used.\n",
      " |          If a DataArray is a dask array, it is written with those chunks.\n",
      " |          If not other chunks are found, Zarr uses its own heuristics to\n",
      " |          choose automatic chunk sizes.\n",
      " |      \n",
      " |      encoding:\n",
      " |          The encoding attribute (if exists) of the DataArray(s) will be\n",
      " |          used. Override any existing encodings by providing the ``encoding`` kwarg.\n",
      " |      \n",
      " |      See Also\n",
      " |      --------\n",
      " |      :ref:`io.zarr`\n",
      " |          The I/O user guide, with more details and examples.\n",
      " |  \n",
      " |  transpose(self: 'T_Dataset', *dims: 'Hashable', missing_dims: 'ErrorOptionsWithWarn' = 'raise') -> 'T_Dataset'\n",
      " |      Return a new Dataset object with all array dimensions transposed.\n",
      " |      \n",
      " |      Although the order of dimensions on each array will change, the dataset\n",
      " |      dimensions themselves will remain in fixed (sorted) order.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      *dims : hashable, optional\n",
      " |          By default, reverse the dimensions on each array. Otherwise,\n",
      " |          reorder the dimensions to this order.\n",
      " |      missing_dims : {\"raise\", \"warn\", \"ignore\"}, default: \"raise\"\n",
      " |          What to do if dimensions that should be selected from are not present in the\n",
      " |          Dataset:\n",
      " |          - \"raise\": raise an exception\n",
      " |          - \"warn\": raise a warning, and ignore the missing dimensions\n",
      " |          - \"ignore\": ignore the missing dimensions\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      transposed : Dataset\n",
      " |          Each array in the dataset (including) coordinates will be\n",
      " |          transposed to the given order.\n",
      " |      \n",
      " |      Notes\n",
      " |      -----\n",
      " |      This operation returns a view of each array's data. It is\n",
      " |      lazy for dask-backed DataArrays but not for numpy-backed DataArrays\n",
      " |      -- the data will be fully loaded into memory.\n",
      " |      \n",
      " |      See Also\n",
      " |      --------\n",
      " |      numpy.transpose\n",
      " |      DataArray.transpose\n",
      " |  \n",
      " |  unify_chunks(self: 'T_Dataset') -> 'T_Dataset'\n",
      " |      Unify chunk size along all chunked dimensions of this Dataset.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      Dataset with consistent chunk sizes for all dask-array variables\n",
      " |      \n",
      " |      See Also\n",
      " |      --------\n",
      " |      dask.array.core.unify_chunks\n",
      " |  \n",
      " |  unstack(self: 'T_Dataset', dim: 'Hashable | Iterable[Hashable] | None' = None, fill_value: 'Any' = <NA>, sparse: 'bool' = False) -> 'T_Dataset'\n",
      " |      Unstack existing dimensions corresponding to MultiIndexes into\n",
      " |      multiple new dimensions.\n",
      " |      \n",
      " |      New dimensions will be added at the end.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      dim : hashable or iterable of hashable, optional\n",
      " |          Dimension(s) over which to unstack. By default unstacks all\n",
      " |          MultiIndexes.\n",
      " |      fill_value : scalar or dict-like, default: nan\n",
      " |          value to be filled. If a dict-like, maps variable names to\n",
      " |          fill values. If not provided or if the dict-like does not\n",
      " |          contain all variables, the dtype's NA value will be used.\n",
      " |      sparse : bool, default: False\n",
      " |          use sparse-array if True\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      unstacked : Dataset\n",
      " |          Dataset with unstacked data.\n",
      " |      \n",
      " |      See Also\n",
      " |      --------\n",
      " |      Dataset.stack\n",
      " |  \n",
      " |  update(self: 'T_Dataset', other: 'CoercibleMapping') -> 'T_Dataset'\n",
      " |      Update this dataset's variables with those from another dataset.\n",
      " |      \n",
      " |      Just like :py:meth:`dict.update` this is a in-place operation.\n",
      " |      For a non-inplace version, see :py:meth:`Dataset.merge`.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      other : Dataset or mapping\n",
      " |          Variables with which to update this dataset. One of:\n",
      " |      \n",
      " |          - Dataset\n",
      " |          - mapping {var name: DataArray}\n",
      " |          - mapping {var name: Variable}\n",
      " |          - mapping {var name: (dimension name, array-like)}\n",
      " |          - mapping {var name: (tuple of dimension names, array-like)}\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      updated : Dataset\n",
      " |          Updated dataset. Note that since the update is in-place this is the input\n",
      " |          dataset.\n",
      " |      \n",
      " |          It is deprecated since version 0.17 and scheduled to be removed in 0.21.\n",
      " |      \n",
      " |      Raises\n",
      " |      ------\n",
      " |      ValueError\n",
      " |          If any dimensions would have inconsistent sizes in the updated\n",
      " |          dataset.\n",
      " |      \n",
      " |      See Also\n",
      " |      --------\n",
      " |      Dataset.assign\n",
      " |      Dataset.merge\n",
      " |  \n",
      " |  weighted(self, weights: 'DataArray') -> 'DatasetWeighted'\n",
      " |      Weighted Dataset operations.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      weights : DataArray\n",
      " |          An array of weights associated with the values in this Dataset.\n",
      " |          Each value in the data contributes to the reduction operation\n",
      " |          according to its associated weight.\n",
      " |      \n",
      " |      Notes\n",
      " |      -----\n",
      " |      ``weights`` must be a DataArray and cannot contain missing values.\n",
      " |      Missing values can be replaced by ``weights.fillna(0)``.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      core.weighted.DatasetWeighted\n",
      " |      \n",
      " |      See Also\n",
      " |      --------\n",
      " |      DataArray.weighted\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Class methods defined here:\n",
      " |  \n",
      " |  from_dataframe(dataframe: 'pd.DataFrame', sparse: 'bool' = False) -> 'T_Dataset' from abc.ABCMeta\n",
      " |      Convert a pandas.DataFrame into an xarray.Dataset\n",
      " |      \n",
      " |      Each column will be converted into an independent variable in the\n",
      " |      Dataset. If the dataframe's index is a MultiIndex, it will be expanded\n",
      " |      into a tensor product of one-dimensional indices (filling in missing\n",
      " |      values with NaN). This method will produce a Dataset very similar to\n",
      " |      that on which the 'to_dataframe' method was called, except with\n",
      " |      possibly redundant dimensions (since all dataset variables will have\n",
      " |      the same dimensionality)\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      dataframe : DataFrame\n",
      " |          DataFrame from which to copy data and indices.\n",
      " |      sparse : bool, default: False\n",
      " |          If true, create a sparse arrays instead of dense numpy arrays. This\n",
      " |          can potentially save a large amount of memory if the DataFrame has\n",
      " |          a MultiIndex. Requires the sparse package (sparse.pydata.org).\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      New Dataset.\n",
      " |      \n",
      " |      See Also\n",
      " |      --------\n",
      " |      xarray.DataArray.from_series\n",
      " |      pandas.DataFrame.to_xarray\n",
      " |  \n",
      " |  from_dict(d: 'Mapping[Any, Any]') -> 'T_Dataset' from abc.ABCMeta\n",
      " |      Convert a dictionary into an xarray.Dataset.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      d : dict-like\n",
      " |          Mapping with a minimum structure of\n",
      " |              ``{\"var_0\": {\"dims\": [..], \"data\": [..]},                             ...}``\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      obj : Dataset\n",
      " |      \n",
      " |      See also\n",
      " |      --------\n",
      " |      Dataset.to_dict\n",
      " |      DataArray.from_dict\n",
      " |      \n",
      " |      Examples\n",
      " |      --------\n",
      " |      >>> d = {\n",
      " |      ...     \"t\": {\"dims\": (\"t\"), \"data\": [0, 1, 2]},\n",
      " |      ...     \"a\": {\"dims\": (\"t\"), \"data\": [\"a\", \"b\", \"c\"]},\n",
      " |      ...     \"b\": {\"dims\": (\"t\"), \"data\": [10, 20, 30]},\n",
      " |      ... }\n",
      " |      >>> ds = xr.Dataset.from_dict(d)\n",
      " |      >>> ds\n",
      " |      <xarray.Dataset>\n",
      " |      Dimensions:  (t: 3)\n",
      " |      Coordinates:\n",
      " |        * t        (t) int64 0 1 2\n",
      " |      Data variables:\n",
      " |          a        (t) <U1 'a' 'b' 'c'\n",
      " |          b        (t) int64 10 20 30\n",
      " |      \n",
      " |      >>> d = {\n",
      " |      ...     \"coords\": {\n",
      " |      ...         \"t\": {\"dims\": \"t\", \"data\": [0, 1, 2], \"attrs\": {\"units\": \"s\"}}\n",
      " |      ...     },\n",
      " |      ...     \"attrs\": {\"title\": \"air temperature\"},\n",
      " |      ...     \"dims\": \"t\",\n",
      " |      ...     \"data_vars\": {\n",
      " |      ...         \"a\": {\"dims\": \"t\", \"data\": [10, 20, 30]},\n",
      " |      ...         \"b\": {\"dims\": \"t\", \"data\": [\"a\", \"b\", \"c\"]},\n",
      " |      ...     },\n",
      " |      ... }\n",
      " |      >>> ds = xr.Dataset.from_dict(d)\n",
      " |      >>> ds\n",
      " |      <xarray.Dataset>\n",
      " |      Dimensions:  (t: 3)\n",
      " |      Coordinates:\n",
      " |        * t        (t) int64 0 1 2\n",
      " |      Data variables:\n",
      " |          a        (t) int64 10 20 30\n",
      " |          b        (t) <U1 'a' 'b' 'c'\n",
      " |      Attributes:\n",
      " |          title:    air temperature\n",
      " |  \n",
      " |  load_store(store, decoder=None) -> 'T_Dataset' from abc.ABCMeta\n",
      " |      Create a new dataset from the contents of a backends.*DataStore\n",
      " |      object\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Readonly properties defined here:\n",
      " |  \n",
      " |  __dask_optimize__\n",
      " |  \n",
      " |  __dask_scheduler__\n",
      " |  \n",
      " |  chunks\n",
      " |      Mapping from dimension names to block lengths for this dataset's data, or None if\n",
      " |      the underlying data is not a dask array.\n",
      " |      Cannot be modified directly, but can be modified by calling .chunk().\n",
      " |      \n",
      " |      Same as Dataset.chunksizes, but maintained for backwards compatibility.\n",
      " |      \n",
      " |      See Also\n",
      " |      --------\n",
      " |      Dataset.chunk\n",
      " |      Dataset.chunksizes\n",
      " |      xarray.unify_chunks\n",
      " |  \n",
      " |  chunksizes\n",
      " |      Mapping from dimension names to block lengths for this dataset's data, or None if\n",
      " |      the underlying data is not a dask array.\n",
      " |      Cannot be modified directly, but can be modified by calling .chunk().\n",
      " |      \n",
      " |      Same as Dataset.chunks.\n",
      " |      \n",
      " |      See Also\n",
      " |      --------\n",
      " |      Dataset.chunk\n",
      " |      Dataset.chunks\n",
      " |      xarray.unify_chunks\n",
      " |  \n",
      " |  coords\n",
      " |      Dictionary of xarray.DataArray objects corresponding to coordinate\n",
      " |      variables\n",
      " |  \n",
      " |  data_vars\n",
      " |      Dictionary of DataArray objects corresponding to data variables\n",
      " |  \n",
      " |  dims\n",
      " |      Mapping from dimension names to lengths.\n",
      " |      \n",
      " |      Cannot be modified directly, but is updated when adding new variables.\n",
      " |      \n",
      " |      Note that type of this object differs from `DataArray.dims`.\n",
      " |      See `Dataset.sizes` and `DataArray.sizes` for consistently named\n",
      " |      properties.\n",
      " |      \n",
      " |      See Also\n",
      " |      --------\n",
      " |      Dataset.sizes\n",
      " |      DataArray.dims\n",
      " |  \n",
      " |  dtypes\n",
      " |      Mapping from data variable names to dtypes.\n",
      " |      \n",
      " |      Cannot be modified directly, but is updated when adding new variables.\n",
      " |      \n",
      " |      See Also\n",
      " |      --------\n",
      " |      DataArray.dtype\n",
      " |  \n",
      " |  imag\n",
      " |  \n",
      " |  indexes\n",
      " |      Mapping of pandas.Index objects used for label based indexing.\n",
      " |      \n",
      " |      Raises an error if this Dataset has indexes that cannot be coerced\n",
      " |      to pandas.Index objects.\n",
      " |      \n",
      " |      See Also\n",
      " |      --------\n",
      " |      Dataset.xindexes\n",
      " |  \n",
      " |  loc\n",
      " |      Attribute for location based indexing. Only supports __getitem__,\n",
      " |      and only when the key is a dict of the form {dim: labels}.\n",
      " |  \n",
      " |  nbytes\n",
      " |  \n",
      " |  real\n",
      " |  \n",
      " |  sizes\n",
      " |      Mapping from dimension names to lengths.\n",
      " |      \n",
      " |      Cannot be modified directly, but is updated when adding new variables.\n",
      " |      \n",
      " |      This is an alias for `Dataset.dims` provided for the benefit of\n",
      " |      consistency with `DataArray.sizes`.\n",
      " |      \n",
      " |      See Also\n",
      " |      --------\n",
      " |      DataArray.sizes\n",
      " |  \n",
      " |  variables\n",
      " |      Low level interface to Dataset contents as dict of Variable objects.\n",
      " |      \n",
      " |      This ordered dictionary is frozen to prevent mutation that could\n",
      " |      violate Dataset invariants. It contains all variable objects\n",
      " |      constituting the Dataset, including both data variables and\n",
      " |      coordinates.\n",
      " |  \n",
      " |  xindexes\n",
      " |      Mapping of xarray Index objects used for label based indexing.\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Data descriptors defined here:\n",
      " |  \n",
      " |  __weakref__\n",
      " |      list of weak references to the object (if defined)\n",
      " |  \n",
      " |  attrs\n",
      " |      Dictionary of global attributes on this dataset\n",
      " |  \n",
      " |  encoding\n",
      " |      Dictionary of global encoding attributes on this dataset\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Data and other attributes defined here:\n",
      " |  \n",
      " |  __abstractmethods__ = frozenset()\n",
      " |  \n",
      " |  __annotations__ = {'_attrs': 'dict[Hashable, Any] | None', '_cache': '...\n",
      " |  \n",
      " |  __hash__ = None\n",
      " |  \n",
      " |  __orig_bases__ = (<class 'xarray.core.common.DataWithCoords'>, <class ...\n",
      " |  \n",
      " |  __parameters__ = ()\n",
      " |  \n",
      " |  plot = <class 'xarray.plot.dataset_plot._Dataset_PlotMethods'>\n",
      " |      Enables use of xarray.plot functions as attributes on a Dataset.\n",
      " |      For example, Dataset.plot.scatter\n",
      " |  \n",
      " |  \n",
      " |  rio = <class 'rioxarray.raster_dataset.RasterDataset'>\n",
      " |      This is the GIS extension for :class:`xarray.Dataset`\n",
      " |  \n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Methods inherited from xarray.core.common.DataWithCoords:\n",
      " |  \n",
      " |  __enter__(self: 'T_DataWithCoords') -> 'T_DataWithCoords'\n",
      " |  \n",
      " |  __exit__(self, exc_type, exc_value, traceback) -> 'None'\n",
      " |  \n",
      " |  assign_attrs(self: 'T_DataWithCoords', *args: 'Any', **kwargs: 'Any') -> 'T_DataWithCoords'\n",
      " |      Assign new attrs to this object.\n",
      " |      \n",
      " |      Returns a new object equivalent to ``self.attrs.update(*args, **kwargs)``.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      *args\n",
      " |          positional arguments passed into ``attrs.update``.\n",
      " |      **kwargs\n",
      " |          keyword arguments passed into ``attrs.update``.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      assigned : same type as caller\n",
      " |          A new object with the new attrs in addition to the existing data.\n",
      " |      \n",
      " |      See Also\n",
      " |      --------\n",
      " |      Dataset.assign\n",
      " |  \n",
      " |  assign_coords(self: 'T_DataWithCoords', coords: 'Mapping[Any, Any] | None' = None, **coords_kwargs: 'Any') -> 'T_DataWithCoords'\n",
      " |      Assign new coordinates to this object.\n",
      " |      \n",
      " |      Returns a new object with all the original data in addition to the new\n",
      " |      coordinates.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      coords : dict-like or None, optional\n",
      " |          A dict where the keys are the names of the coordinates\n",
      " |          with the new values to assign. If the values are callable, they are\n",
      " |          computed on this object and assigned to new coordinate variables.\n",
      " |          If the values are not callable, (e.g. a ``DataArray``, scalar, or\n",
      " |          array), they are simply assigned. A new coordinate can also be\n",
      " |          defined and attached to an existing dimension using a tuple with\n",
      " |          the first element the dimension name and the second element the\n",
      " |          values for this new coordinate.\n",
      " |      **coords_kwargs : optional\n",
      " |          The keyword arguments form of ``coords``.\n",
      " |          One of ``coords`` or ``coords_kwargs`` must be provided.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      assigned : same type as caller\n",
      " |          A new object with the new coordinates in addition to the existing\n",
      " |          data.\n",
      " |      \n",
      " |      Examples\n",
      " |      --------\n",
      " |      Convert `DataArray` longitude coordinates from 0-359 to -180-179:\n",
      " |      \n",
      " |      >>> da = xr.DataArray(\n",
      " |      ...     np.random.rand(4),\n",
      " |      ...     coords=[np.array([358, 359, 0, 1])],\n",
      " |      ...     dims=\"lon\",\n",
      " |      ... )\n",
      " |      >>> da\n",
      " |      <xarray.DataArray (lon: 4)>\n",
      " |      array([0.5488135 , 0.71518937, 0.60276338, 0.54488318])\n",
      " |      Coordinates:\n",
      " |        * lon      (lon) int64 358 359 0 1\n",
      " |      >>> da.assign_coords(lon=(((da.lon + 180) % 360) - 180))\n",
      " |      <xarray.DataArray (lon: 4)>\n",
      " |      array([0.5488135 , 0.71518937, 0.60276338, 0.54488318])\n",
      " |      Coordinates:\n",
      " |        * lon      (lon) int64 -2 -1 0 1\n",
      " |      \n",
      " |      The function also accepts dictionary arguments:\n",
      " |      \n",
      " |      >>> da.assign_coords({\"lon\": (((da.lon + 180) % 360) - 180)})\n",
      " |      <xarray.DataArray (lon: 4)>\n",
      " |      array([0.5488135 , 0.71518937, 0.60276338, 0.54488318])\n",
      " |      Coordinates:\n",
      " |        * lon      (lon) int64 -2 -1 0 1\n",
      " |      \n",
      " |      New coordinate can also be attached to an existing dimension:\n",
      " |      \n",
      " |      >>> lon_2 = np.array([300, 289, 0, 1])\n",
      " |      >>> da.assign_coords(lon_2=(\"lon\", lon_2))\n",
      " |      <xarray.DataArray (lon: 4)>\n",
      " |      array([0.5488135 , 0.71518937, 0.60276338, 0.54488318])\n",
      " |      Coordinates:\n",
      " |        * lon      (lon) int64 358 359 0 1\n",
      " |          lon_2    (lon) int64 300 289 0 1\n",
      " |      \n",
      " |      Note that the same result can also be obtained with a dict e.g.\n",
      " |      \n",
      " |      >>> _ = da.assign_coords({\"lon_2\": (\"lon\", lon_2)})\n",
      " |      \n",
      " |      Note the same method applies to `Dataset` objects.\n",
      " |      \n",
      " |      Convert `Dataset` longitude coordinates from 0-359 to -180-179:\n",
      " |      \n",
      " |      >>> temperature = np.linspace(20, 32, num=16).reshape(2, 2, 4)\n",
      " |      >>> precipitation = 2 * np.identity(4).reshape(2, 2, 4)\n",
      " |      >>> ds = xr.Dataset(\n",
      " |      ...     data_vars=dict(\n",
      " |      ...         temperature=([\"x\", \"y\", \"time\"], temperature),\n",
      " |      ...         precipitation=([\"x\", \"y\", \"time\"], precipitation),\n",
      " |      ...     ),\n",
      " |      ...     coords=dict(\n",
      " |      ...         lon=([\"x\", \"y\"], [[260.17, 260.68], [260.21, 260.77]]),\n",
      " |      ...         lat=([\"x\", \"y\"], [[42.25, 42.21], [42.63, 42.59]]),\n",
      " |      ...         time=pd.date_range(\"2014-09-06\", periods=4),\n",
      " |      ...         reference_time=pd.Timestamp(\"2014-09-05\"),\n",
      " |      ...     ),\n",
      " |      ...     attrs=dict(description=\"Weather-related data\"),\n",
      " |      ... )\n",
      " |      >>> ds\n",
      " |      <xarray.Dataset>\n",
      " |      Dimensions:         (x: 2, y: 2, time: 4)\n",
      " |      Coordinates:\n",
      " |          lon             (x, y) float64 260.2 260.7 260.2 260.8\n",
      " |          lat             (x, y) float64 42.25 42.21 42.63 42.59\n",
      " |        * time            (time) datetime64[ns] 2014-09-06 2014-09-07 ... 2014-09-09\n",
      " |          reference_time  datetime64[ns] 2014-09-05\n",
      " |      Dimensions without coordinates: x, y\n",
      " |      Data variables:\n",
      " |          temperature     (x, y, time) float64 20.0 20.8 21.6 22.4 ... 30.4 31.2 32.0\n",
      " |          precipitation   (x, y, time) float64 2.0 0.0 0.0 0.0 0.0 ... 0.0 0.0 0.0 2.0\n",
      " |      Attributes:\n",
      " |          description:  Weather-related data\n",
      " |      >>> ds.assign_coords(lon=(((ds.lon + 180) % 360) - 180))\n",
      " |      <xarray.Dataset>\n",
      " |      Dimensions:         (x: 2, y: 2, time: 4)\n",
      " |      Coordinates:\n",
      " |          lon             (x, y) float64 -99.83 -99.32 -99.79 -99.23\n",
      " |          lat             (x, y) float64 42.25 42.21 42.63 42.59\n",
      " |        * time            (time) datetime64[ns] 2014-09-06 2014-09-07 ... 2014-09-09\n",
      " |          reference_time  datetime64[ns] 2014-09-05\n",
      " |      Dimensions without coordinates: x, y\n",
      " |      Data variables:\n",
      " |          temperature     (x, y, time) float64 20.0 20.8 21.6 22.4 ... 30.4 31.2 32.0\n",
      " |          precipitation   (x, y, time) float64 2.0 0.0 0.0 0.0 0.0 ... 0.0 0.0 0.0 2.0\n",
      " |      Attributes:\n",
      " |          description:  Weather-related data\n",
      " |      \n",
      " |      Notes\n",
      " |      -----\n",
      " |      Since ``coords_kwargs`` is a dictionary, the order of your arguments\n",
      " |      may not be preserved, and so the order of the new variables is not well\n",
      " |      defined. Assigning multiple variables within the same ``assign_coords``\n",
      " |      is possible, but you cannot reference other variables created within\n",
      " |      the same ``assign_coords`` call.\n",
      " |      \n",
      " |      See Also\n",
      " |      --------\n",
      " |      Dataset.assign\n",
      " |      Dataset.swap_dims\n",
      " |  \n",
      " |  astype(self: 'T_DataWithCoords', dtype, *, order=None, casting=None, subok=None, copy=None, keep_attrs=True) -> 'T_DataWithCoords'\n",
      " |      Copy of the xarray object, with data cast to a specified type.\n",
      " |      Leaves coordinate dtype unchanged.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      dtype : str or dtype\n",
      " |          Typecode or data-type to which the array is cast.\n",
      " |      order : {'C', 'F', 'A', 'K'}, optional\n",
      " |          Controls the memory layout order of the result. C means C order,\n",
      " |          F means Fortran order, A means F order if all the arrays are\n",
      " |          Fortran contiguous, C order otherwise, and K means as close to\n",
      " |          the order the array elements appear in memory as possible.\n",
      " |      casting : {'no', 'equiv', 'safe', 'same_kind', 'unsafe'}, optional\n",
      " |          Controls what kind of data casting may occur.\n",
      " |      \n",
      " |          * 'no' means the data types should not be cast at all.\n",
      " |          * 'equiv' means only byte-order changes are allowed.\n",
      " |          * 'safe' means only casts which can preserve values are allowed.\n",
      " |          * 'same_kind' means only safe casts or casts within a kind,\n",
      " |            like float64 to float32, are allowed.\n",
      " |          * 'unsafe' means any data conversions may be done.\n",
      " |      subok : bool, optional\n",
      " |          If True, then sub-classes will be passed-through, otherwise the\n",
      " |          returned array will be forced to be a base-class array.\n",
      " |      copy : bool, optional\n",
      " |          By default, astype always returns a newly allocated array. If this\n",
      " |          is set to False and the `dtype` requirement is satisfied, the input\n",
      " |          array is returned instead of a copy.\n",
      " |      keep_attrs : bool, optional\n",
      " |          By default, astype keeps attributes. Set to False to remove\n",
      " |          attributes in the returned object.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      out : same as object\n",
      " |          New object with data cast to the specified type.\n",
      " |      \n",
      " |      Notes\n",
      " |      -----\n",
      " |      The ``order``, ``casting``, ``subok`` and ``copy`` arguments are only passed\n",
      " |      through to the ``astype`` method of the underlying array when a value\n",
      " |      different than ``None`` is supplied.\n",
      " |      Make sure to only supply these arguments if the underlying array class\n",
      " |      supports them.\n",
      " |      \n",
      " |      See Also\n",
      " |      --------\n",
      " |      numpy.ndarray.astype\n",
      " |      dask.array.Array.astype\n",
      " |      sparse.COO.astype\n",
      " |  \n",
      " |  clip(self: 'T_DataWithCoords', min: 'ScalarOrArray | None' = None, max: 'ScalarOrArray | None' = None, *, keep_attrs: 'bool | None' = None) -> 'T_DataWithCoords'\n",
      " |      Return an array whose values are limited to ``[min, max]``.\n",
      " |      At least one of max or min must be given.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      min : None or Hashable, optional\n",
      " |          Minimum value. If None, no lower clipping is performed.\n",
      " |      max : None or Hashable, optional\n",
      " |          Maximum value. If None, no upper clipping is performed.\n",
      " |      keep_attrs : bool or None, optional\n",
      " |          If True, the attributes (`attrs`) will be copied from\n",
      " |          the original object to the new one. If False, the new\n",
      " |          object will be returned without attributes.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      clipped : same type as caller\n",
      " |          This object, but with with values < min are replaced with min,\n",
      " |          and those > max with max.\n",
      " |      \n",
      " |      See Also\n",
      " |      --------\n",
      " |      numpy.clip : equivalent function\n",
      " |  \n",
      " |  close(self) -> 'None'\n",
      " |      Release any resources linked to this object.\n",
      " |  \n",
      " |  get_index(self, key: 'Hashable') -> 'pd.Index'\n",
      " |      Get an index for a dimension, with fall-back to a default RangeIndex\n",
      " |  \n",
      " |  isin(self: 'T_DataWithCoords', test_elements: 'Any') -> 'T_DataWithCoords'\n",
      " |      Tests each value in the array for whether it is in test elements.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      test_elements : array_like\n",
      " |          The values against which to test each value of `element`.\n",
      " |          This argument is flattened if an array or array_like.\n",
      " |          See numpy notes for behavior with non-array-like parameters.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      isin : DataArray or Dataset\n",
      " |          Has the same type and shape as this object, but with a bool dtype.\n",
      " |      \n",
      " |      Examples\n",
      " |      --------\n",
      " |      >>> array = xr.DataArray([1, 2, 3], dims=\"x\")\n",
      " |      >>> array.isin([1, 3])\n",
      " |      <xarray.DataArray (x: 3)>\n",
      " |      array([ True, False,  True])\n",
      " |      Dimensions without coordinates: x\n",
      " |      \n",
      " |      See Also\n",
      " |      --------\n",
      " |      numpy.isin\n",
      " |  \n",
      " |  isnull(self: 'T_DataWithCoords', keep_attrs: 'bool | None' = None) -> 'T_DataWithCoords'\n",
      " |      Test each value in the array for whether it is a missing value.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      keep_attrs : bool or None, optional\n",
      " |          If True, the attributes (`attrs`) will be copied from\n",
      " |          the original object to the new one. If False, the new\n",
      " |          object will be returned without attributes.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      isnull : DataArray or Dataset\n",
      " |          Same type and shape as object, but the dtype of the data is bool.\n",
      " |      \n",
      " |      See Also\n",
      " |      --------\n",
      " |      pandas.isnull\n",
      " |      \n",
      " |      Examples\n",
      " |      --------\n",
      " |      >>> array = xr.DataArray([1, np.nan, 3], dims=\"x\")\n",
      " |      >>> array\n",
      " |      <xarray.DataArray (x: 3)>\n",
      " |      array([ 1., nan,  3.])\n",
      " |      Dimensions without coordinates: x\n",
      " |      >>> array.isnull()\n",
      " |      <xarray.DataArray (x: 3)>\n",
      " |      array([False,  True, False])\n",
      " |      Dimensions without coordinates: x\n",
      " |  \n",
      " |  notnull(self: 'T_DataWithCoords', keep_attrs: 'bool | None' = None) -> 'T_DataWithCoords'\n",
      " |      Test each value in the array for whether it is not a missing value.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      keep_attrs : bool or None, optional\n",
      " |          If True, the attributes (`attrs`) will be copied from\n",
      " |          the original object to the new one. If False, the new\n",
      " |          object will be returned without attributes.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      notnull : DataArray or Dataset\n",
      " |          Same type and shape as object, but the dtype of the data is bool.\n",
      " |      \n",
      " |      See Also\n",
      " |      --------\n",
      " |      pandas.notnull\n",
      " |      \n",
      " |      Examples\n",
      " |      --------\n",
      " |      >>> array = xr.DataArray([1, np.nan, 3], dims=\"x\")\n",
      " |      >>> array\n",
      " |      <xarray.DataArray (x: 3)>\n",
      " |      array([ 1., nan,  3.])\n",
      " |      Dimensions without coordinates: x\n",
      " |      >>> array.notnull()\n",
      " |      <xarray.DataArray (x: 3)>\n",
      " |      array([ True, False,  True])\n",
      " |      Dimensions without coordinates: x\n",
      " |  \n",
      " |  pipe(self, func: 'Callable[..., T] | tuple[Callable[..., T], str]', *args: 'Any', **kwargs: 'Any') -> 'T'\n",
      " |      Apply ``func(self, *args, **kwargs)``\n",
      " |      \n",
      " |      This method replicates the pandas method of the same name.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      func : callable\n",
      " |          function to apply to this xarray object (Dataset/DataArray).\n",
      " |          ``args``, and ``kwargs`` are passed into ``func``.\n",
      " |          Alternatively a ``(callable, data_keyword)`` tuple where\n",
      " |          ``data_keyword`` is a string indicating the keyword of\n",
      " |          ``callable`` that expects the xarray object.\n",
      " |      *args\n",
      " |          positional arguments passed into ``func``.\n",
      " |      **kwargs\n",
      " |          a dictionary of keyword arguments passed into ``func``.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      object : Any\n",
      " |          the return type of ``func``.\n",
      " |      \n",
      " |      Notes\n",
      " |      -----\n",
      " |      Use ``.pipe`` when chaining together functions that expect\n",
      " |      xarray or pandas objects, e.g., instead of writing\n",
      " |      \n",
      " |      .. code:: python\n",
      " |      \n",
      " |          f(g(h(ds), arg1=a), arg2=b, arg3=c)\n",
      " |      \n",
      " |      You can write\n",
      " |      \n",
      " |      .. code:: python\n",
      " |      \n",
      " |          (ds.pipe(h).pipe(g, arg1=a).pipe(f, arg2=b, arg3=c))\n",
      " |      \n",
      " |      If you have a function that takes the data as (say) the second\n",
      " |      argument, pass a tuple indicating which keyword expects the\n",
      " |      data. For example, suppose ``f`` takes its data as ``arg2``:\n",
      " |      \n",
      " |      .. code:: python\n",
      " |      \n",
      " |          (ds.pipe(h).pipe(g, arg1=a).pipe((f, \"arg2\"), arg1=a, arg3=c))\n",
      " |      \n",
      " |      Examples\n",
      " |      --------\n",
      " |      >>> x = xr.Dataset(\n",
      " |      ...     {\n",
      " |      ...         \"temperature_c\": (\n",
      " |      ...             (\"lat\", \"lon\"),\n",
      " |      ...             20 * np.random.rand(4).reshape(2, 2),\n",
      " |      ...         ),\n",
      " |      ...         \"precipitation\": ((\"lat\", \"lon\"), np.random.rand(4).reshape(2, 2)),\n",
      " |      ...     },\n",
      " |      ...     coords={\"lat\": [10, 20], \"lon\": [150, 160]},\n",
      " |      ... )\n",
      " |      >>> x\n",
      " |      <xarray.Dataset>\n",
      " |      Dimensions:        (lat: 2, lon: 2)\n",
      " |      Coordinates:\n",
      " |        * lat            (lat) int64 10 20\n",
      " |        * lon            (lon) int64 150 160\n",
      " |      Data variables:\n",
      " |          temperature_c  (lat, lon) float64 10.98 14.3 12.06 10.9\n",
      " |          precipitation  (lat, lon) float64 0.4237 0.6459 0.4376 0.8918\n",
      " |      \n",
      " |      >>> def adder(data, arg):\n",
      " |      ...     return data + arg\n",
      " |      ...\n",
      " |      >>> def div(data, arg):\n",
      " |      ...     return data / arg\n",
      " |      ...\n",
      " |      >>> def sub_mult(data, sub_arg, mult_arg):\n",
      " |      ...     return (data * mult_arg) - sub_arg\n",
      " |      ...\n",
      " |      >>> x.pipe(adder, 2)\n",
      " |      <xarray.Dataset>\n",
      " |      Dimensions:        (lat: 2, lon: 2)\n",
      " |      Coordinates:\n",
      " |        * lat            (lat) int64 10 20\n",
      " |        * lon            (lon) int64 150 160\n",
      " |      Data variables:\n",
      " |          temperature_c  (lat, lon) float64 12.98 16.3 14.06 12.9\n",
      " |          precipitation  (lat, lon) float64 2.424 2.646 2.438 2.892\n",
      " |      \n",
      " |      >>> x.pipe(adder, arg=2)\n",
      " |      <xarray.Dataset>\n",
      " |      Dimensions:        (lat: 2, lon: 2)\n",
      " |      Coordinates:\n",
      " |        * lat            (lat) int64 10 20\n",
      " |        * lon            (lon) int64 150 160\n",
      " |      Data variables:\n",
      " |          temperature_c  (lat, lon) float64 12.98 16.3 14.06 12.9\n",
      " |          precipitation  (lat, lon) float64 2.424 2.646 2.438 2.892\n",
      " |      \n",
      " |      >>> (\n",
      " |      ...     x.pipe(adder, arg=2)\n",
      " |      ...     .pipe(div, arg=2)\n",
      " |      ...     .pipe(sub_mult, sub_arg=2, mult_arg=2)\n",
      " |      ... )\n",
      " |      <xarray.Dataset>\n",
      " |      Dimensions:        (lat: 2, lon: 2)\n",
      " |      Coordinates:\n",
      " |        * lat            (lat) int64 10 20\n",
      " |        * lon            (lon) int64 150 160\n",
      " |      Data variables:\n",
      " |          temperature_c  (lat, lon) float64 10.98 14.3 12.06 10.9\n",
      " |          precipitation  (lat, lon) float64 0.4237 0.6459 0.4376 0.8918\n",
      " |      \n",
      " |      See Also\n",
      " |      --------\n",
      " |      pandas.DataFrame.pipe\n",
      " |  \n",
      " |  rolling_exp(self: 'T_DataWithCoords', window: 'Mapping[Any, int]' = None, window_type: 'str' = 'span', **window_kwargs) -> 'RollingExp[T_DataWithCoords]'\n",
      " |      Exponentially-weighted moving window.\n",
      " |      Similar to EWM in pandas\n",
      " |      \n",
      " |      Requires the optional Numbagg dependency.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      window : mapping of hashable to int, optional\n",
      " |          A mapping from the name of the dimension to create the rolling\n",
      " |          exponential window along (e.g. `time`) to the size of the moving window.\n",
      " |      window_type : {\"span\", \"com\", \"halflife\", \"alpha\"}, default: \"span\"\n",
      " |          The format of the previously supplied window. Each is a simple\n",
      " |          numerical transformation of the others. Described in detail:\n",
      " |          https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.DataFrame.ewm.html\n",
      " |      **window_kwargs : optional\n",
      " |          The keyword arguments form of ``window``.\n",
      " |          One of window or window_kwargs must be provided.\n",
      " |      \n",
      " |      See Also\n",
      " |      --------\n",
      " |      core.rolling_exp.RollingExp\n",
      " |  \n",
      " |  set_close(self, close: 'Callable[[], None] | None') -> 'None'\n",
      " |      Register the function that releases any resources linked to this object.\n",
      " |      \n",
      " |      This method controls how xarray cleans up resources associated\n",
      " |      with this object when the ``.close()`` method is called. It is mostly\n",
      " |      intended for backend developers and it is rarely needed by regular\n",
      " |      end-users.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      close : callable\n",
      " |          The function that when called like ``close()`` releases\n",
      " |          any resources linked to this object.\n",
      " |  \n",
      " |  squeeze(self: 'T_DataWithCoords', dim: 'Hashable | Iterable[Hashable] | None' = None, drop: 'bool' = False, axis: 'int | Iterable[int] | None' = None) -> 'T_DataWithCoords'\n",
      " |      Return a new object with squeezed data.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      dim : None or Hashable or iterable of Hashable, optional\n",
      " |          Selects a subset of the length one dimensions. If a dimension is\n",
      " |          selected with length greater than one, an error is raised. If\n",
      " |          None, all length one dimensions are squeezed.\n",
      " |      drop : bool, default: False\n",
      " |          If ``drop=True``, drop squeezed coordinates instead of making them\n",
      " |          scalar.\n",
      " |      axis : None or int or iterable of int, optional\n",
      " |          Like dim, but positional.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      squeezed : same type as caller\n",
      " |          This object, but with with all or a subset of the dimensions of\n",
      " |          length 1 removed.\n",
      " |      \n",
      " |      See Also\n",
      " |      --------\n",
      " |      numpy.squeeze\n",
      " |  \n",
      " |  where(self: 'T_DataWithCoords', cond: 'Any', other: 'Any' = <NA>, drop: 'bool' = False) -> 'T_DataWithCoords'\n",
      " |      Filter elements from this object according to a condition.\n",
      " |      \n",
      " |      This operation follows the normal broadcasting and alignment rules that\n",
      " |      xarray uses for binary arithmetic.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      cond : DataArray, Dataset, or callable\n",
      " |          Locations at which to preserve this object's values. dtype must be `bool`.\n",
      " |          If a callable, it must expect this object as its only parameter.\n",
      " |      other : scalar, DataArray or Dataset, optional\n",
      " |          Value to use for locations in this object where ``cond`` is False.\n",
      " |          By default, these locations filled with NA.\n",
      " |      drop : bool, default: False\n",
      " |          If True, coordinate labels that only correspond to False values of\n",
      " |          the condition are dropped from the result.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      DataArray or Dataset\n",
      " |          Same xarray type as caller, with dtype float64.\n",
      " |      \n",
      " |      Examples\n",
      " |      --------\n",
      " |      >>> a = xr.DataArray(np.arange(25).reshape(5, 5), dims=(\"x\", \"y\"))\n",
      " |      >>> a\n",
      " |      <xarray.DataArray (x: 5, y: 5)>\n",
      " |      array([[ 0,  1,  2,  3,  4],\n",
      " |             [ 5,  6,  7,  8,  9],\n",
      " |             [10, 11, 12, 13, 14],\n",
      " |             [15, 16, 17, 18, 19],\n",
      " |             [20, 21, 22, 23, 24]])\n",
      " |      Dimensions without coordinates: x, y\n",
      " |      \n",
      " |      >>> a.where(a.x + a.y < 4)\n",
      " |      <xarray.DataArray (x: 5, y: 5)>\n",
      " |      array([[ 0.,  1.,  2.,  3., nan],\n",
      " |             [ 5.,  6.,  7., nan, nan],\n",
      " |             [10., 11., nan, nan, nan],\n",
      " |             [15., nan, nan, nan, nan],\n",
      " |             [nan, nan, nan, nan, nan]])\n",
      " |      Dimensions without coordinates: x, y\n",
      " |      \n",
      " |      >>> a.where(a.x + a.y < 5, -1)\n",
      " |      <xarray.DataArray (x: 5, y: 5)>\n",
      " |      array([[ 0,  1,  2,  3,  4],\n",
      " |             [ 5,  6,  7,  8, -1],\n",
      " |             [10, 11, 12, -1, -1],\n",
      " |             [15, 16, -1, -1, -1],\n",
      " |             [20, -1, -1, -1, -1]])\n",
      " |      Dimensions without coordinates: x, y\n",
      " |      \n",
      " |      >>> a.where(a.x + a.y < 4, drop=True)\n",
      " |      <xarray.DataArray (x: 4, y: 4)>\n",
      " |      array([[ 0.,  1.,  2.,  3.],\n",
      " |             [ 5.,  6.,  7., nan],\n",
      " |             [10., 11., nan, nan],\n",
      " |             [15., nan, nan, nan]])\n",
      " |      Dimensions without coordinates: x, y\n",
      " |      \n",
      " |      >>> a.where(lambda x: x.x + x.y < 4, drop=True)\n",
      " |      <xarray.DataArray (x: 4, y: 4)>\n",
      " |      array([[ 0.,  1.,  2.,  3.],\n",
      " |             [ 5.,  6.,  7., nan],\n",
      " |             [10., 11., nan, nan],\n",
      " |             [15., nan, nan, nan]])\n",
      " |      Dimensions without coordinates: x, y\n",
      " |      \n",
      " |      >>> a.where(a.x + a.y < 4, -1, drop=True)\n",
      " |      <xarray.DataArray (x: 4, y: 4)>\n",
      " |      array([[ 0,  1,  2,  3],\n",
      " |             [ 5,  6,  7, -1],\n",
      " |             [10, 11, -1, -1],\n",
      " |             [15, -1, -1, -1]])\n",
      " |      Dimensions without coordinates: x, y\n",
      " |      \n",
      " |      See Also\n",
      " |      --------\n",
      " |      numpy.where : corresponding numpy function\n",
      " |      where : equivalent function\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Methods inherited from xarray.core.common.AttrAccessMixin:\n",
      " |  \n",
      " |  __dir__(self) -> 'list[str]'\n",
      " |      Provide method name lookup and completion. Only provide 'public'\n",
      " |      methods.\n",
      " |  \n",
      " |  __getattr__(self, name: 'str') -> 'Any'\n",
      " |  \n",
      " |  __setattr__(self, name: 'str', value: 'Any') -> 'None'\n",
      " |      Objects with ``__slots__`` raise AttributeError if you try setting an\n",
      " |      undeclared attribute. This is desirable, but the error message could use some\n",
      " |      improvement.\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Class methods inherited from xarray.core.common.AttrAccessMixin:\n",
      " |  \n",
      " |  __init_subclass__(**kwargs) from abc.ABCMeta\n",
      " |      Verify that all subclasses explicitly define ``__slots__``. If they don't,\n",
      " |      raise error in the core xarray module and a FutureWarning in third-party\n",
      " |      extensions.\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Methods inherited from xarray.core._reductions.DatasetReductions:\n",
      " |  \n",
      " |  all(self, dim: 'None | Hashable | Sequence[Hashable]' = None, *, keep_attrs: 'bool | None' = None, **kwargs: 'Any') -> 'Dataset'\n",
      " |      Reduce this Dataset's data by applying ``all`` along some dimension(s).\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      dim : hashable or iterable of hashable, default: None\n",
      " |          Name of dimension[s] along which to apply ``all``. For e.g. ``dim=\"x\"``\n",
      " |          or ``dim=[\"x\", \"y\"]``. If None, will reduce over all dimensions.\n",
      " |      keep_attrs : bool or None, optional\n",
      " |          If True, ``attrs`` will be copied from the original\n",
      " |          object to the new one.  If False, the new object will be\n",
      " |          returned without attributes.\n",
      " |      **kwargs : Any\n",
      " |          Additional keyword arguments passed on to the appropriate array\n",
      " |          function for calculating ``all`` on this object's data.\n",
      " |          These could include dask-specific kwargs like ``split_every``.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      reduced : Dataset\n",
      " |          New Dataset with ``all`` applied to its data and the\n",
      " |          indicated dimension(s) removed\n",
      " |      \n",
      " |      See Also\n",
      " |      --------\n",
      " |      numpy.all\n",
      " |      dask.array.all\n",
      " |      DataArray.all\n",
      " |      :ref:`agg`\n",
      " |          User guide on reduction or aggregation operations.\n",
      " |      \n",
      " |      Examples\n",
      " |      --------\n",
      " |      >>> da = xr.DataArray(\n",
      " |      ...     np.array([True, True, True, True, True, False], dtype=bool),\n",
      " |      ...     dims=\"time\",\n",
      " |      ...     coords=dict(\n",
      " |      ...         time=(\"time\", pd.date_range(\"01-01-2001\", freq=\"M\", periods=6)),\n",
      " |      ...         labels=(\"time\", np.array([\"a\", \"b\", \"c\", \"c\", \"b\", \"a\"])),\n",
      " |      ...     ),\n",
      " |      ... )\n",
      " |      >>> ds = xr.Dataset(dict(da=da))\n",
      " |      >>> ds\n",
      " |      <xarray.Dataset>\n",
      " |      Dimensions:  (time: 6)\n",
      " |      Coordinates:\n",
      " |        * time     (time) datetime64[ns] 2001-01-31 2001-02-28 ... 2001-06-30\n",
      " |          labels   (time) <U1 'a' 'b' 'c' 'c' 'b' 'a'\n",
      " |      Data variables:\n",
      " |          da       (time) bool True True True True True False\n",
      " |      \n",
      " |      >>> ds.all()\n",
      " |      <xarray.Dataset>\n",
      " |      Dimensions:  ()\n",
      " |      Data variables:\n",
      " |          da       bool False\n",
      " |  \n",
      " |  any(self, dim: 'None | Hashable | Sequence[Hashable]' = None, *, keep_attrs: 'bool | None' = None, **kwargs: 'Any') -> 'Dataset'\n",
      " |      Reduce this Dataset's data by applying ``any`` along some dimension(s).\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      dim : hashable or iterable of hashable, default: None\n",
      " |          Name of dimension[s] along which to apply ``any``. For e.g. ``dim=\"x\"``\n",
      " |          or ``dim=[\"x\", \"y\"]``. If None, will reduce over all dimensions.\n",
      " |      keep_attrs : bool or None, optional\n",
      " |          If True, ``attrs`` will be copied from the original\n",
      " |          object to the new one.  If False, the new object will be\n",
      " |          returned without attributes.\n",
      " |      **kwargs : Any\n",
      " |          Additional keyword arguments passed on to the appropriate array\n",
      " |          function for calculating ``any`` on this object's data.\n",
      " |          These could include dask-specific kwargs like ``split_every``.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      reduced : Dataset\n",
      " |          New Dataset with ``any`` applied to its data and the\n",
      " |          indicated dimension(s) removed\n",
      " |      \n",
      " |      See Also\n",
      " |      --------\n",
      " |      numpy.any\n",
      " |      dask.array.any\n",
      " |      DataArray.any\n",
      " |      :ref:`agg`\n",
      " |          User guide on reduction or aggregation operations.\n",
      " |      \n",
      " |      Examples\n",
      " |      --------\n",
      " |      >>> da = xr.DataArray(\n",
      " |      ...     np.array([True, True, True, True, True, False], dtype=bool),\n",
      " |      ...     dims=\"time\",\n",
      " |      ...     coords=dict(\n",
      " |      ...         time=(\"time\", pd.date_range(\"01-01-2001\", freq=\"M\", periods=6)),\n",
      " |      ...         labels=(\"time\", np.array([\"a\", \"b\", \"c\", \"c\", \"b\", \"a\"])),\n",
      " |      ...     ),\n",
      " |      ... )\n",
      " |      >>> ds = xr.Dataset(dict(da=da))\n",
      " |      >>> ds\n",
      " |      <xarray.Dataset>\n",
      " |      Dimensions:  (time: 6)\n",
      " |      Coordinates:\n",
      " |        * time     (time) datetime64[ns] 2001-01-31 2001-02-28 ... 2001-06-30\n",
      " |          labels   (time) <U1 'a' 'b' 'c' 'c' 'b' 'a'\n",
      " |      Data variables:\n",
      " |          da       (time) bool True True True True True False\n",
      " |      \n",
      " |      >>> ds.any()\n",
      " |      <xarray.Dataset>\n",
      " |      Dimensions:  ()\n",
      " |      Data variables:\n",
      " |          da       bool True\n",
      " |  \n",
      " |  count(self, dim: 'None | Hashable | Sequence[Hashable]' = None, *, keep_attrs: 'bool | None' = None, **kwargs: 'Any') -> 'Dataset'\n",
      " |      Reduce this Dataset's data by applying ``count`` along some dimension(s).\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      dim : hashable or iterable of hashable, default: None\n",
      " |          Name of dimension[s] along which to apply ``count``. For e.g. ``dim=\"x\"``\n",
      " |          or ``dim=[\"x\", \"y\"]``. If None, will reduce over all dimensions.\n",
      " |      keep_attrs : bool or None, optional\n",
      " |          If True, ``attrs`` will be copied from the original\n",
      " |          object to the new one.  If False, the new object will be\n",
      " |          returned without attributes.\n",
      " |      **kwargs : Any\n",
      " |          Additional keyword arguments passed on to the appropriate array\n",
      " |          function for calculating ``count`` on this object's data.\n",
      " |          These could include dask-specific kwargs like ``split_every``.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      reduced : Dataset\n",
      " |          New Dataset with ``count`` applied to its data and the\n",
      " |          indicated dimension(s) removed\n",
      " |      \n",
      " |      See Also\n",
      " |      --------\n",
      " |      numpy.count\n",
      " |      dask.array.count\n",
      " |      DataArray.count\n",
      " |      :ref:`agg`\n",
      " |          User guide on reduction or aggregation operations.\n",
      " |      \n",
      " |      Examples\n",
      " |      --------\n",
      " |      >>> da = xr.DataArray(\n",
      " |      ...     np.array([1, 2, 3, 1, 2, np.nan]),\n",
      " |      ...     dims=\"time\",\n",
      " |      ...     coords=dict(\n",
      " |      ...         time=(\"time\", pd.date_range(\"01-01-2001\", freq=\"M\", periods=6)),\n",
      " |      ...         labels=(\"time\", np.array([\"a\", \"b\", \"c\", \"c\", \"b\", \"a\"])),\n",
      " |      ...     ),\n",
      " |      ... )\n",
      " |      >>> ds = xr.Dataset(dict(da=da))\n",
      " |      >>> ds\n",
      " |      <xarray.Dataset>\n",
      " |      Dimensions:  (time: 6)\n",
      " |      Coordinates:\n",
      " |        * time     (time) datetime64[ns] 2001-01-31 2001-02-28 ... 2001-06-30\n",
      " |          labels   (time) <U1 'a' 'b' 'c' 'c' 'b' 'a'\n",
      " |      Data variables:\n",
      " |          da       (time) float64 1.0 2.0 3.0 1.0 2.0 nan\n",
      " |      \n",
      " |      >>> ds.count()\n",
      " |      <xarray.Dataset>\n",
      " |      Dimensions:  ()\n",
      " |      Data variables:\n",
      " |          da       int64 5\n",
      " |  \n",
      " |  max(self, dim: 'None | Hashable | Sequence[Hashable]' = None, *, skipna: 'bool | None' = None, keep_attrs: 'bool | None' = None, **kwargs: 'Any') -> 'Dataset'\n",
      " |      Reduce this Dataset's data by applying ``max`` along some dimension(s).\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      dim : hashable or iterable of hashable, default: None\n",
      " |          Name of dimension[s] along which to apply ``max``. For e.g. ``dim=\"x\"``\n",
      " |          or ``dim=[\"x\", \"y\"]``. If None, will reduce over all dimensions.\n",
      " |      skipna : bool or None, optional\n",
      " |          If True, skip missing values (as marked by NaN). By default, only\n",
      " |          skips missing values for float dtypes; other dtypes either do not\n",
      " |          have a sentinel missing value (int) or ``skipna=True`` has not been\n",
      " |          implemented (object, datetime64 or timedelta64).\n",
      " |      keep_attrs : bool or None, optional\n",
      " |          If True, ``attrs`` will be copied from the original\n",
      " |          object to the new one.  If False, the new object will be\n",
      " |          returned without attributes.\n",
      " |      **kwargs : Any\n",
      " |          Additional keyword arguments passed on to the appropriate array\n",
      " |          function for calculating ``max`` on this object's data.\n",
      " |          These could include dask-specific kwargs like ``split_every``.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      reduced : Dataset\n",
      " |          New Dataset with ``max`` applied to its data and the\n",
      " |          indicated dimension(s) removed\n",
      " |      \n",
      " |      See Also\n",
      " |      --------\n",
      " |      numpy.max\n",
      " |      dask.array.max\n",
      " |      DataArray.max\n",
      " |      :ref:`agg`\n",
      " |          User guide on reduction or aggregation operations.\n",
      " |      \n",
      " |      Examples\n",
      " |      --------\n",
      " |      >>> da = xr.DataArray(\n",
      " |      ...     np.array([1, 2, 3, 1, 2, np.nan]),\n",
      " |      ...     dims=\"time\",\n",
      " |      ...     coords=dict(\n",
      " |      ...         time=(\"time\", pd.date_range(\"01-01-2001\", freq=\"M\", periods=6)),\n",
      " |      ...         labels=(\"time\", np.array([\"a\", \"b\", \"c\", \"c\", \"b\", \"a\"])),\n",
      " |      ...     ),\n",
      " |      ... )\n",
      " |      >>> ds = xr.Dataset(dict(da=da))\n",
      " |      >>> ds\n",
      " |      <xarray.Dataset>\n",
      " |      Dimensions:  (time: 6)\n",
      " |      Coordinates:\n",
      " |        * time     (time) datetime64[ns] 2001-01-31 2001-02-28 ... 2001-06-30\n",
      " |          labels   (time) <U1 'a' 'b' 'c' 'c' 'b' 'a'\n",
      " |      Data variables:\n",
      " |          da       (time) float64 1.0 2.0 3.0 1.0 2.0 nan\n",
      " |      \n",
      " |      >>> ds.max()\n",
      " |      <xarray.Dataset>\n",
      " |      Dimensions:  ()\n",
      " |      Data variables:\n",
      " |          da       float64 3.0\n",
      " |      \n",
      " |      Use ``skipna`` to control whether NaNs are ignored.\n",
      " |      \n",
      " |      >>> ds.max(skipna=False)\n",
      " |      <xarray.Dataset>\n",
      " |      Dimensions:  ()\n",
      " |      Data variables:\n",
      " |          da       float64 nan\n",
      " |  \n",
      " |  mean(self, dim: 'None | Hashable | Sequence[Hashable]' = None, *, skipna: 'bool | None' = None, keep_attrs: 'bool | None' = None, **kwargs: 'Any') -> 'Dataset'\n",
      " |      Reduce this Dataset's data by applying ``mean`` along some dimension(s).\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      dim : hashable or iterable of hashable, default: None\n",
      " |          Name of dimension[s] along which to apply ``mean``. For e.g. ``dim=\"x\"``\n",
      " |          or ``dim=[\"x\", \"y\"]``. If None, will reduce over all dimensions.\n",
      " |      skipna : bool or None, optional\n",
      " |          If True, skip missing values (as marked by NaN). By default, only\n",
      " |          skips missing values for float dtypes; other dtypes either do not\n",
      " |          have a sentinel missing value (int) or ``skipna=True`` has not been\n",
      " |          implemented (object, datetime64 or timedelta64).\n",
      " |      keep_attrs : bool or None, optional\n",
      " |          If True, ``attrs`` will be copied from the original\n",
      " |          object to the new one.  If False, the new object will be\n",
      " |          returned without attributes.\n",
      " |      **kwargs : Any\n",
      " |          Additional keyword arguments passed on to the appropriate array\n",
      " |          function for calculating ``mean`` on this object's data.\n",
      " |          These could include dask-specific kwargs like ``split_every``.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      reduced : Dataset\n",
      " |          New Dataset with ``mean`` applied to its data and the\n",
      " |          indicated dimension(s) removed\n",
      " |      \n",
      " |      See Also\n",
      " |      --------\n",
      " |      numpy.mean\n",
      " |      dask.array.mean\n",
      " |      DataArray.mean\n",
      " |      :ref:`agg`\n",
      " |          User guide on reduction or aggregation operations.\n",
      " |      \n",
      " |      Notes\n",
      " |      -----\n",
      " |      Non-numeric variables will be removed prior to reducing.\n",
      " |      \n",
      " |      Examples\n",
      " |      --------\n",
      " |      >>> da = xr.DataArray(\n",
      " |      ...     np.array([1, 2, 3, 1, 2, np.nan]),\n",
      " |      ...     dims=\"time\",\n",
      " |      ...     coords=dict(\n",
      " |      ...         time=(\"time\", pd.date_range(\"01-01-2001\", freq=\"M\", periods=6)),\n",
      " |      ...         labels=(\"time\", np.array([\"a\", \"b\", \"c\", \"c\", \"b\", \"a\"])),\n",
      " |      ...     ),\n",
      " |      ... )\n",
      " |      >>> ds = xr.Dataset(dict(da=da))\n",
      " |      >>> ds\n",
      " |      <xarray.Dataset>\n",
      " |      Dimensions:  (time: 6)\n",
      " |      Coordinates:\n",
      " |        * time     (time) datetime64[ns] 2001-01-31 2001-02-28 ... 2001-06-30\n",
      " |          labels   (time) <U1 'a' 'b' 'c' 'c' 'b' 'a'\n",
      " |      Data variables:\n",
      " |          da       (time) float64 1.0 2.0 3.0 1.0 2.0 nan\n",
      " |      \n",
      " |      >>> ds.mean()\n",
      " |      <xarray.Dataset>\n",
      " |      Dimensions:  ()\n",
      " |      Data variables:\n",
      " |          da       float64 1.8\n",
      " |      \n",
      " |      Use ``skipna`` to control whether NaNs are ignored.\n",
      " |      \n",
      " |      >>> ds.mean(skipna=False)\n",
      " |      <xarray.Dataset>\n",
      " |      Dimensions:  ()\n",
      " |      Data variables:\n",
      " |          da       float64 nan\n",
      " |  \n",
      " |  median(self, dim: 'None | Hashable | Sequence[Hashable]' = None, *, skipna: 'bool | None' = None, keep_attrs: 'bool | None' = None, **kwargs: 'Any') -> 'Dataset'\n",
      " |      Reduce this Dataset's data by applying ``median`` along some dimension(s).\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      dim : hashable or iterable of hashable, default: None\n",
      " |          Name of dimension[s] along which to apply ``median``. For e.g. ``dim=\"x\"``\n",
      " |          or ``dim=[\"x\", \"y\"]``. If None, will reduce over all dimensions.\n",
      " |      skipna : bool or None, optional\n",
      " |          If True, skip missing values (as marked by NaN). By default, only\n",
      " |          skips missing values for float dtypes; other dtypes either do not\n",
      " |          have a sentinel missing value (int) or ``skipna=True`` has not been\n",
      " |          implemented (object, datetime64 or timedelta64).\n",
      " |      keep_attrs : bool or None, optional\n",
      " |          If True, ``attrs`` will be copied from the original\n",
      " |          object to the new one.  If False, the new object will be\n",
      " |          returned without attributes.\n",
      " |      **kwargs : Any\n",
      " |          Additional keyword arguments passed on to the appropriate array\n",
      " |          function for calculating ``median`` on this object's data.\n",
      " |          These could include dask-specific kwargs like ``split_every``.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      reduced : Dataset\n",
      " |          New Dataset with ``median`` applied to its data and the\n",
      " |          indicated dimension(s) removed\n",
      " |      \n",
      " |      See Also\n",
      " |      --------\n",
      " |      numpy.median\n",
      " |      dask.array.median\n",
      " |      DataArray.median\n",
      " |      :ref:`agg`\n",
      " |          User guide on reduction or aggregation operations.\n",
      " |      \n",
      " |      Notes\n",
      " |      -----\n",
      " |      Non-numeric variables will be removed prior to reducing.\n",
      " |      \n",
      " |      Examples\n",
      " |      --------\n",
      " |      >>> da = xr.DataArray(\n",
      " |      ...     np.array([1, 2, 3, 1, 2, np.nan]),\n",
      " |      ...     dims=\"time\",\n",
      " |      ...     coords=dict(\n",
      " |      ...         time=(\"time\", pd.date_range(\"01-01-2001\", freq=\"M\", periods=6)),\n",
      " |      ...         labels=(\"time\", np.array([\"a\", \"b\", \"c\", \"c\", \"b\", \"a\"])),\n",
      " |      ...     ),\n",
      " |      ... )\n",
      " |      >>> ds = xr.Dataset(dict(da=da))\n",
      " |      >>> ds\n",
      " |      <xarray.Dataset>\n",
      " |      Dimensions:  (time: 6)\n",
      " |      Coordinates:\n",
      " |        * time     (time) datetime64[ns] 2001-01-31 2001-02-28 ... 2001-06-30\n",
      " |          labels   (time) <U1 'a' 'b' 'c' 'c' 'b' 'a'\n",
      " |      Data variables:\n",
      " |          da       (time) float64 1.0 2.0 3.0 1.0 2.0 nan\n",
      " |      \n",
      " |      >>> ds.median()\n",
      " |      <xarray.Dataset>\n",
      " |      Dimensions:  ()\n",
      " |      Data variables:\n",
      " |          da       float64 2.0\n",
      " |      \n",
      " |      Use ``skipna`` to control whether NaNs are ignored.\n",
      " |      \n",
      " |      >>> ds.median(skipna=False)\n",
      " |      <xarray.Dataset>\n",
      " |      Dimensions:  ()\n",
      " |      Data variables:\n",
      " |          da       float64 nan\n",
      " |  \n",
      " |  min(self, dim: 'None | Hashable | Sequence[Hashable]' = None, *, skipna: 'bool | None' = None, keep_attrs: 'bool | None' = None, **kwargs: 'Any') -> 'Dataset'\n",
      " |      Reduce this Dataset's data by applying ``min`` along some dimension(s).\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      dim : hashable or iterable of hashable, default: None\n",
      " |          Name of dimension[s] along which to apply ``min``. For e.g. ``dim=\"x\"``\n",
      " |          or ``dim=[\"x\", \"y\"]``. If None, will reduce over all dimensions.\n",
      " |      skipna : bool or None, optional\n",
      " |          If True, skip missing values (as marked by NaN). By default, only\n",
      " |          skips missing values for float dtypes; other dtypes either do not\n",
      " |          have a sentinel missing value (int) or ``skipna=True`` has not been\n",
      " |          implemented (object, datetime64 or timedelta64).\n",
      " |      keep_attrs : bool or None, optional\n",
      " |          If True, ``attrs`` will be copied from the original\n",
      " |          object to the new one.  If False, the new object will be\n",
      " |          returned without attributes.\n",
      " |      **kwargs : Any\n",
      " |          Additional keyword arguments passed on to the appropriate array\n",
      " |          function for calculating ``min`` on this object's data.\n",
      " |          These could include dask-specific kwargs like ``split_every``.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      reduced : Dataset\n",
      " |          New Dataset with ``min`` applied to its data and the\n",
      " |          indicated dimension(s) removed\n",
      " |      \n",
      " |      See Also\n",
      " |      --------\n",
      " |      numpy.min\n",
      " |      dask.array.min\n",
      " |      DataArray.min\n",
      " |      :ref:`agg`\n",
      " |          User guide on reduction or aggregation operations.\n",
      " |      \n",
      " |      Examples\n",
      " |      --------\n",
      " |      >>> da = xr.DataArray(\n",
      " |      ...     np.array([1, 2, 3, 1, 2, np.nan]),\n",
      " |      ...     dims=\"time\",\n",
      " |      ...     coords=dict(\n",
      " |      ...         time=(\"time\", pd.date_range(\"01-01-2001\", freq=\"M\", periods=6)),\n",
      " |      ...         labels=(\"time\", np.array([\"a\", \"b\", \"c\", \"c\", \"b\", \"a\"])),\n",
      " |      ...     ),\n",
      " |      ... )\n",
      " |      >>> ds = xr.Dataset(dict(da=da))\n",
      " |      >>> ds\n",
      " |      <xarray.Dataset>\n",
      " |      Dimensions:  (time: 6)\n",
      " |      Coordinates:\n",
      " |        * time     (time) datetime64[ns] 2001-01-31 2001-02-28 ... 2001-06-30\n",
      " |          labels   (time) <U1 'a' 'b' 'c' 'c' 'b' 'a'\n",
      " |      Data variables:\n",
      " |          da       (time) float64 1.0 2.0 3.0 1.0 2.0 nan\n",
      " |      \n",
      " |      >>> ds.min()\n",
      " |      <xarray.Dataset>\n",
      " |      Dimensions:  ()\n",
      " |      Data variables:\n",
      " |          da       float64 1.0\n",
      " |      \n",
      " |      Use ``skipna`` to control whether NaNs are ignored.\n",
      " |      \n",
      " |      >>> ds.min(skipna=False)\n",
      " |      <xarray.Dataset>\n",
      " |      Dimensions:  ()\n",
      " |      Data variables:\n",
      " |          da       float64 nan\n",
      " |  \n",
      " |  prod(self, dim: 'None | Hashable | Sequence[Hashable]' = None, *, skipna: 'bool | None' = None, min_count: 'int | None' = None, keep_attrs: 'bool | None' = None, **kwargs: 'Any') -> 'Dataset'\n",
      " |      Reduce this Dataset's data by applying ``prod`` along some dimension(s).\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      dim : hashable or iterable of hashable, default: None\n",
      " |          Name of dimension[s] along which to apply ``prod``. For e.g. ``dim=\"x\"``\n",
      " |          or ``dim=[\"x\", \"y\"]``. If None, will reduce over all dimensions.\n",
      " |      skipna : bool or None, optional\n",
      " |          If True, skip missing values (as marked by NaN). By default, only\n",
      " |          skips missing values for float dtypes; other dtypes either do not\n",
      " |          have a sentinel missing value (int) or ``skipna=True`` has not been\n",
      " |          implemented (object, datetime64 or timedelta64).\n",
      " |      min_count : int or None, optional\n",
      " |          The required number of valid values to perform the operation. If\n",
      " |          fewer than min_count non-NA values are present the result will be\n",
      " |          NA. Only used if skipna is set to True or defaults to True for the\n",
      " |          array's dtype. Changed in version 0.17.0: if specified on an integer\n",
      " |          array and skipna=True, the result will be a float array.\n",
      " |      keep_attrs : bool or None, optional\n",
      " |          If True, ``attrs`` will be copied from the original\n",
      " |          object to the new one.  If False, the new object will be\n",
      " |          returned without attributes.\n",
      " |      **kwargs : Any\n",
      " |          Additional keyword arguments passed on to the appropriate array\n",
      " |          function for calculating ``prod`` on this object's data.\n",
      " |          These could include dask-specific kwargs like ``split_every``.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      reduced : Dataset\n",
      " |          New Dataset with ``prod`` applied to its data and the\n",
      " |          indicated dimension(s) removed\n",
      " |      \n",
      " |      See Also\n",
      " |      --------\n",
      " |      numpy.prod\n",
      " |      dask.array.prod\n",
      " |      DataArray.prod\n",
      " |      :ref:`agg`\n",
      " |          User guide on reduction or aggregation operations.\n",
      " |      \n",
      " |      Notes\n",
      " |      -----\n",
      " |      Non-numeric variables will be removed prior to reducing.\n",
      " |      \n",
      " |      Examples\n",
      " |      --------\n",
      " |      >>> da = xr.DataArray(\n",
      " |      ...     np.array([1, 2, 3, 1, 2, np.nan]),\n",
      " |      ...     dims=\"time\",\n",
      " |      ...     coords=dict(\n",
      " |      ...         time=(\"time\", pd.date_range(\"01-01-2001\", freq=\"M\", periods=6)),\n",
      " |      ...         labels=(\"time\", np.array([\"a\", \"b\", \"c\", \"c\", \"b\", \"a\"])),\n",
      " |      ...     ),\n",
      " |      ... )\n",
      " |      >>> ds = xr.Dataset(dict(da=da))\n",
      " |      >>> ds\n",
      " |      <xarray.Dataset>\n",
      " |      Dimensions:  (time: 6)\n",
      " |      Coordinates:\n",
      " |        * time     (time) datetime64[ns] 2001-01-31 2001-02-28 ... 2001-06-30\n",
      " |          labels   (time) <U1 'a' 'b' 'c' 'c' 'b' 'a'\n",
      " |      Data variables:\n",
      " |          da       (time) float64 1.0 2.0 3.0 1.0 2.0 nan\n",
      " |      \n",
      " |      >>> ds.prod()\n",
      " |      <xarray.Dataset>\n",
      " |      Dimensions:  ()\n",
      " |      Data variables:\n",
      " |          da       float64 12.0\n",
      " |      \n",
      " |      Use ``skipna`` to control whether NaNs are ignored.\n",
      " |      \n",
      " |      >>> ds.prod(skipna=False)\n",
      " |      <xarray.Dataset>\n",
      " |      Dimensions:  ()\n",
      " |      Data variables:\n",
      " |          da       float64 nan\n",
      " |      \n",
      " |      Specify ``min_count`` for finer control over when NaNs are ignored.\n",
      " |      \n",
      " |      >>> ds.prod(skipna=True, min_count=2)\n",
      " |      <xarray.Dataset>\n",
      " |      Dimensions:  ()\n",
      " |      Data variables:\n",
      " |          da       float64 12.0\n",
      " |  \n",
      " |  std(self, dim: 'None | Hashable | Sequence[Hashable]' = None, *, skipna: 'bool | None' = None, ddof: 'int' = 0, keep_attrs: 'bool | None' = None, **kwargs: 'Any') -> 'Dataset'\n",
      " |      Reduce this Dataset's data by applying ``std`` along some dimension(s).\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      dim : hashable or iterable of hashable, default: None\n",
      " |          Name of dimension[s] along which to apply ``std``. For e.g. ``dim=\"x\"``\n",
      " |          or ``dim=[\"x\", \"y\"]``. If None, will reduce over all dimensions.\n",
      " |      skipna : bool or None, optional\n",
      " |          If True, skip missing values (as marked by NaN). By default, only\n",
      " |          skips missing values for float dtypes; other dtypes either do not\n",
      " |          have a sentinel missing value (int) or ``skipna=True`` has not been\n",
      " |          implemented (object, datetime64 or timedelta64).\n",
      " |      ddof : int, default: 0\n",
      " |          Delta Degrees of Freedom: the divisor used in the calculation is ``N - ddof``,\n",
      " |          where ``N`` represents the number of elements.\n",
      " |      keep_attrs : bool or None, optional\n",
      " |          If True, ``attrs`` will be copied from the original\n",
      " |          object to the new one.  If False, the new object will be\n",
      " |          returned without attributes.\n",
      " |      **kwargs : Any\n",
      " |          Additional keyword arguments passed on to the appropriate array\n",
      " |          function for calculating ``std`` on this object's data.\n",
      " |          These could include dask-specific kwargs like ``split_every``.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      reduced : Dataset\n",
      " |          New Dataset with ``std`` applied to its data and the\n",
      " |          indicated dimension(s) removed\n",
      " |      \n",
      " |      See Also\n",
      " |      --------\n",
      " |      numpy.std\n",
      " |      dask.array.std\n",
      " |      DataArray.std\n",
      " |      :ref:`agg`\n",
      " |          User guide on reduction or aggregation operations.\n",
      " |      \n",
      " |      Notes\n",
      " |      -----\n",
      " |      Non-numeric variables will be removed prior to reducing.\n",
      " |      \n",
      " |      Examples\n",
      " |      --------\n",
      " |      >>> da = xr.DataArray(\n",
      " |      ...     np.array([1, 2, 3, 1, 2, np.nan]),\n",
      " |      ...     dims=\"time\",\n",
      " |      ...     coords=dict(\n",
      " |      ...         time=(\"time\", pd.date_range(\"01-01-2001\", freq=\"M\", periods=6)),\n",
      " |      ...         labels=(\"time\", np.array([\"a\", \"b\", \"c\", \"c\", \"b\", \"a\"])),\n",
      " |      ...     ),\n",
      " |      ... )\n",
      " |      >>> ds = xr.Dataset(dict(da=da))\n",
      " |      >>> ds\n",
      " |      <xarray.Dataset>\n",
      " |      Dimensions:  (time: 6)\n",
      " |      Coordinates:\n",
      " |        * time     (time) datetime64[ns] 2001-01-31 2001-02-28 ... 2001-06-30\n",
      " |          labels   (time) <U1 'a' 'b' 'c' 'c' 'b' 'a'\n",
      " |      Data variables:\n",
      " |          da       (time) float64 1.0 2.0 3.0 1.0 2.0 nan\n",
      " |      \n",
      " |      >>> ds.std()\n",
      " |      <xarray.Dataset>\n",
      " |      Dimensions:  ()\n",
      " |      Data variables:\n",
      " |          da       float64 0.7483\n",
      " |      \n",
      " |      Use ``skipna`` to control whether NaNs are ignored.\n",
      " |      \n",
      " |      >>> ds.std(skipna=False)\n",
      " |      <xarray.Dataset>\n",
      " |      Dimensions:  ()\n",
      " |      Data variables:\n",
      " |          da       float64 nan\n",
      " |      \n",
      " |      Specify ``ddof=1`` for an unbiased estimate.\n",
      " |      \n",
      " |      >>> ds.std(skipna=True, ddof=1)\n",
      " |      <xarray.Dataset>\n",
      " |      Dimensions:  ()\n",
      " |      Data variables:\n",
      " |          da       float64 0.8367\n",
      " |  \n",
      " |  sum(self, dim: 'None | Hashable | Sequence[Hashable]' = None, *, skipna: 'bool | None' = None, min_count: 'int | None' = None, keep_attrs: 'bool | None' = None, **kwargs: 'Any') -> 'Dataset'\n",
      " |      Reduce this Dataset's data by applying ``sum`` along some dimension(s).\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      dim : hashable or iterable of hashable, default: None\n",
      " |          Name of dimension[s] along which to apply ``sum``. For e.g. ``dim=\"x\"``\n",
      " |          or ``dim=[\"x\", \"y\"]``. If None, will reduce over all dimensions.\n",
      " |      skipna : bool or None, optional\n",
      " |          If True, skip missing values (as marked by NaN). By default, only\n",
      " |          skips missing values for float dtypes; other dtypes either do not\n",
      " |          have a sentinel missing value (int) or ``skipna=True`` has not been\n",
      " |          implemented (object, datetime64 or timedelta64).\n",
      " |      min_count : int or None, optional\n",
      " |          The required number of valid values to perform the operation. If\n",
      " |          fewer than min_count non-NA values are present the result will be\n",
      " |          NA. Only used if skipna is set to True or defaults to True for the\n",
      " |          array's dtype. Changed in version 0.17.0: if specified on an integer\n",
      " |          array and skipna=True, the result will be a float array.\n",
      " |      keep_attrs : bool or None, optional\n",
      " |          If True, ``attrs`` will be copied from the original\n",
      " |          object to the new one.  If False, the new object will be\n",
      " |          returned without attributes.\n",
      " |      **kwargs : Any\n",
      " |          Additional keyword arguments passed on to the appropriate array\n",
      " |          function for calculating ``sum`` on this object's data.\n",
      " |          These could include dask-specific kwargs like ``split_every``.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      reduced : Dataset\n",
      " |          New Dataset with ``sum`` applied to its data and the\n",
      " |          indicated dimension(s) removed\n",
      " |      \n",
      " |      See Also\n",
      " |      --------\n",
      " |      numpy.sum\n",
      " |      dask.array.sum\n",
      " |      DataArray.sum\n",
      " |      :ref:`agg`\n",
      " |          User guide on reduction or aggregation operations.\n",
      " |      \n",
      " |      Notes\n",
      " |      -----\n",
      " |      Non-numeric variables will be removed prior to reducing.\n",
      " |      \n",
      " |      Examples\n",
      " |      --------\n",
      " |      >>> da = xr.DataArray(\n",
      " |      ...     np.array([1, 2, 3, 1, 2, np.nan]),\n",
      " |      ...     dims=\"time\",\n",
      " |      ...     coords=dict(\n",
      " |      ...         time=(\"time\", pd.date_range(\"01-01-2001\", freq=\"M\", periods=6)),\n",
      " |      ...         labels=(\"time\", np.array([\"a\", \"b\", \"c\", \"c\", \"b\", \"a\"])),\n",
      " |      ...     ),\n",
      " |      ... )\n",
      " |      >>> ds = xr.Dataset(dict(da=da))\n",
      " |      >>> ds\n",
      " |      <xarray.Dataset>\n",
      " |      Dimensions:  (time: 6)\n",
      " |      Coordinates:\n",
      " |        * time     (time) datetime64[ns] 2001-01-31 2001-02-28 ... 2001-06-30\n",
      " |          labels   (time) <U1 'a' 'b' 'c' 'c' 'b' 'a'\n",
      " |      Data variables:\n",
      " |          da       (time) float64 1.0 2.0 3.0 1.0 2.0 nan\n",
      " |      \n",
      " |      >>> ds.sum()\n",
      " |      <xarray.Dataset>\n",
      " |      Dimensions:  ()\n",
      " |      Data variables:\n",
      " |          da       float64 9.0\n",
      " |      \n",
      " |      Use ``skipna`` to control whether NaNs are ignored.\n",
      " |      \n",
      " |      >>> ds.sum(skipna=False)\n",
      " |      <xarray.Dataset>\n",
      " |      Dimensions:  ()\n",
      " |      Data variables:\n",
      " |          da       float64 nan\n",
      " |      \n",
      " |      Specify ``min_count`` for finer control over when NaNs are ignored.\n",
      " |      \n",
      " |      >>> ds.sum(skipna=True, min_count=2)\n",
      " |      <xarray.Dataset>\n",
      " |      Dimensions:  ()\n",
      " |      Data variables:\n",
      " |          da       float64 9.0\n",
      " |  \n",
      " |  var(self, dim: 'None | Hashable | Sequence[Hashable]' = None, *, skipna: 'bool | None' = None, ddof: 'int' = 0, keep_attrs: 'bool | None' = None, **kwargs: 'Any') -> 'Dataset'\n",
      " |      Reduce this Dataset's data by applying ``var`` along some dimension(s).\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      dim : hashable or iterable of hashable, default: None\n",
      " |          Name of dimension[s] along which to apply ``var``. For e.g. ``dim=\"x\"``\n",
      " |          or ``dim=[\"x\", \"y\"]``. If None, will reduce over all dimensions.\n",
      " |      skipna : bool or None, optional\n",
      " |          If True, skip missing values (as marked by NaN). By default, only\n",
      " |          skips missing values for float dtypes; other dtypes either do not\n",
      " |          have a sentinel missing value (int) or ``skipna=True`` has not been\n",
      " |          implemented (object, datetime64 or timedelta64).\n",
      " |      ddof : int, default: 0\n",
      " |          Delta Degrees of Freedom: the divisor used in the calculation is ``N - ddof``,\n",
      " |          where ``N`` represents the number of elements.\n",
      " |      keep_attrs : bool or None, optional\n",
      " |          If True, ``attrs`` will be copied from the original\n",
      " |          object to the new one.  If False, the new object will be\n",
      " |          returned without attributes.\n",
      " |      **kwargs : Any\n",
      " |          Additional keyword arguments passed on to the appropriate array\n",
      " |          function for calculating ``var`` on this object's data.\n",
      " |          These could include dask-specific kwargs like ``split_every``.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      reduced : Dataset\n",
      " |          New Dataset with ``var`` applied to its data and the\n",
      " |          indicated dimension(s) removed\n",
      " |      \n",
      " |      See Also\n",
      " |      --------\n",
      " |      numpy.var\n",
      " |      dask.array.var\n",
      " |      DataArray.var\n",
      " |      :ref:`agg`\n",
      " |          User guide on reduction or aggregation operations.\n",
      " |      \n",
      " |      Notes\n",
      " |      -----\n",
      " |      Non-numeric variables will be removed prior to reducing.\n",
      " |      \n",
      " |      Examples\n",
      " |      --------\n",
      " |      >>> da = xr.DataArray(\n",
      " |      ...     np.array([1, 2, 3, 1, 2, np.nan]),\n",
      " |      ...     dims=\"time\",\n",
      " |      ...     coords=dict(\n",
      " |      ...         time=(\"time\", pd.date_range(\"01-01-2001\", freq=\"M\", periods=6)),\n",
      " |      ...         labels=(\"time\", np.array([\"a\", \"b\", \"c\", \"c\", \"b\", \"a\"])),\n",
      " |      ...     ),\n",
      " |      ... )\n",
      " |      >>> ds = xr.Dataset(dict(da=da))\n",
      " |      >>> ds\n",
      " |      <xarray.Dataset>\n",
      " |      Dimensions:  (time: 6)\n",
      " |      Coordinates:\n",
      " |        * time     (time) datetime64[ns] 2001-01-31 2001-02-28 ... 2001-06-30\n",
      " |          labels   (time) <U1 'a' 'b' 'c' 'c' 'b' 'a'\n",
      " |      Data variables:\n",
      " |          da       (time) float64 1.0 2.0 3.0 1.0 2.0 nan\n",
      " |      \n",
      " |      >>> ds.var()\n",
      " |      <xarray.Dataset>\n",
      " |      Dimensions:  ()\n",
      " |      Data variables:\n",
      " |          da       float64 0.56\n",
      " |      \n",
      " |      Use ``skipna`` to control whether NaNs are ignored.\n",
      " |      \n",
      " |      >>> ds.var(skipna=False)\n",
      " |      <xarray.Dataset>\n",
      " |      Dimensions:  ()\n",
      " |      Data variables:\n",
      " |          da       float64 nan\n",
      " |      \n",
      " |      Specify ``ddof=1`` for an unbiased estimate.\n",
      " |      \n",
      " |      >>> ds.var(skipna=True, ddof=1)\n",
      " |      <xarray.Dataset>\n",
      " |      Dimensions:  ()\n",
      " |      Data variables:\n",
      " |          da       float64 0.7\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Data and other attributes inherited from xarray.core.arithmetic.DatasetArithmetic:\n",
      " |  \n",
      " |  __array_priority__ = 50\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Methods inherited from xarray.core.arithmetic.SupportsArithmetic:\n",
      " |  \n",
      " |  __array_ufunc__(self, ufunc, method, *inputs, **kwargs)\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Methods inherited from xarray.core._typed_ops.DatasetOpsMixin:\n",
      " |  \n",
      " |  __abs__(self)\n",
      " |      Same as abs(a).\n",
      " |  \n",
      " |  __add__(self, other)\n",
      " |      Same as a + b.\n",
      " |  \n",
      " |  __and__(self, other)\n",
      " |      Same as a & b.\n",
      " |  \n",
      " |  __eq__(self, other)\n",
      " |      Return self==value.\n",
      " |  \n",
      " |  __floordiv__(self, other)\n",
      " |      Same as a // b.\n",
      " |  \n",
      " |  __ge__(self, other)\n",
      " |      Same as a >= b.\n",
      " |  \n",
      " |  __gt__(self, other)\n",
      " |      Same as a > b.\n",
      " |  \n",
      " |  __iadd__(self, other)\n",
      " |      Same as a += b.\n",
      " |  \n",
      " |  __iand__(self, other)\n",
      " |      Same as a &= b.\n",
      " |  \n",
      " |  __ifloordiv__(self, other)\n",
      " |      Same as a //= b.\n",
      " |  \n",
      " |  __imod__(self, other)\n",
      " |      Same as a %= b.\n",
      " |  \n",
      " |  __imul__(self, other)\n",
      " |      Same as a *= b.\n",
      " |  \n",
      " |  __invert__(self)\n",
      " |      Same as ~a.\n",
      " |  \n",
      " |  __ior__(self, other)\n",
      " |      Same as a |= b.\n",
      " |  \n",
      " |  __ipow__(self, other)\n",
      " |      Same as a **= b.\n",
      " |  \n",
      " |  __isub__(self, other)\n",
      " |      Same as a -= b.\n",
      " |  \n",
      " |  __itruediv__(self, other)\n",
      " |      Same as a /= b.\n",
      " |  \n",
      " |  __ixor__(self, other)\n",
      " |      Same as a ^= b.\n",
      " |  \n",
      " |  __le__(self, other)\n",
      " |      Same as a <= b.\n",
      " |  \n",
      " |  __lt__(self, other)\n",
      " |      Same as a < b.\n",
      " |  \n",
      " |  __mod__(self, other)\n",
      " |      Same as a % b.\n",
      " |  \n",
      " |  __mul__(self, other)\n",
      " |      Same as a * b.\n",
      " |  \n",
      " |  __ne__(self, other)\n",
      " |      Return self!=value.\n",
      " |  \n",
      " |  __neg__(self)\n",
      " |      Same as -a.\n",
      " |  \n",
      " |  __or__(self, other)\n",
      " |      Same as a | b.\n",
      " |  \n",
      " |  __pos__(self)\n",
      " |      Same as +a.\n",
      " |  \n",
      " |  __pow__(self, other)\n",
      " |      Same as a ** b.\n",
      " |  \n",
      " |  __radd__(self, other)\n",
      " |      Same as a + b.\n",
      " |  \n",
      " |  __rand__(self, other)\n",
      " |      Same as a & b.\n",
      " |  \n",
      " |  __rfloordiv__(self, other)\n",
      " |      Same as a // b.\n",
      " |  \n",
      " |  __rmod__(self, other)\n",
      " |      Same as a % b.\n",
      " |  \n",
      " |  __rmul__(self, other)\n",
      " |      Same as a * b.\n",
      " |  \n",
      " |  __ror__(self, other)\n",
      " |      Same as a | b.\n",
      " |  \n",
      " |  __rpow__(self, other)\n",
      " |      Same as a ** b.\n",
      " |  \n",
      " |  __rsub__(self, other)\n",
      " |      Same as a - b.\n",
      " |  \n",
      " |  __rtruediv__(self, other)\n",
      " |      Same as a / b.\n",
      " |  \n",
      " |  __rxor__(self, other)\n",
      " |      Same as a ^ b.\n",
      " |  \n",
      " |  __sub__(self, other)\n",
      " |      Same as a - b.\n",
      " |  \n",
      " |  __truediv__(self, other)\n",
      " |      Same as a / b.\n",
      " |  \n",
      " |  __xor__(self, other)\n",
      " |      Same as a ^ b.\n",
      " |  \n",
      " |  argsort(self, *args, **kwargs)\n",
      " |      a.argsort(axis=-1, kind=None, order=None)\n",
      " |      \n",
      " |      Returns the indices that would sort this array.\n",
      " |      \n",
      " |      Refer to `numpy.argsort` for full documentation.\n",
      " |      \n",
      " |      See Also\n",
      " |      --------\n",
      " |      numpy.argsort : equivalent function\n",
      " |  \n",
      " |  conj(self, *args, **kwargs)\n",
      " |      a.conj()\n",
      " |      \n",
      " |      Complex-conjugate all elements.\n",
      " |      \n",
      " |      Refer to `numpy.conjugate` for full documentation.\n",
      " |      \n",
      " |      See Also\n",
      " |      --------\n",
      " |      numpy.conjugate : equivalent function\n",
      " |  \n",
      " |  conjugate(self, *args, **kwargs)\n",
      " |      a.conjugate()\n",
      " |      \n",
      " |      Return the complex conjugate, element-wise.\n",
      " |      \n",
      " |      Refer to `numpy.conjugate` for full documentation.\n",
      " |      \n",
      " |      See Also\n",
      " |      --------\n",
      " |      numpy.conjugate : equivalent function\n",
      " |  \n",
      " |  round(self, *args, **kwargs)\n",
      " |      Evenly round to the given number of decimals.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      a : array_like\n",
      " |          Input data.\n",
      " |      decimals : int, optional\n",
      " |          Number of decimal places to round to (default: 0).  If\n",
      " |          decimals is negative, it specifies the number of positions to\n",
      " |          the left of the decimal point.\n",
      " |      out : ndarray, optional\n",
      " |          Alternative output array in which to place the result. It must have\n",
      " |          the same shape as the expected output, but the type of the output\n",
      " |          values will be cast if necessary. See :ref:`ufuncs-output-type` for more\n",
      " |          details.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      rounded_array : ndarray\n",
      " |          An array of the same type as `a`, containing the rounded values.\n",
      " |          Unless `out` was specified, a new array is created.  A reference to\n",
      " |          the result is returned.\n",
      " |      \n",
      " |          The real and imaginary parts of complex numbers are rounded\n",
      " |          separately.  The result of rounding a float is a float.\n",
      " |      \n",
      " |      See Also\n",
      " |      --------\n",
      " |      ndarray.round : equivalent method\n",
      " |      \n",
      " |      ceil, fix, floor, rint, trunc\n",
      " |      \n",
      " |      \n",
      " |      Notes\n",
      " |      -----\n",
      " |      For values exactly halfway between rounded decimal values, NumPy\n",
      " |      rounds to the nearest even value. Thus 1.5 and 2.5 round to 2.0,\n",
      " |      -0.5 and 0.5 round to 0.0, etc.\n",
      " |      \n",
      " |      ``np.around`` uses a fast but sometimes inexact algorithm to round\n",
      " |      floating-point datatypes. For positive `decimals` it is equivalent to\n",
      " |      ``np.true_divide(np.rint(a * 10**decimals), 10**decimals)``, which has\n",
      " |      error due to the inexact representation of decimal fractions in the IEEE\n",
      " |      floating point standard [1]_ and errors introduced when scaling by powers\n",
      " |      of ten. For instance, note the extra \"1\" in the following:\n",
      " |      \n",
      " |          >>> np.round(56294995342131.5, 3)\n",
      " |          56294995342131.51\n",
      " |      \n",
      " |      If your goal is to print such values with a fixed number of decimals, it is\n",
      " |      preferable to use numpy's float printing routines to limit the number of\n",
      " |      printed decimals:\n",
      " |      \n",
      " |          >>> np.format_float_positional(56294995342131.5, precision=3)\n",
      " |          '56294995342131.5'\n",
      " |      \n",
      " |      The float printing routines use an accurate but much more computationally\n",
      " |      demanding algorithm to compute the number of digits after the decimal\n",
      " |      point.\n",
      " |      \n",
      " |      Alternatively, Python's builtin `round` function uses a more accurate\n",
      " |      but slower algorithm for 64-bit floating point values:\n",
      " |      \n",
      " |          >>> round(56294995342131.5, 3)\n",
      " |          56294995342131.5\n",
      " |          >>> np.round(16.055, 2), round(16.055, 2)  # equals 16.0549999999999997\n",
      " |          (16.06, 16.05)\n",
      " |      \n",
      " |      \n",
      " |      References\n",
      " |      ----------\n",
      " |      .. [1] \"Lecture Notes on the Status of IEEE 754\", William Kahan,\n",
      " |             https://people.eecs.berkeley.edu/~wkahan/ieee754status/IEEE754.PDF\n",
      " |      \n",
      " |      Examples\n",
      " |      --------\n",
      " |      >>> np.around([0.37, 1.64])\n",
      " |      array([0., 2.])\n",
      " |      >>> np.around([0.37, 1.64], decimals=1)\n",
      " |      array([0.4, 1.6])\n",
      " |      >>> np.around([.5, 1.5, 2.5, 3.5, 4.5]) # rounds to nearest even value\n",
      " |      array([0., 2., 2., 4., 4.])\n",
      " |      >>> np.around([1,2,3,11], decimals=1) # ndarray of ints is returned\n",
      " |      array([ 1,  2,  3, 11])\n",
      " |      >>> np.around([1,2,3,11], decimals=-1)\n",
      " |      array([ 0,  0,  0, 10])\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Methods inherited from collections.abc.Mapping:\n",
      " |  \n",
      " |  get(self, key, default=None)\n",
      " |      D.get(k[,d]) -> D[k] if k in D, else d.  d defaults to None.\n",
      " |  \n",
      " |  items(self)\n",
      " |      D.items() -> a set-like object providing a view on D's items\n",
      " |  \n",
      " |  keys(self)\n",
      " |      D.keys() -> a set-like object providing a view on D's keys\n",
      " |  \n",
      " |  values(self)\n",
      " |      D.values() -> an object providing a view on D's values\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Data and other attributes inherited from collections.abc.Mapping:\n",
      " |  \n",
      " |  __reversed__ = None\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Class methods inherited from collections.abc.Collection:\n",
      " |  \n",
      " |  __subclasshook__(C) from abc.ABCMeta\n",
      " |      Abstract classes can override this to customize issubclass().\n",
      " |      \n",
      " |      This is invoked early on by abc.ABCMeta.__subclasscheck__().\n",
      " |      It should return True, False or NotImplemented.  If it returns\n",
      " |      NotImplemented, the normal algorithm is used.  Otherwise, it\n",
      " |      overrides the normal algorithm (and the outcome is cached).\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Class methods inherited from collections.abc.Iterable:\n",
      " |  \n",
      " |  __class_getitem__ = GenericAlias(...) from abc.ABCMeta\n",
      " |      Represent a PEP 585 generic type\n",
      " |      \n",
      " |      E.g. for t = list[int], t.__origin__ is list and t.__args__ is (int,).\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "help(caliop_raster)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "cf6e44d1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div><svg style=\"position: absolute; width: 0; height: 0; overflow: hidden\">\n",
       "<defs>\n",
       "<symbol id=\"icon-database\" viewBox=\"0 0 32 32\">\n",
       "<path d=\"M16 0c-8.837 0-16 2.239-16 5v4c0 2.761 7.163 5 16 5s16-2.239 16-5v-4c0-2.761-7.163-5-16-5z\"></path>\n",
       "<path d=\"M16 17c-8.837 0-16-2.239-16-5v6c0 2.761 7.163 5 16 5s16-2.239 16-5v-6c0 2.761-7.163 5-16 5z\"></path>\n",
       "<path d=\"M16 26c-8.837 0-16-2.239-16-5v6c0 2.761 7.163 5 16 5s16-2.239 16-5v-6c0 2.761-7.163 5-16 5z\"></path>\n",
       "</symbol>\n",
       "<symbol id=\"icon-file-text2\" viewBox=\"0 0 32 32\">\n",
       "<path d=\"M28.681 7.159c-0.694-0.947-1.662-2.053-2.724-3.116s-2.169-2.030-3.116-2.724c-1.612-1.182-2.393-1.319-2.841-1.319h-15.5c-1.378 0-2.5 1.121-2.5 2.5v27c0 1.378 1.122 2.5 2.5 2.5h23c1.378 0 2.5-1.122 2.5-2.5v-19.5c0-0.448-0.137-1.23-1.319-2.841zM24.543 5.457c0.959 0.959 1.712 1.825 2.268 2.543h-4.811v-4.811c0.718 0.556 1.584 1.309 2.543 2.268zM28 29.5c0 0.271-0.229 0.5-0.5 0.5h-23c-0.271 0-0.5-0.229-0.5-0.5v-27c0-0.271 0.229-0.5 0.5-0.5 0 0 15.499-0 15.5 0v7c0 0.552 0.448 1 1 1h7v19.5z\"></path>\n",
       "<path d=\"M23 26h-14c-0.552 0-1-0.448-1-1s0.448-1 1-1h14c0.552 0 1 0.448 1 1s-0.448 1-1 1z\"></path>\n",
       "<path d=\"M23 22h-14c-0.552 0-1-0.448-1-1s0.448-1 1-1h14c0.552 0 1 0.448 1 1s-0.448 1-1 1z\"></path>\n",
       "<path d=\"M23 18h-14c-0.552 0-1-0.448-1-1s0.448-1 1-1h14c0.552 0 1 0.448 1 1s-0.448 1-1 1z\"></path>\n",
       "</symbol>\n",
       "</defs>\n",
       "</svg>\n",
       "<style>/* CSS stylesheet for displaying xarray objects in jupyterlab.\n",
       " *\n",
       " */\n",
       "\n",
       ":root {\n",
       "  --xr-font-color0: var(--jp-content-font-color0, rgba(0, 0, 0, 1));\n",
       "  --xr-font-color2: var(--jp-content-font-color2, rgba(0, 0, 0, 0.54));\n",
       "  --xr-font-color3: var(--jp-content-font-color3, rgba(0, 0, 0, 0.38));\n",
       "  --xr-border-color: var(--jp-border-color2, #e0e0e0);\n",
       "  --xr-disabled-color: var(--jp-layout-color3, #bdbdbd);\n",
       "  --xr-background-color: var(--jp-layout-color0, white);\n",
       "  --xr-background-color-row-even: var(--jp-layout-color1, white);\n",
       "  --xr-background-color-row-odd: var(--jp-layout-color2, #eeeeee);\n",
       "}\n",
       "\n",
       "html[theme=dark],\n",
       "body[data-theme=dark],\n",
       "body.vscode-dark {\n",
       "  --xr-font-color0: rgba(255, 255, 255, 1);\n",
       "  --xr-font-color2: rgba(255, 255, 255, 0.54);\n",
       "  --xr-font-color3: rgba(255, 255, 255, 0.38);\n",
       "  --xr-border-color: #1F1F1F;\n",
       "  --xr-disabled-color: #515151;\n",
       "  --xr-background-color: #111111;\n",
       "  --xr-background-color-row-even: #111111;\n",
       "  --xr-background-color-row-odd: #313131;\n",
       "}\n",
       "\n",
       ".xr-wrap {\n",
       "  display: block !important;\n",
       "  min-width: 300px;\n",
       "  max-width: 700px;\n",
       "}\n",
       "\n",
       ".xr-text-repr-fallback {\n",
       "  /* fallback to plain text repr when CSS is not injected (untrusted notebook) */\n",
       "  display: none;\n",
       "}\n",
       "\n",
       ".xr-header {\n",
       "  padding-top: 6px;\n",
       "  padding-bottom: 6px;\n",
       "  margin-bottom: 4px;\n",
       "  border-bottom: solid 1px var(--xr-border-color);\n",
       "}\n",
       "\n",
       ".xr-header > div,\n",
       ".xr-header > ul {\n",
       "  display: inline;\n",
       "  margin-top: 0;\n",
       "  margin-bottom: 0;\n",
       "}\n",
       "\n",
       ".xr-obj-type,\n",
       ".xr-array-name {\n",
       "  margin-left: 2px;\n",
       "  margin-right: 10px;\n",
       "}\n",
       "\n",
       ".xr-obj-type {\n",
       "  color: var(--xr-font-color2);\n",
       "}\n",
       "\n",
       ".xr-sections {\n",
       "  padding-left: 0 !important;\n",
       "  display: grid;\n",
       "  grid-template-columns: 150px auto auto 1fr 20px 20px;\n",
       "}\n",
       "\n",
       ".xr-section-item {\n",
       "  display: contents;\n",
       "}\n",
       "\n",
       ".xr-section-item input {\n",
       "  display: none;\n",
       "}\n",
       "\n",
       ".xr-section-item input + label {\n",
       "  color: var(--xr-disabled-color);\n",
       "}\n",
       "\n",
       ".xr-section-item input:enabled + label {\n",
       "  cursor: pointer;\n",
       "  color: var(--xr-font-color2);\n",
       "}\n",
       "\n",
       ".xr-section-item input:enabled + label:hover {\n",
       "  color: var(--xr-font-color0);\n",
       "}\n",
       "\n",
       ".xr-section-summary {\n",
       "  grid-column: 1;\n",
       "  color: var(--xr-font-color2);\n",
       "  font-weight: 500;\n",
       "}\n",
       "\n",
       ".xr-section-summary > span {\n",
       "  display: inline-block;\n",
       "  padding-left: 0.5em;\n",
       "}\n",
       "\n",
       ".xr-section-summary-in:disabled + label {\n",
       "  color: var(--xr-font-color2);\n",
       "}\n",
       "\n",
       ".xr-section-summary-in + label:before {\n",
       "  display: inline-block;\n",
       "  content: '';\n",
       "  font-size: 11px;\n",
       "  width: 15px;\n",
       "  text-align: center;\n",
       "}\n",
       "\n",
       ".xr-section-summary-in:disabled + label:before {\n",
       "  color: var(--xr-disabled-color);\n",
       "}\n",
       "\n",
       ".xr-section-summary-in:checked + label:before {\n",
       "  content: '';\n",
       "}\n",
       "\n",
       ".xr-section-summary-in:checked + label > span {\n",
       "  display: none;\n",
       "}\n",
       "\n",
       ".xr-section-summary,\n",
       ".xr-section-inline-details {\n",
       "  padding-top: 4px;\n",
       "  padding-bottom: 4px;\n",
       "}\n",
       "\n",
       ".xr-section-inline-details {\n",
       "  grid-column: 2 / -1;\n",
       "}\n",
       "\n",
       ".xr-section-details {\n",
       "  display: none;\n",
       "  grid-column: 1 / -1;\n",
       "  margin-bottom: 5px;\n",
       "}\n",
       "\n",
       ".xr-section-summary-in:checked ~ .xr-section-details {\n",
       "  display: contents;\n",
       "}\n",
       "\n",
       ".xr-array-wrap {\n",
       "  grid-column: 1 / -1;\n",
       "  display: grid;\n",
       "  grid-template-columns: 20px auto;\n",
       "}\n",
       "\n",
       ".xr-array-wrap > label {\n",
       "  grid-column: 1;\n",
       "  vertical-align: top;\n",
       "}\n",
       "\n",
       ".xr-preview {\n",
       "  color: var(--xr-font-color3);\n",
       "}\n",
       "\n",
       ".xr-array-preview,\n",
       ".xr-array-data {\n",
       "  padding: 0 5px !important;\n",
       "  grid-column: 2;\n",
       "}\n",
       "\n",
       ".xr-array-data,\n",
       ".xr-array-in:checked ~ .xr-array-preview {\n",
       "  display: none;\n",
       "}\n",
       "\n",
       ".xr-array-in:checked ~ .xr-array-data,\n",
       ".xr-array-preview {\n",
       "  display: inline-block;\n",
       "}\n",
       "\n",
       ".xr-dim-list {\n",
       "  display: inline-block !important;\n",
       "  list-style: none;\n",
       "  padding: 0 !important;\n",
       "  margin: 0;\n",
       "}\n",
       "\n",
       ".xr-dim-list li {\n",
       "  display: inline-block;\n",
       "  padding: 0;\n",
       "  margin: 0;\n",
       "}\n",
       "\n",
       ".xr-dim-list:before {\n",
       "  content: '(';\n",
       "}\n",
       "\n",
       ".xr-dim-list:after {\n",
       "  content: ')';\n",
       "}\n",
       "\n",
       ".xr-dim-list li:not(:last-child):after {\n",
       "  content: ',';\n",
       "  padding-right: 5px;\n",
       "}\n",
       "\n",
       ".xr-has-index {\n",
       "  font-weight: bold;\n",
       "}\n",
       "\n",
       ".xr-var-list,\n",
       ".xr-var-item {\n",
       "  display: contents;\n",
       "}\n",
       "\n",
       ".xr-var-item > div,\n",
       ".xr-var-item label,\n",
       ".xr-var-item > .xr-var-name span {\n",
       "  background-color: var(--xr-background-color-row-even);\n",
       "  margin-bottom: 0;\n",
       "}\n",
       "\n",
       ".xr-var-item > .xr-var-name:hover span {\n",
       "  padding-right: 5px;\n",
       "}\n",
       "\n",
       ".xr-var-list > li:nth-child(odd) > div,\n",
       ".xr-var-list > li:nth-child(odd) > label,\n",
       ".xr-var-list > li:nth-child(odd) > .xr-var-name span {\n",
       "  background-color: var(--xr-background-color-row-odd);\n",
       "}\n",
       "\n",
       ".xr-var-name {\n",
       "  grid-column: 1;\n",
       "}\n",
       "\n",
       ".xr-var-dims {\n",
       "  grid-column: 2;\n",
       "}\n",
       "\n",
       ".xr-var-dtype {\n",
       "  grid-column: 3;\n",
       "  text-align: right;\n",
       "  color: var(--xr-font-color2);\n",
       "}\n",
       "\n",
       ".xr-var-preview {\n",
       "  grid-column: 4;\n",
       "}\n",
       "\n",
       ".xr-var-name,\n",
       ".xr-var-dims,\n",
       ".xr-var-dtype,\n",
       ".xr-preview,\n",
       ".xr-attrs dt {\n",
       "  white-space: nowrap;\n",
       "  overflow: hidden;\n",
       "  text-overflow: ellipsis;\n",
       "  padding-right: 10px;\n",
       "}\n",
       "\n",
       ".xr-var-name:hover,\n",
       ".xr-var-dims:hover,\n",
       ".xr-var-dtype:hover,\n",
       ".xr-attrs dt:hover {\n",
       "  overflow: visible;\n",
       "  width: auto;\n",
       "  z-index: 1;\n",
       "}\n",
       "\n",
       ".xr-var-attrs,\n",
       ".xr-var-data {\n",
       "  display: none;\n",
       "  background-color: var(--xr-background-color) !important;\n",
       "  padding-bottom: 5px !important;\n",
       "}\n",
       "\n",
       ".xr-var-attrs-in:checked ~ .xr-var-attrs,\n",
       ".xr-var-data-in:checked ~ .xr-var-data {\n",
       "  display: block;\n",
       "}\n",
       "\n",
       ".xr-var-data > table {\n",
       "  float: right;\n",
       "}\n",
       "\n",
       ".xr-var-name span,\n",
       ".xr-var-data,\n",
       ".xr-attrs {\n",
       "  padding-left: 25px !important;\n",
       "}\n",
       "\n",
       ".xr-attrs,\n",
       ".xr-var-attrs,\n",
       ".xr-var-data {\n",
       "  grid-column: 1 / -1;\n",
       "}\n",
       "\n",
       "dl.xr-attrs {\n",
       "  padding: 0;\n",
       "  margin: 0;\n",
       "  display: grid;\n",
       "  grid-template-columns: 125px auto;\n",
       "}\n",
       "\n",
       ".xr-attrs dt,\n",
       ".xr-attrs dd {\n",
       "  padding: 0;\n",
       "  margin: 0;\n",
       "  float: left;\n",
       "  padding-right: 10px;\n",
       "  width: auto;\n",
       "}\n",
       "\n",
       ".xr-attrs dt {\n",
       "  font-weight: normal;\n",
       "  grid-column: 1;\n",
       "}\n",
       "\n",
       ".xr-attrs dt:hover span {\n",
       "  display: inline-block;\n",
       "  background: var(--xr-background-color);\n",
       "  padding-right: 10px;\n",
       "}\n",
       "\n",
       ".xr-attrs dd {\n",
       "  grid-column: 2;\n",
       "  white-space: pre-wrap;\n",
       "  word-break: break-all;\n",
       "}\n",
       "\n",
       ".xr-icon-database,\n",
       ".xr-icon-file-text2 {\n",
       "  display: inline-block;\n",
       "  vertical-align: middle;\n",
       "  width: 1em;\n",
       "  height: 1.5em !important;\n",
       "  stroke-width: 0;\n",
       "  stroke: currentColor;\n",
       "  fill: currentColor;\n",
       "}\n",
       "</style><pre class='xr-text-repr-fallback'>&lt;xarray.Dataset&gt;\n",
       "Dimensions:      (band: 1, x: 18, y: 1)\n",
       "Coordinates:\n",
       "  * band         (band) int64 1\n",
       "  * x            (x) float64 0.5 1.5 2.5 3.5 4.5 ... 13.5 14.5 15.5 16.5 17.5\n",
       "  * y            (y) float64 0.5\n",
       "    spatial_ref  int64 0\n",
       "Data variables:\n",
       "    0            (band, y, x) float32 -170.0 -150.0 -130.0 ... 130.0 150.0 170.0\n",
       "Attributes: (12/25)\n",
       "    archivemetadata:     \\nGROUP                  = ARCHIVEDMETADATA\\n  GROUP...\n",
       "    BROWSE:              N\n",
       "    DAYNIGHT:            N\n",
       "    GRANULEID:           CAL_LID_L3_Stratospheric_APro\n",
       "    GRANULENAME:         CAL_LID_L3_Stratospheric_APro-Standard-V1-01.2021-12...\n",
       "    GRANULEVERSION:      V1-00\n",
       "    ...                  ...\n",
       "    STARTORBITNUMBER:    -999\n",
       "    STARTPATHNUMBER:     -999\n",
       "    START_DATE:          2021-12-01T00:00:00Z\n",
       "    STOPORBITNUMBER:     -999\n",
       "    STOPPATHNUMBER:      -999\n",
       "    STOP_DATE:           2022-01-01T00:00:00Z</pre><div class='xr-wrap' style='display:none'><div class='xr-header'><div class='xr-obj-type'>xarray.Dataset</div></div><ul class='xr-sections'><li class='xr-section-item'><input id='section-5f7a1bee-c6bd-4b80-bf59-363c0276e35b' class='xr-section-summary-in' type='checkbox' disabled ><label for='section-5f7a1bee-c6bd-4b80-bf59-363c0276e35b' class='xr-section-summary'  title='Expand/collapse section'>Dimensions:</label><div class='xr-section-inline-details'><ul class='xr-dim-list'><li><span class='xr-has-index'>band</span>: 1</li><li><span class='xr-has-index'>x</span>: 18</li><li><span class='xr-has-index'>y</span>: 1</li></ul></div><div class='xr-section-details'></div></li><li class='xr-section-item'><input id='section-ae713359-45cc-471e-8914-db32b93bcd7c' class='xr-section-summary-in' type='checkbox'  checked><label for='section-ae713359-45cc-471e-8914-db32b93bcd7c' class='xr-section-summary' >Coordinates: <span>(4)</span></label><div class='xr-section-inline-details'></div><div class='xr-section-details'><ul class='xr-var-list'><li class='xr-var-item'><div class='xr-var-name'><span class='xr-has-index'>band</span></div><div class='xr-var-dims'>(band)</div><div class='xr-var-dtype'>int64</div><div class='xr-var-preview xr-preview'>1</div><input id='attrs-1223d350-79ca-45e7-9f53-11cf091d7ed1' class='xr-var-attrs-in' type='checkbox' disabled><label for='attrs-1223d350-79ca-45e7-9f53-11cf091d7ed1' title='Show/Hide attributes'><svg class='icon xr-icon-file-text2'><use xlink:href='#icon-file-text2'></use></svg></label><input id='data-8e9ce099-6049-4d85-bf23-6b6b48ee894c' class='xr-var-data-in' type='checkbox'><label for='data-8e9ce099-6049-4d85-bf23-6b6b48ee894c' title='Show/Hide data repr'><svg class='icon xr-icon-database'><use xlink:href='#icon-database'></use></svg></label><div class='xr-var-attrs'><dl class='xr-attrs'></dl></div><div class='xr-var-data'><pre>array([1])</pre></div></li><li class='xr-var-item'><div class='xr-var-name'><span class='xr-has-index'>x</span></div><div class='xr-var-dims'>(x)</div><div class='xr-var-dtype'>float64</div><div class='xr-var-preview xr-preview'>0.5 1.5 2.5 3.5 ... 15.5 16.5 17.5</div><input id='attrs-eeecb3b1-b36f-46aa-ac4d-53242f061e09' class='xr-var-attrs-in' type='checkbox' disabled><label for='attrs-eeecb3b1-b36f-46aa-ac4d-53242f061e09' title='Show/Hide attributes'><svg class='icon xr-icon-file-text2'><use xlink:href='#icon-file-text2'></use></svg></label><input id='data-fc6e1acc-6572-4306-b2c5-c9906b0c2113' class='xr-var-data-in' type='checkbox'><label for='data-fc6e1acc-6572-4306-b2c5-c9906b0c2113' title='Show/Hide data repr'><svg class='icon xr-icon-database'><use xlink:href='#icon-database'></use></svg></label><div class='xr-var-attrs'><dl class='xr-attrs'></dl></div><div class='xr-var-data'><pre>array([ 0.5,  1.5,  2.5,  3.5,  4.5,  5.5,  6.5,  7.5,  8.5,  9.5, 10.5, 11.5,\n",
       "       12.5, 13.5, 14.5, 15.5, 16.5, 17.5])</pre></div></li><li class='xr-var-item'><div class='xr-var-name'><span class='xr-has-index'>y</span></div><div class='xr-var-dims'>(y)</div><div class='xr-var-dtype'>float64</div><div class='xr-var-preview xr-preview'>0.5</div><input id='attrs-413e7dcb-4abe-480e-948d-4d32b74094b3' class='xr-var-attrs-in' type='checkbox' disabled><label for='attrs-413e7dcb-4abe-480e-948d-4d32b74094b3' title='Show/Hide attributes'><svg class='icon xr-icon-file-text2'><use xlink:href='#icon-file-text2'></use></svg></label><input id='data-bae34d87-9043-4a25-a561-591d8427ab47' class='xr-var-data-in' type='checkbox'><label for='data-bae34d87-9043-4a25-a561-591d8427ab47' title='Show/Hide data repr'><svg class='icon xr-icon-database'><use xlink:href='#icon-database'></use></svg></label><div class='xr-var-attrs'><dl class='xr-attrs'></dl></div><div class='xr-var-data'><pre>array([0.5])</pre></div></li><li class='xr-var-item'><div class='xr-var-name'><span>spatial_ref</span></div><div class='xr-var-dims'>()</div><div class='xr-var-dtype'>int64</div><div class='xr-var-preview xr-preview'>0</div><input id='attrs-c0c53014-154b-45c0-ae14-cd380969dcc9' class='xr-var-attrs-in' type='checkbox' ><label for='attrs-c0c53014-154b-45c0-ae14-cd380969dcc9' title='Show/Hide attributes'><svg class='icon xr-icon-file-text2'><use xlink:href='#icon-file-text2'></use></svg></label><input id='data-927fce3e-ae17-4845-be04-8d7ea954ca49' class='xr-var-data-in' type='checkbox'><label for='data-927fce3e-ae17-4845-be04-8d7ea954ca49' title='Show/Hide data repr'><svg class='icon xr-icon-database'><use xlink:href='#icon-database'></use></svg></label><div class='xr-var-attrs'><dl class='xr-attrs'><dt><span>GeoTransform :</span></dt><dd>0.0 1.0 0.0 0.0 0.0 1.0</dd></dl></div><div class='xr-var-data'><pre>array(0)</pre></div></li></ul></div></li><li class='xr-section-item'><input id='section-4028d695-5a2c-49e5-b448-c0e43db02f0c' class='xr-section-summary-in' type='checkbox'  checked><label for='section-4028d695-5a2c-49e5-b448-c0e43db02f0c' class='xr-section-summary' >Data variables: <span>(1)</span></label><div class='xr-section-inline-details'></div><div class='xr-section-details'><ul class='xr-var-list'><li class='xr-var-item'><div class='xr-var-name'><span>0</span></div><div class='xr-var-dims'>(band, y, x)</div><div class='xr-var-dtype'>float32</div><div class='xr-var-preview xr-preview'>...</div><input id='attrs-21ed0de9-963d-4474-a0c1-ecb6f78c080b' class='xr-var-attrs-in' type='checkbox' ><label for='attrs-21ed0de9-963d-4474-a0c1-ecb6f78c080b' title='Show/Hide attributes'><svg class='icon xr-icon-file-text2'><use xlink:href='#icon-file-text2'></use></svg></label><input id='data-20e5daf1-5ad1-4a62-9016-456111342459' class='xr-var-data-in' type='checkbox'><label for='data-20e5daf1-5ad1-4a62-9016-456111342459' title='Show/Hide data repr'><svg class='icon xr-icon-database'><use xlink:href='#icon-database'></use></svg></label><div class='xr-var-attrs'><dl class='xr-attrs'><dt><span>scale_factor :</span></dt><dd>1.0</dd><dt><span>add_offset :</span></dt><dd>0.0</dd></dl></div><div class='xr-var-data'><pre>array([[[-170., -150., -130., -110.,  -90.,  -70.,  -50.,  -30.,  -10.,\n",
       "           10.,   30.,   50.,   70.,   90.,  110.,  130.,  150.,  170.]]],\n",
       "      dtype=float32)</pre></div></li></ul></div></li><li class='xr-section-item'><input id='section-8df8d2e3-5ca8-412f-a579-e433326d9a3c' class='xr-section-summary-in' type='checkbox'  ><label for='section-8df8d2e3-5ca8-412f-a579-e433326d9a3c' class='xr-section-summary' >Attributes: <span>(25)</span></label><div class='xr-section-inline-details'></div><div class='xr-section-details'><dl class='xr-attrs'><dt><span>archivemetadata :</span></dt><dd>\n",
       "GROUP                  = ARCHIVEDMETADATA\n",
       "  GROUPTYPE            = MASTERGROUP\n",
       "\n",
       "  OBJECT                 = NUMBEROFRECORDS\n",
       "    NUM_VAL              = 1\n",
       "    VALUE                = -1\n",
       "  END_OBJECT             = NUMBEROFRECORDS\n",
       "\n",
       "END_GROUP              = ARCHIVEDMETADATA\n",
       "\n",
       "END\n",
       "</dd><dt><span>BROWSE :</span></dt><dd>N</dd><dt><span>DAYNIGHT :</span></dt><dd>N</dd><dt><span>GRANULEID :</span></dt><dd>CAL_LID_L3_Stratospheric_APro</dd><dt><span>GRANULENAME :</span></dt><dd>CAL_LID_L3_Stratospheric_APro-Standard-V1-01.2021-12N.hdf</dd><dt><span>GRANULEVERSION :</span></dt><dd>V1-00</dd><dt><span>GRINGLATITUDE.1 :</span></dt><dd>90.0, 45.0, 0.0, -45.0, -90.0, -90.0, -90.0, -90.0, -90.0, -90.0, -90.0, -45.0, 0.0, 45.0, 90.0, 90.0, 90.0, 90.0, 90.0, 90.0, 90.0</dd><dt><span>GRINGLONGITUDE.1 :</span></dt><dd>-180.0, -180.0, -180.0, -180.0, -180.0, -120.0, -60.0, 0.0, 60.0, 120.0, 180.0, 180.0, 180.0, 180.0, 180.0, 120.0, 60.0, 0.0, -60.0, -120.0, -180.0</dd><dt><span>GRINGTYPE :</span></dt><dd>R</dd><dt><span>MAXLAT :</span></dt><dd>90.0</dd><dt><span>MAXLON :</span></dt><dd>180.0</dd><dt><span>METADATANAME :</span></dt><dd>CAL_LID_L3_Stratospheric_APro-Standard-V1-01.2021-12N.hdf.met</dd><dt><span>MINLAT :</span></dt><dd>-90.0</dd><dt><span>MINLON :</span></dt><dd>-180.0</dd><dt><span>ORBITCHANGETIME :</span></dt><dd>-999.0</dd><dt><span>PATHCHANGETIME :</span></dt><dd>-999.0</dd><dt><span>PRODUCTIONDATETIME :</span></dt><dd>2022-04-26T17:44:53Z</dd><dt><span>QAEXPLANATION :</span></dt><dd>All data passed during checkout</dd><dt><span>QAFLAG :</span></dt><dd>Passed</dd><dt><span>STARTORBITNUMBER :</span></dt><dd>-999</dd><dt><span>STARTPATHNUMBER :</span></dt><dd>-999</dd><dt><span>START_DATE :</span></dt><dd>2021-12-01T00:00:00Z</dd><dt><span>STOPORBITNUMBER :</span></dt><dd>-999</dd><dt><span>STOPPATHNUMBER :</span></dt><dd>-999</dd><dt><span>STOP_DATE :</span></dt><dd>2022-01-01T00:00:00Z</dd></dl></div></li></ul></div></div>"
      ],
      "text/plain": [
       "<xarray.Dataset>\n",
       "Dimensions:      (band: 1, x: 18, y: 1)\n",
       "Coordinates:\n",
       "  * band         (band) int64 1\n",
       "  * x            (x) float64 0.5 1.5 2.5 3.5 4.5 ... 13.5 14.5 15.5 16.5 17.5\n",
       "  * y            (y) float64 0.5\n",
       "    spatial_ref  int64 0\n",
       "Data variables:\n",
       "    0            (band, y, x) float32 ...\n",
       "Attributes: (12/25)\n",
       "    archivemetadata:     \\nGROUP                  = ARCHIVEDMETADATA\\n  GROUP...\n",
       "    BROWSE:              N\n",
       "    DAYNIGHT:            N\n",
       "    GRANULEID:           CAL_LID_L3_Stratospheric_APro\n",
       "    GRANULENAME:         CAL_LID_L3_Stratospheric_APro-Standard-V1-01.2021-12...\n",
       "    GRANULEVERSION:      V1-00\n",
       "    ...                  ...\n",
       "    STARTORBITNUMBER:    -999\n",
       "    STARTPATHNUMBER:     -999\n",
       "    START_DATE:          2021-12-01T00:00:00Z\n",
       "    STOPORBITNUMBER:     -999\n",
       "    STOPPATHNUMBER:      -999\n",
       "    STOP_DATE:           2022-01-01T00:00:00Z"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "caliop_raster"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc71a1f4",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
